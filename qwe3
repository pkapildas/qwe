import pandas as pd
import numpy as np
from sklearn.utils import shuffle
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
from nltk.stem import LancasterStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import RegexpTokenizer

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import metrics
 
from textblob import TextBlob
import matplotlib.pyplot as pltm
from tensorflow.keras import backend

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
A word cloud is a visual representation of data where the size of each word corresponds to its frequency or importance in the dataset. it helps quickly identify frequently occuring words, or patterns in textual data.

Word cloud used for sentiment analysis to identify common words for positive or negative reviews or comments. And also used in Topic analysis highlighting keywords or more frequent words from text data. Alo used in providing Social Media insights by understanding trending words, comments.

Drawbacks of Wordcloud

Word cloud ignore the relationships between words,
Since words are treated individualy without considering relation grammer or structre of sentence
Its bais towards common words.
Word cloud will show high frequency words and ignoring important words , ie. stop words like is, the, and might appear if not removed properly.


N-grams are a type of tokenization when individual sentences are split into multiple word string or characters. Its a continous sequence of N words from a given sample of source text(corpus). N-gram is a continous sequence of N items(tokens) from text or speech in NLP.
N-grams are used in applicaions like text analysis machine learning and predictive search input etc. N-grams are different types

Unigram(1-gram ) A sequence of one item ( eg., cat, dog
Bigram (2-gram) a sequence of two items ( eg., deep learning, national language,)
Trigram(3-gram) s sequence of three items. (eg., Machine learning model, data science approach)
4-gram, 5-gram longer sequences of items, for or more consecutive words.
  
Applications of N-grams in NLP

Speech recognition - N-grams play crucial role in modeling and recognizing spoken language patterns, Improving the accuracy of speech recognition systems
Machine Translation - unerstanding and translating phrases with in a broader context.
Predictive Input - N-gram used to suggest next word based of the context of sequence
Named Entity Recognition (NER) - N-grams aid in identifying and extracting named entities from text.
Search Engine Algorithm - Search engine use N-grams to index and retrieve relavant documents based on User Queries and improves the quality of search results.
##Use the data_set.csv data.The “review” column contains the review of several e-shopping items and the “Star” column contains the rating given by the customers.
data = pd.read_csv('data_set.csv')
data.head()
Add a column named ‘Emotion’. If the “Star” (i.e the rating) column is more than 3, mention Emotion as ‘positive’ otherwise ‘negative’ (Marks – 2)¶
data['Emotion']= ['positive' if x>3 else 'negative' for x in data['Star']]
Tokenize (based on space) (1 Marks)
2. Convert to lower Case (1 marks)
3. Remove Punctuations (2 Marks)
4. Remove Stopwords (2 marks)
5. Perform Lemitization (2 Marks)

#1. Tokenise based on space
review_text = data.Review
data_list = list()
for txt in review_text:
    data_list.append(word_tokenize(txt))
print(data_list)

def clean_text(text):
    # tokenizaition 
    words = word_tokenize(text)
    # remove puctuations
    words = [word for word in words if word.isalnum()]
    # remove stop words 
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in words  if word not in stop_words]
    # lemmatization 
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word, pos ='v') for word in tokens]
    return  " ".join(tokens)

# apply Text cleaning 
data["Clean_review"] = data['Review'].apply( lambda x : clean_text(str(x)))
#data['Clean_text']=data['Review'].apply(func_clean_text)
data.head(10)
df = data.copy()
df = df.drop(columns = "Review")
df.head()

df['Review'] = df['Clean_review']
df = df.drop(columns = "Clean_review")
df.head(10)
df['Review'].value_counts()

4.i. Find out the 10 most frequent words used while providing the review for `Refrigerator` & `Mobile`.  (2 +2 )
from collections import Counter

def get_top_N_words(reviews, n=10):
    all_words = " ".join(reviews).split()
    word_counts = Counter(all_words)
    return word_counts.most_common(n);
refrigerator_review = df[df["Item_Review"] =='refrigerator']['Review']
top_refrigerator_words = get_top_N_words(refrigerator_review, 10)
mobile_review = df[df["Item_Review"] =='Mobile']['Review']
print(f'10 Most frequent Word used in Review For Mobile ') 
print(top_mobile_words)
4. ii. Find out the essential/meaningfull words (top 5) which were used in the review for both Refrigerator & Mobile
refrigerator_words = [word for tokens in  refrigerator_review for word in tokens]
mobile_words = [word for tokens in  mobile_review for word in tokens]


refrigerator_set = set(refrigerator_words)
mobile_set = set(mobile_words)
common_words = refrigerator_set.intersection(mobile_set)
common_word_counts = [ (word , refrigerator_words.count(word) + mobile_words.count(word))
                      for word in common_words ]
common_word_counts.sort( key=lambda x:x[1], reverse=True )
top_essential_words = common_word_counts[:5]
print('Top 5 essential/meaningfull words  common to both  Refrigerator & Mobile : \n' , top_essential_words)
5. Build a basic logistic regression model which will categorise the review as per Item_Review
hint:Steps to use
Encode Item_Review  to numeric
Set cleaned text as feature
Segreegate train and test set
vectorize text using count-vectorizer
Build logistic regression model on train set
Measure accuracy on test set
df['label'] = df['Item_Review'].apply(lambda x: 1 if x=='Mobile' else 0)
df['label'].value_counts()
cnt_vct = CountVectorizer(max_features=1000)
cv = cnt_vct.fit_transform(df.Review)
df2 = pd.DataFrame(cv.todense(), columns= cnt_vct.get_feature_names_out())
df2['label'] = df['label']
df2.head()
X = df2.drop(columns='label')
Y = df2['label']
X.shape,Y.shape
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape
lr = LogisticRegression()
model = lr.fit(X_train, y_train)
lr.score(X_train,y_train)
lr.score(X_test,y_test)
pred = lr.predict(X_test)
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, pred))
print(classification_report(y_test, pred))
5. ii. Categorise the following review into categories using the model prepared above
processed_review1 = clean_text(Review1[0])
processed_review2 = clean_text(Review2[0])
processed_review3 = clean_text(Review3[0])
processed_review4 = clean_text(Review4[0])
reviews_vec = cnt_vct.transform(
[ processed_review1,
 processed_review2,
 processed_review3,
 processed_review4 
])
predicted_labels = model.predict(reviews_vec)
predicted_cat = [df['Item_Review'].astype('category').cat.categories[label] for label in predicted_labels]
print('Category for Reviews ')
print(f'Review1 : {predicted_cat[3]}' )
print(f'Review2 : {predicted_cat[2]}' )
print(f'Review3 : {predicted_cat[0]}' )
print(f'Review4 : {predicted_cat[1]}' )


# Question 6 : Sentiment Analysis 
### 6.i. : Assign the subjetivity response to the content
review_df = pd.DataFrame()
review_df['Review'] = df['Review']
review_df.head(5)

def getTextSubjectivity(text):
    return TextBlob(text).sentiment.subjectivity

review_df['subjectivity'] = review_df['Review'].transform(lambda x: getTextSubjectivity(str(x)))
review_df['subjectivity']
### 6.ii : Assign the polarity response to the content 
def getTextPolarity(text):
    return TextBlob(text).sentiment.polarity

review_df['polarity'] = review_df['Review'].transform(lambda x: getTextPolarity(str(x)))
review_df['polarity'
### 6. iii. : Assing sentiment to each of the Review 
hint : 
- if polarity <  0 then sentiment ->"Negative"
- if polarity == 0 then sentiment ->"Neutral"
- if polarity >  0 then sentiment ->"Neutral"

def getTextAnalysis(a):
    if a< 0:
        return 'Negative'
    elif a == 0:
        return 'Neutral'
    else:
        return 'Positive'
    
review_df['sentiment'] = review_df['polarity'].apply(getTextAnalysis)
positive = review_df[review_df['sentiment']== 'Positive']
positivePercentage = (positive.shape[0]/review_df.shape[0])*100
print(f'{positivePercentage}% of Positive Review') 
plt.figure(figsize=(10,6))
labels = review_df.groupby('sentiment').count().index.values
values = review_df.groupby('sentiment').size().values
plt.bar(labels, values)
plt.show()
-----
corpus =  data['review']
<a id="TextCleaning"> </a>
#### 1.2 Cleaning of Corpus
#from spellchecker import SpellChecker
import pandas as pd
from nltk.corpus import stopwords 
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer 
import nltk 
import re
import numpy as np 
 
def remove_punctuation(text) :
    """Remove punctuations from a string"""
    import string
    text = text.translate(str.maketrans('', '', string.punctuation))
    return text
lemmatizer = WordNetLemmatizer()
wordnet_map = {"N":wordnet.NOUN, "V":wordnet.VERB, "J":wordnet.ADJ, "R":wordnet.ADV }
def clean_text(text):
    """
        text: a string
        
        return: modified initial string
  """
    text = text.lower() # lowercase text
    #remove_html_tags(text)
    text =re.sub('<[^<]+?>', '', text)
    remove_punctuation(text)
    # remove urls
    text  = re.sub(r"https?://\S+|www\.\S+", "", text )
    # remove digits
    text= re.sub(r'[0-9]',' ',text)
    text = lemma_words(text)
    text= re.sub(r'[^\w\s]',' ',text) #Removing every thing other than space, word and hash
    text = re.sub(' +', ' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text    
    return text

data['review'] = data['review'].apply(clean_text)
cleaned_corpus = data['review']
#### 1.3 Tokenization of Corpus
from nltk.tokenize import RegexpTokenizer
token_list = list()
for word in cleaned_corpus :
    token_list.append(RegexpTokenizer('\w+').tokenize(word))
#token_list
len(token_list)
Vocabulary of Corpus
- Vocabulary of Corpus refers to unique words of corpus 
- Vocabluary count of Corpus refers to no of unique words of corpus
vocabulary = set(x for lst in token_list for x in lst)
vocabulary
vocabluary_count = len(vocabulary)
vocabluary_count
Text to Numeric Convesion
# toy example 
from sklearn.feature_extraction.text import CountVectorizer
# list of text documents
text = ["The quick brown fox jumped over the lazy dog."]
# create the transform
vectorizer = CountVectorizer()
# tokenize and build vocab
vectorizer.fit(text)
# summarize
print(vectorizer.vocabulary_)
# encode document
vector = vectorizer.transform(text)
# summarize encoded vector
print(vector.shape)
print(type(vector))
print(vector.toarray())
# encode another document
text2 = ["the puppy"]
vector = vectorizer.transform(text2)
print(vector.toarray())


from sklearn.feature_extraction.text import CountVectorizer
count_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))
# analyzer should be word/ character
#ngram_range lower and upper boundary of the range of n-values
## let'sconvert text to vectors
data_countvectors = count_vectorizer.fit_transform(cleaned_corpus)

# converting sparce to dense vector 
print(data_countvectors.shape)
print(data_countvectors[0])
print(data_countvectors[0].todense())
#TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
# tfidf toy example
# list of text documents
text = ["The quick brown fox jumped over the lazy dog.",
        "The dog.",
        "The fox"]
# create the transform
vectorizer = TfidfVectorizer()
# tokenize and build vocab
vectorizer.fit(text)
# summarize
print(vectorizer.vocabulary_)
print(vectorizer.idf_)
# encode document
vector = vectorizer.transform([text[0]])
# summarize encoded vector
print(vector.shape)

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(analyzer='word',stop_words='english', ngram_range=(1, 1))
# training tfidf on corpus
tfidf_vectorizer.fit(cleaned_corpus)

data_tfidfvectors = tfidf_vectorizer.transform(cleaned_corpus)
# can do fit_tranform at once alos like countvectorizer conversion 
print(data_tfidfvectors.shape)
print(data_tfidfvectors) 
print(data_tfidfvectors.todense().shape)
print(data_tfidfvectors[0].todense())
 Word2Vec

3.1.1. SkipGram¶
#Build the model for our tokenized corpus
import gensim 
from time import time
t = time()
# Option1 :  intializa and then  build  skip gram embedding  model
# initialize skipgram model
sg_model = gensim.models.Word2Vec(min_count=10,
                    window=5, sg = 1,
                    sample=5e-5, alpha=0.05, 
                    min_alpha=0.0005,negative=20 )

# build model vocabulary
sg_model.build_vocab(token_list)

# train the model
sg_model.train(token_list, total_examples=len(token_list), epochs=10, report_delay=1)

print('Time to build & Train Skip-Gram model vocab: {} mins'.format(round((time() - t) / 60, 2)))

## option2 : intialize &  build together
# sg_model_2 = gensim.models.Word2Vec(token_list, #Word list       
#                                min_count=10, #Ignore all words with total frequency lower than this                           
#                                workers=4, #Number of CPU Cores
#                                window=5, #Neighbours on the left and right
#                                sg = 1 # 1 skipgram , 0 cbow                          
#                               ) 

#Model size
sg_model.wv.vectors.shape
# total no of words in model : 1052 
# dimension 100
sg_model.wv['etc']
# another way to get embeeding of a word 
sg_model.wv.__getitem__('cup')
len(sg_model.wv.__getitem__('early'))
type(sg_model.wv.__getitem__('cup'))
# top 5 similar words
sg_model.wv.most_similar('early')[:5]
Continuous Bag of Words (CBOW)¶
from gensim.models import Word2Vec
from time import time
t = time()
# initialize
cbow_model = Word2Vec(min_count=2,window=2, sg = 0,sample=5e-5, alpha=0.05, min_alpha=0.0005, 
                     negative=20 )
# note just change  sg= 0 for cbow  every other argument remains same
# build model vocabulary
cbow_model.build_vocab(token_list)

# train the model
cbow_model.train(token_list, total_examples=cbow_model.corpus_count, epochs=30, report_delay=1)

print('Time to build & Train CBOW model vocab: {} mins'.format(round((time() - t) / 60, 2)))
cbow_model.wv.__getitem__('cup')
# top 5 similar words
cbow_model.wv.most_similar('early')[:5]
Global Vector
## The glove_python library is not updated lately by its developers, hence the installation may throw error,
## This code is optional
#!pip install glove_python

# # importing the glove library
# # glove toy implementtion 
# from glove import Corpus, Glove

# # creating a corpus object
# corpus = Corpus() 

# #training the corpus to generate the co occurence matrix which is used in GloVe
# corpus.fit(tokens_list_of_lists, window=3)

# #creating a Glove object which will use the matrix created in the above lines to create embeddings
# #we can set the learning rate as glove uses Gradient Descent
# glove = Glove(no_components=300, learning_rate=0.05)
# glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)
# glove.add_dictionary(corpus.dictionary)
# glove.save('glove.model')
#option1 :
from gensim.models import FastText
from time import time
t = time()

#Initialize
fasttext_model = FastText(vector_size=4, window=3, min_count=1)  # instantiate
# build 
fasttext_model.build_vocab(corpus_iterable=token_list)
# train 
fasttext_model.train(corpus_iterable=token_list, total_examples=len(token_list), epochs=10) 
print('Time to build & Train Skip-Gram model vocab: {} mins'.format(round((time() - t) / 60, 2)))
# ## option2 : intialize,  build  & train together
# fasttext_model = FastText(tokens_list, window=5, min_count=5, workers=4, sg=1)
fasttext_model.wv.__getitem__('people')
# fasttext word similarity measure
fasttext_model.wv.similarity('evacuation','shelter' )
# most similar words
fasttext_model.wv.most_similar('fire')[:5]
#fetching  pretrain wordvector
from gensim.models.keyedvectors import load_word2vec_format
pretrained_w2vec_embedding = load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
pretrained_w2vec_embedding['people']
# shape of pretrain w2vec embedding
pretrained_w2vec_embedding.vectors.shape
import gensim.downloader as api

#genism is not working properly  for Python3.7+.
#Load Glove model  pretrained word embedding  
# pip3 install --upgrade certifi
pretrained_glove_model = api.load('glove-wiki-gigaword-50')
#Size of the model
pretrained_glove_model.vectors.shape
#Embedding for word  early
pretrained_glove_model['early']
#Embedding length based on selected model - we are using 50d here.
pretrained_glove_model.vector_size
### 4.3. Pre Trained FastText

def get_coefs(word, *arr): 
    return word, np.asarray(arr, dtype='float32')
EMBEDDING_FILE = 'wiki-news-300d-1M-002.vec'
pretrained_fasttext_embedding = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in (open(EMBEDDING_FILE)))
## no of words in pretrained fasttext embedding
len(pretrained_fasttext_embedding)
pretrained_fasttext_embedding['fire']
len(pretrained_fasttext_embedding['fire'])
---------------

Text Processing Using Keras library
import tensorflow as tf
# how to decided on vocabulary size ?
# do you remember vocabluary_count was 30916 ?
desired_vocab_size = 20000 #Vocablury size
# initialzing keras tokenizer
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=desired_vocab_size, oov_token='OOV') 
 
 token_list = list()
  for word in cleaned_corpus :
      token_list.append(RegexpTokenizer('\w+').tokenize(word))
#Fit tokenizer with actual training data
tokenizer.fit_on_texts(token_list)
#Vocabulary
tokenizer.word_index

# Sequences & Pad-Sequences Get the word index for each of the word in the review data
data_sequence = tokenizer.texts_to_sequences(token_list)
data_sequence
# Pad Sequences ->to make all list of same length 
max_review_length = 200
data_sequence_padded = tf.keras.preprocessing.sequence.pad_sequences(data_sequence,
                                                        maxlen=max_review_length,padding='pre')
type(data_sequence_padded)
data_sequence_padded.shape
Embedding Layer(with Embedding Matrix)

embedding_vector_length = dimension = 100
embedding_matrix = np.zeros((desired_vocab_size + 1, embedding_vector_length))
# create Embedding Matrix Load word vectors for each word in our vocabulary from from Glove pre-trained model

for word, i in sorted(tokenizer.word_index.items(), key=lambda x:x[1]):
    if i > (desired_vocab_size+1):
        break
    try:
        embedding_vector = pretrained_glove_model[word] #Reading word's embedding from Glove model for a given word
        embedding_matrix[i] = embedding_vector
    except:
        pass
Model Definition
#Initialize model
tf.keras.backend.clear_session()
model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(desired_vocab_size + 1, #Vocablury size
                                    embedding_vector_length, #Embedding size
                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model
                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.
                                    input_length=max_review_length) #Number of words in each review
          )
model.output
model.summary()
Use Case 1 : Building Sentiment Classification Model using TFIDF vectorizer and XGBoost
# we have already vectorize review data with 
data_tfidfvectors.shape
X = data_tfidfvectors
y = data['sentiment']
import xgboost as xgb
xgb_clf = xgb.XGBClassifier()
from sklearn.model_selection import cross_val_score
print(cross_val_score(xgb_clf, X, y, cv=5,scoring='accuracy'))
Use Case 2 : Building Sentiment Classification Model using GloVe Pre-trained Embeeding and DNN
#Initialize model

#Initialize model
# building on to previous model 
tf.keras.backend.clear_session()
dnn_model = tf.keras.Sequential()
dnn_model.add(tf.keras.layers.Embedding(desired_vocab_size + 1, #Vocablury size
                                    embedding_vector_length, #Embedding size
                                    weights=[embedding_matrix], #Embeddings taken from GloVe pre-trained
                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.
                                    input_length=max_review_length) #Number of words in each review
          )
 

#Flatten the data as we will use Dense layers
dnn_model.add(tf.keras.layers.Flatten())


#Add Hidden layers (Dense layers)
 
dnn_model.add(tf.keras.layers.Dense(100, activation='relu', input_shape=()))
dnn_model.add(tf.keras.layers.BatchNormalization())
dnn_model.add(tf.keras.layers.Dense(50, activation='relu'))
dnn_model.add(tf.keras.layers.BatchNormalization())
dnn_model.add(tf.keras.layers.Dense(25, activation='relu'))
dnn_model.add(tf.keras.layers.Dropout(0.25))

# output layer
dnn_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

#Compile the model
dnn_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

dnn_model.summary()

train Model 
target = data['sentiment'] 

my_callbacks = [tf.keras.callbacks.EarlyStopping(patience=5) ]

dnn_model.fit(data_sequence_padded,target,
          epochs=5,
          batch_size=32, validation_split=0.2, callbacks= my_callbacks)
Looks like TF-IDF model performance is better than DNN 
=========================

df17 = pd.read_csv(path17) ; 
df1 = df17.copy()
df1 = df1.rename(columns=({'Text':'Review'}))
df1=df1.drop(columns=['Id', 'Unnamed: 0'] , axis=1)
#df1 = df1.drop_duplicates(())
df1.head(2)

############## STEP 0 : Clean Text, Preprocessing  ################

############# 1 Clean the Text ##################
stop_words=set(stopwords.words('english'))
stop_words.discard('no')
stop_words.discard('not')
def func_clean_text(text):
    text   = text.lower()
    text   = unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8')
    text   = re.sub(r'http\S+|www\S+', '', text)  # Remove URLs
    text   = re.sub(r'\d','',text)                # Remove digits
    text   = text.translate(str.maketrans('','',string.punctuation))    # Remove punctuation
    tokens = [word for word in nltk.word_tokenize(text)                            if word not in stop_words]
    #tokens = [nltk.PorterStemmer().stem(word) for word in nltk.word_tokenize(text) if word not in stop_words]
    #tokens = [nltk.stem.WordNetLemmatizer().lemmatize(word) for word in nltk.word_tokenize(text)         if word not in stop_words]
    text   = ' '.join(tokens)                     
    text   = re.sub(r'\s+',' ',text).strip()       # Eliminate multiple spaces
    return text
df1['Clean_text']=df1['Review'].apply(func_clean_text)


################# 2  Polarity & Subjectivity & Sentiment #####
def getTextSubjectivity(txt):
    return TextBlob(txt).sentiment.subjectivity

def getTextPolarity(txt):
    return TextBlob(txt).sentiment.polarity

def getTextSentiment(a):
    if a < 0:
        return "Negative"
    elif a == 0:
        return "Neutral"
    else:
        return "Positive"

df1['Polarity'    ]   = df1['Clean_text'].transform(lambda x: getTextPolarity(str(x)))
df1['Subjectivity']   = df1['Clean_text'].transform(lambda x: getTextSubjectivity(str(x)))
df1['Sentiment'   ]   = df1['Polarity'].apply(getTextSentiment)
df1['ScoreEncoded']   = df1['Score']-1
df1['ScoreEncoded']   = df1['ScoreEncoded'].astype(int)
display(df1.head(2))


#df1['Emotion'] = df1['Polarity'].apply(lambda x: 'Positive' if x>3 else 'Negative') ; #data.head()

############ 3 Questions on Sentiment Analysis ########
a= df1[df1['Sentiment'] == 'Positive']
b = a.shape[0]/(df1.shape[0])*100
print(b , " % of positive Review")

#Visualize the frequency distribution of the sentiment on each content
plt.figure(figsize = (4,2))
labels = df1.groupby('Sentiment').count().index.values
values = df1.groupby('Sentiment').size().values
plt.bar(labels, values); plt.show();

#################### 4 Similar words to food [CBOW, Word to vec] ##########################
df_tokenize = df1['Clean_text'].apply(nltk.word_tokenize)          ## tokenized words
# df_tokenize = [text.split() for text in df1['Clean_text']]

model=Word2Vec(df_tokenize, window=5,sg=1,vector_size=100,min_count=1,epochs=300) ;  
similar_words = model.wv.most_similar(positive=['food'],topn=5) ; print(similar_words)


############## STEP 1 : Feature Engineering  ###############

#TF-IDF values are not raw counts but normalized weights,
############## 5 Count Vectoriser : Top 6 most frequent words  ################
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

#model          = CountVectorizer()                       # common words dominate    #SPAM/HAM
model           = TfidfVectorizer()                       # reduce importance common words dominate   
model_fit       = model.fit_transform(df1['Clean_text'])  # tv.toarray()[:2]
vocabulary      = model.get_feature_names_out()           # Retrieve the feature names (words)
term_importance = model_fit.sum(axis=0)                   # Sum the occurrences of each word across all documents
word_freq_df    = pd.DataFrame({'Word': vocabulary, 'Frequency': term_importance .A1})
word_freq_df    = word_freq_df.sort_values(by='Frequency', ascending=False)

model_shape     = word_freq_df.shape           ; print(model_shape)      # top 5 word
top_5_words     = word_freq_df.head(6) ; print(top_5_words)      # top 5 word
importance_of_food = word_freq_df[word_freq_df['Word'] =='food'] # checking importance of word "food"
print(importance_of_food)

####################### Section C ################################
####################### Train Test ###############################
# Convert text into numerical representations (CountVectorizer & TF-IDF)
vectorizer_count = CountVectorizer()
vectorizer_tfidf = TfidfVectorizer()

X_count = vectorizer_count.fit_transform(df1['Clean_text'])
X_tfidf = vectorizer_tfidf.fit_transform(df1['Clean_text'])
y = df1['Review']

# Split dataset (Ensuring stratified sampling)
X_train_count, X_test_count, y_train, y_test = train_test_split(X_count, df1['Sentiment'] , test_size=0.25, stratify= df1['Sentiment'], random_state=42)
X_train_tfidf, X_test_tfidf,  _     , _      = train_test_split(X_tfidf, df1['Sentiment'] , test_size=0.25, stratify= df1['Sentiment'], random_state=42)

####################### LR, Naive Bayas ###############################
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

# Logistic Regression Model
def train_model(model_class, X_train, X_test, y_train, y_test, model_type):
    model = model_class
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\n{model_type} {model_class} Accuracy:", accuracy)
    return model  

# Train & Evaluate Logistic Regression
train_model(LogisticRegression(), X_train_count, X_test_count, y_train, y_test, "Count-Vectorized")
train_model(LogisticRegression(), X_train_tfidf, X_test_tfidf, y_train, y_test, "TF-IDF")
train_model(MultinomialNB()     , X_train_count, X_test_count, y_train, y_test, "Count-Vectorized")
#train_model(MultinomialNB()    , X_train_tfidf, X_test_tfidf, y_train, y_test, "TF-IDF")

#---------------------------------------------------------------------------------------------------------------------------------------------------




import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Flatten, BatchNormalization, Bidirectional, SpatialDropout1D
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import regularizers
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Load dataset
df  = pd.read_csv('/content/drive/MyDrive/dataset/july24_data_set.csv')
df = df.rename(columns=({'Text':'Review'}))
df = df.drop(columns=['Id', 'Unnamed: 0'] , axis=1)
#df = df.drop_duplicates(())


############# 1 Clean the Text ##################
stop_words=set(stopwords.words('english'))
stop_words.discard('no')
stop_words.discard('not')
def func_clean_text(text):
    text   = text.lower()
    text   = unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8')
    text   = re.sub(r'http\S+|www\S+', '', text)  # Remove URLs
    text   = re.sub(r'\d','',text)                # Remove digits
    text   = text.translate(str.maketrans('','',string.punctuation))    # Remove punctuation
    tokens = [word for word in nltk.word_tokenize(text)                            if word not in stop_words]
    #tokens = [nltk.PorterStemmer().stem(word) for word in nltk.word_tokenize(text) if word not in stop_words]
    #tokens = [nltk.stem.WordNetLemmatizer().lemmatize(word) for word in nltk.word_tokenize(text)         if word not in stop_words]
    text   = ' '.join(tokens)
    text   = re.sub(r'\s+',' ',text).strip()       # Eliminate multiple spaces
    return text
df['Clean_text']=df['Review'].apply(func_clean_text)

# Tokenization
print("Tokenizing text...")
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(df['Clean_text'])
sequences = tokenizer.texts_to_sequences(df['Clean_text'])

# Sequence Padding
max_len = max([len(seq) for seq in sequences])
embed_dim = 100  # Own embedding size
vocab_size = len(tokenizer.word_index) + 1
print(f'The max sentence length: {max_len}, Unique words: {vocab_size}')

# Padding Sequences
padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')

# Label Encoding
y = df['Score']
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Splitting Data
xtrain, xtest, ytrain, ytest = train_test_split(padded_sequences, y_encoded, test_size=0.3, random_state=48, stratify=y_encoded)

# Encoding Labels for LSTM
ytrain_en = to_categorical(ytrain, num_classes=len(np.unique(y_encoded)))
ytest_en = to_categorical(ytest, num_classes=len(np.unique(y_encoded)))

# Building LSTM Model
print("Building LSTM Model...")
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=max_len))

model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))

#model.add(LSTM(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01), return_sequences=True))
#model.add(Dropout(0.2))
#model.add(LSTM(32, activation='tanh'))
#model.add(Flatten())

model.add(Dense(64, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dense(len(np.unique(y_encoded)), activation='softmax'))


# Compile Model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train Model with Early Stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = model.fit(xtrain, ytrain_en, batch_size=32, epochs=10, validation_data=(xtest, ytest_en), callbacks=[early_stopping])

# Predict
test_predictions = model.predict(xtest)
ypred_pr = np.argmax(test_predictions, axis=1)

# Model Evaluation
print(f"Length of X_test: {len(xtest)}")
print(f"Length of y_test: {len(ytest)}")
print(f"Length of predictions: {len(ypred_pr)}")
print(f"Model Accuracy: {accuracy_score(ytest, ypred_pr) * 100:.2f}%")

# Confusion Matrix & Classification Report
plt.figure(figsize=(6, 5))
sns.heatmap(confusion_matrix(ytest, ypred_pr), annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_.astype(str), yticklabels=label_encoder.classes_.astype(str))
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

print("Classification Report:")
print(classification_report(ytest, ypred_pr, target_names=label_encoder.classes_.astype(str)))


