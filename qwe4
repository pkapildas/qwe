
# Step 1: Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, cohen_kappa_score, confusion_matrix, roc_curve, auc
from scipy.stats import zscore
from imblearn.over_sampling import SMOTE
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Step 2: Load Dataset
df = pd.read_csv("loan.csv")

# -------------------- (A) Data Exploration --------------------
print("\nDataset Shape:", df.shape)
print("\nData Info:\n", df.info())
print("\nMissing Values:\n", df.isnull().sum())

# Identify Numerical & Categorical Columns
numerical_cols = df.select_dtypes(include=['number']).columns.tolist()
categorical_cols = df.select_dtypes(exclude=['number']).columns.tolist()

print("\nNumerical Columns:", numerical_cols)
print("\nCategorical Columns:", categorical_cols)

# -------------------- (B) Data Cleaning --------------------

#First run a check for the presence of missing values and their percentage for each column. Then choose the right approach to treat them.
Total = df_admissions.isnull().sum().sort_values(ascending=False)          
Percent = (df_admissions.isnull().sum()*100/df_admissions.isnull().count()).sort_values(ascending=False)   
missing_data = pd.concat([Total, Percent], axis = 1, keys = ['Total', 'Percentage of Missing Values'])    
missing_data
df_num = df_feature.select_dtypes(include = [np.number])
df_cat = df_feature.select_dtypes(include = [np.object])

# change the data type of 'Research'
df_admissions['Research'] = df_admissions['Research'].astype(object)

# Handling Missing Values

"""
imputer_num = SimpleImputer(strategy="median")
df[numerical_cols] = imputer_num.fit_transform(df[numerical_cols])

imputer_cat = SimpleImputer(strategy="most_frequent")
df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])
"""

for col in numerical_cols:
    if df[col].isnull().sum() > 0:  
        median_value = df[col].median()  # Compute median dynamically
        df[col].fillna(median_value, inplace=True)  # Replace missing values with median

for col in categorical_cols:
    if df[col].isnull().sum() > 0:
        mode_value = df[col].mode()[0]  # Compute mode dynamically
        df[col].fillna(mode_value, inplace=True)  # Replace missing values with mode
#Examine Outliers
for i in df_f.columns:
    sb.boxplot(df_f[i])
    plt.show()

for i in df_f.columns:
    sb.kdeplot(stats.zscore(df_f[i]))
    plt.axvline(3, ymin=0, ymax=0.01, c='r')
    plt.axvline(-3, ymin=0, ymax=0.01, c='r')
    plt.show()

# Handling Outliers using Z-score
z_scores = np.abs(zscore(df[numerical_cols]))
df = df[(z_scores < 3).all(axis=1)]  

 # removal of outliers
Q1=data[num_cols].quantile(0.25)
Q3=data[num_cols].quantile(0.75)
IQR=Q3-Q1
lower_bound=Q1-1.5*IQR
upper_bound=Q3+1.5*IQR
data_fix=data[~((data[num_cols]<lower_bound) | (data[num_cols]>upper_bound)).any(axis=1)]
print(data.shape)
print(data_fix.shape)

# for the independent numeric variables, we plot the histogram to check the distribution of the variables
# Note: the hist() function considers the numeric variables only, by default
# we drop the target variable using drop()
# 'axis=1' drops the specified column
df_admissions.drop('Chance of Admit', axis = 1).hist()
plt.tight_layout()
# skew() returns the coefficient of skewness for each variable
df_admissions.drop('Chance of Admit', axis = 1).skew()
#Correlation 
    sb.heatmap(df_n.corr(), annot=True, cmap='rainbow')

# -------------------- (C) Feature Engineering --------------------
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Scaling from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, precision_score , recall_score, f1_score, classification_report, cohen_kappa_score
scaler = StandardScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])
# create a dataframe of scaled numerical variables
df_num_scaled = pd.DataFrame(num_scaled, columns = df_num.columns)
dummy_var = pd.get_dummies(data = df_cat, drop_first = True)
X = pd.concat([df_num_scaled, dummy_var], axis = 1)




# -------------------- (D) Variance Inflation Factor (VIF) --------------------
X = df.drop(columns=["CustomerId", "Loan"])  
y = df["Loan"]

vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Remove features with high VIF
#X = X.loc[:, vif_data["VIF"] < 10]
X = X.loc[:, vif_data[vif_data["VIF"] < 10]["Feature"]]


# -------------------- (E) Train-Test Split --------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

y_train = y_train.astype(int)
y_test = y_test.astype(int)

# -------------------- (F) Handling Class Imbalance (SMOTE) --------------------
if len(y_train.unique()) > 1:  
    sm = SMOTE(random_state=42)
    X_train, y_train = sm.fit_resample(X_train, y_train)

# -------------------- (G) Model Training & Evaluation --------------------
# Random Forest
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# K-Nearest Neighbors (KNN)
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
y_pred_knn = knn_model.predict(X_test)

# AdaBoost Classifier
ada_model = AdaBoostClassifier(n_estimators=100, random_state=42)
ada_model.fit(X_train, y_train)
y_pred_ada = ada_model.predict(X_test)

# Model Evaluation
accuracy_rf = accuracy_score(y_test, y_pred_rf)
accuracy_knn = accuracy_score(y_test, y_pred_knn)
accuracy_ada = accuracy_score(y_test, y_pred_ada)

print("\nRandom Forest Accuracy:", accuracy_rf)
print("KNN Accuracy:", accuracy_knn)
print("AdaBoost Accuracy:", accuracy_ada)

# -------------------- (H) Model Comparison Visualizations --------------------
# Accuracy Comparison
plt.figure(figsize=(6, 4))
sns.barplot(x=["Random Forest", "KNN", "AdaBoost"], y=[accuracy_rf, accuracy_knn, accuracy_ada], palette="viridis")
plt.title("Accuracy Comparison of Models")
plt.ylabel("Accuracy Score")
plt.show()

# Confusion Matrices
plt.figure(figsize=(18, 5))

plt.subplot(1, 3, 1)
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt="d", cmap="Blues")
plt.title("Random Forest Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")

plt.subplot(1, 3, 2)
sns.heatmap(confusion_matrix(y_test, y_pred_knn), annot=True, fmt="d", cmap="Oranges")
plt.title("KNN Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")

plt.subplot(1, 3, 3)
sns.heatmap(confusion_matrix(y_test, y_pred_ada), annot=True, fmt="d", cmap="Greens")
plt.title("AdaBoost Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")

plt.show()

# -------------------- (I) ROC Curve for All Models --------------------
if len(np.unique(y_test)) == 2:  # ROC only for binary classification
    y_probs_rf = rf_model.predict_proba(X_test)[:, 1]
    y_probs_knn = knn_model.predict_proba(X_test)[:, 1]
    y_probs_ada = ada_model.predict_proba(X_test)[:, 1]

    fpr_rf, tpr_rf, _ = roc_curve(y_test, y_probs_rf)
    fpr_knn, tpr_knn, _ = roc_curve(y_test, y_probs_knn)
    fpr_ada, tpr_ada, _ = roc_curve(y_test, y_probs_ada)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr_rf, tpr_rf, label=f"Random Forest (AUC = {auc(fpr_rf, tpr_rf):.2f})", color="blue")
    plt.plot(fpr_knn, tpr_knn, label=f"KNN (AUC = {auc(fpr_knn, tpr_knn):.2f})", color="orange")
    plt.plot(fpr_ada, tpr_ada, label=f"AdaBoost (AUC = {auc(fpr_ada, tpr_ada):.2f})", color="green")

    plt.plot([0, 1], [0, 1], "k--")  
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve Comparison")
    plt.legend()
    plt.show()

# -------------------- (J) Feature Importance from Random Forest --------------------
feature_importance = pd.DataFrame({
    "Feature": X_train.columns,
    "Importance": rf_model.feature_importances_
}).sort_values(by="Importance", ascending=False)

print("\nTop Features Influencing Loan Payback:\n", feature_importance.head(10))

# Plot Feature Importance
plt.figure(figsize=(10, 5))
sns.barplot(x="Importance", y="Feature", data=feature_importance.head(10))
plt.title("Top Features Affecting Loan Payback")
plt.show()
# -------------------- END OF CODE --------------------
#It uses a Bayes' Theorem with the assumption of independence of predictor variables. The sklearn library provides different naive bayes classifiers, as GaussianNB, MultinomialNB and so on.
stantiate the 'GaussianNB'
gnb = GaussianNB()

# fit the model using fit() on train data
gnb_model = gnb.fit(X_train, y_train)
# pass the gaussian naive bayes model to the function
plot_confusion_matrix(gnb_model)
# define a to plot a confusion matrix for the model
def plot_confusion_matrix(model):
    
    # predict the target values using X_test
    y_pred = model.predict(X_test)
    
    # create a confusion matrix
    # pass the actual and predicted target values to the confusion_matrix()
    cm = confusion_matrix(y_test, y_pred)

    # label the confusion matrix  
    # pass the matrix as 'data'
    # pass the required column names to the parameter, 'columns'
    # pass the required row names to the parameter, 'index'
    conf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])

    # plot a heatmap to visualize the confusion matrix
    # 'annot' prints the value of each grid 
    # 'fmt = d' returns the integer value in each grid
    # 'cmap' assigns color to each grid
    # as we do not require different colors for each grid in the heatmap,
    # use 'ListedColormap' to assign the specified color to the grid
    # 'cbar = False' will not return the color bar to the right side of the heatmap
    # 'linewidths' assigns the width to the line that divides each grid
    # 'annot_kws = {'size':25})' assigns the font size of the annotated text 
    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = ListedColormap(['lightskyblue']), cbar = False, 
                linewidths = 0.1, annot_kws = {'size':25})

    # set the font size of x-axis ticks using 'fontsize'
    plt.xticks(fontsize = 20)

    # set the font size of y-axis ticks using 'fontsize'
    plt.yticks(fontsize = 20)

    # display the plot
    plt.show()
test_report = get_test_report(gnb_model)
plot_roc(gnb_model)
# create a generalized function to calculate the performance metrics values for test set
def get_test_report(model):
    
    # for test set:
    # test_pred: prediction made by the model on the test dataset 'X_test'
    # y_test: actual values of the target variable for the test dataset

    # predict the output of the target variable from the test data 
    test_pred = model.predict(X_test)

    # return the classification report for test data
    return(classification_report(y_test, test_pred))
# define a function to plot the ROC curve and print the ROC-AUC score
def plot_roc(model):
    
    # predict the probability of target variable using X_test
    # consider the probability of positive class by subsetting with '[:,1]'
    y_pred_prob = model.predict_proba(X_test)[:,1]
    
    # the roc_curve() returns the values for false positive rate, true positive rate and threshold
    # pass the actual target values and predicted probabilities to the function
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

    # plot the ROC curve
    plt.plot(fpr, tpr)

    # set limits for x and y axes
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])

    # plot the straight line showing worst prediction for the model
    plt.plot([0, 1], [0, 1],'r--')

    # add plot and axes labels
    # set text size using 'fontsize'
    plt.title('ROC curve for Cancer Prediction Classifier', fontsize = 15)
    plt.xlabel('False positive rate (1-Specificity)', fontsize = 15)
    plt.ylabel('True positive rate (Sensitivity)', fontsize = 15)

    # add the AUC score to the plot
    # 'x' and 'y' gives position of the text
    # 's' is the text 
    # use round() to round-off the AUC score upto 4 digits
    plt.text(x = 0.02, y = 0.9, s = ('AUC Score:',round(roc_auc_score(y_test, y_pred_prob),4)))

    # plot the grid
    plt.grid(True)
Note: Algorithms like Naive Bayes and tree based algorithms do not need feature scaling or normalization. Performing a features scaling in these algorithms may not have much effect.
Tune the Hyperparameters using GridSearchCV (Random Forest)
tuned_paramaters = [{'criterion': ['entropy', 'gini'],
                     'n_estimators': [10, 30, 50, 70, 90],
                     'max_depth': [10, 15, 20],
                     'max_features': ['sqrt', 'log2'],
                     'min_samples_split': [2, 5, 8, 11],
                     'min_samples_leaf': [1, 5, 9],
                     'max_leaf_nodes': [2, 5, 8, 11]}]
random_forest_classification = RandomForestClassifier(random_state = 10)
rf_grid = GridSearchCV(estimator = random_forest_classification, 
                       param_grid = tuned_paramaters, 
                       cv = 5)
rf_grid_model = rf_grid.fit(X_train, y_train)
print('Best parameters for random forest classifier: ', rf_grid_model.best_params_, '\n')

Build the model using the tuned hyperparameters.
rf_model = RandomForestClassifier(criterion = rf_grid_model.best_params_.get('criterion'), 
                                  n_estimators = rf_grid_model.best_params_.get('n_estimators'),
                                  max_depth = rf_grid_model.best_params_.get('max_depth'),
                                  max_features = rf_grid_model.best_params_.get('max_features'),
                                  max_leaf_nodes = rf_grid_model.best_params_.get('max_leaf_nodes'),
                                  min_samples_leaf = rf_grid_model.best_params_.get('min_samples_leaf'),
                                  min_samples_split = rf_grid_model.best_params_.get('min_samples_split'),
                                  random_state = 10)
rf_model = rf_model.fit(X_train, y_train)
print('Classification Report for test set:\n', get_test_report(rf_model))
 
Identify the Important Features
Let us create a barplot to identify the important featur
# use fit() to fit the model on the train set

# get the best parameters
print('Best parameters for random forest classifier: ', rf_grid_model.best_params_, '\n')

Decision Tree is a non-parametric supervised learning method. It builds a model in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets, which is called splitting. A decision node is a node on which a decision of split is to be made. A node that can not be split further is known as the terminal/leaf node. A leaf node represents the decision. A decision tree can work with both numerical and categorical variables.
A decision tree for classification is built using criteria like the Gini index and entropy.
Gini Index
Gini index measures the probability of the sample being wrongly classified. The value of the Gini index varies between 0 and 1. We choose the variable with a low Gini index. The Gini index of the variable is calculated as:
Gini=1−∑ni=1p2i
Where,
pi
: Probability of occurrence of the class 'i'

Entropy
Entropy is one of the criteria used to build the decision tree. It calculates the heterogeneity of the sample. The entropy is zero if the sample is completely homogeneous, and it is equal to 1 if the sample is equally divided. Entropy of the variable 'X' is calculated as:
E(X)=−∑ci=1pilog2pi
Where,
pi
: Probability of occurrence of the class 'i'
And the conditional emtropy of the variable is given as:
E(T,X)=∑cϵXP(c)E(c)
Where,
P(c)
: Probability of occurrence of the class 'c'
E(c)
: Entropy of the class 'c'

The information gain is the difference between the entropy of the target variable and the entropy of the target variable given an independent variable. We split the on the variable that corresponds to the highest information gain.
# instantiate the 'DecisionTreeClassifier' object using 'entropy' criterion
# pass the 'random_state' to obtain the same samples for each time you run the code
decision_tree_classification = DecisionTreeClassifier(criterion = 'entropy', random_state = 10)

# fit the model using fit() on train data
decision_tree = decision_tree_classification.fit(X_train, y_train)
