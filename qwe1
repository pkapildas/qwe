Convert the following matrix to an orthogonal matrix using Gram Schmidt Process?
A = np.array( ((1,1,1), (-1, 0, 1), (1, 1, 2)))
A
u1, u2, u3 = A[:,0], A[:,1], A[:,2]
w1 =u1
# find magnitude of u1
mag_w1 = np.linalg.norm(w1) 
# do normalization 
v1= w1/mag_w1
print(v1.dot(v1))
# remove vector projection of u2 in v1 from u2
w2 =u2- (u2.dot(v1)) *v1 
mag_w2 = np.linalg.norm(w2) 
v2 = w2/mag_w2
v2.dot(v2)
# remove vector projection of u3 in v1 and v2 from u3
w3 = u3- ((u3.dot(v1)) *v1) -  ((u3.dot(v2)) *v2)
mag_w3 = np.linalg.norm(w3) 
v3 = w3/mag_w3
v3.dot(v3)

y = np.column_stack((v1,v2, v3))
print('result ')
print(y)

- Find Covariance matrix ?

x1 = np.array( [ [1.23, 2.12, 3.34, 4.5], [2.56, 2.89, 3.76, 3.95]])

print(x1)
np.cov(x1)

Define a loss function
Calculate derivative at each point
Run Gradient descent for Learning rate = 0.000001
Calculate the optimal value of w
For 60 Epoch plot cost

import matplotlib.pyplot as plt
height = np.array([167,145,170,180,189,155,163,178,173,176])
weight= np.array([83.5,72.5,85,90,94.5,77.5,81.5,89,86.5,88])

N =len(x)

learn_rate = 0.000001
error_rate =0.001
epochs = 60 
w=0
costs =[]
#Gradient Descent 
for epoch in range(epochs):
    predictions = w*height
    errors = weight - predictions
    cost = (errors **2 ).mean()
    costs.append(cost)
    gradient = -2 *(height *errors).mean()
    w-= learn_rate * gradient
    print(f"Epoch {epoch+1}/{epochs}, Cost :{cost}, w:{w}")
    

plt.title('Cost over Epochs ')
plt.xlabel('Epoch ')
plt.ylabel('Cost')
plt.plot( range(epochs), costs)
plt.show()

print(f"Optimal Value of w : {w}")


-Find singular value decomposition of 
from scipy.linalg import svd
A = np.array( ((1,2), (3 , 4), (5,6)))
A


Generate a dummy dataset using
np.random.seed(74)
X = np.random.randint(10,50,100).reshape(20,5)
Preprocess the data Preprocess the data by subtracting the mean and dividing by the standard deviation of each attribute value. The resulting data should be zero-mean with variance 1.
Compute the covriance matrix
obtain the eigenvalues and eigenvectors.
Find principle components.
X = np.random.randint(10,50,100).reshape(20,5)
mean = np.mean(X, axis =0)
std = np.std(X, axis =0)
Xcen = X-mean 
Xnorm = Xcen /std
XnormCov = np.cov(Xnorm, rowvar =False)
evals, evecs =np.linalg.eig(XnormCov)
s_index = np.argsort(evals)[::-1]
s_evals = evals[s_index]
s_devecs= evecs[s_index]
#np.transpose(s_devecs)
PCs =  np.transpose(np.dot(np.transpose(s_devecs), np.transpose(Xnorm)))

print(XnormCov)
print(evals)
print(evecs)
print(PCs)

Solve the following system $$\begin{pmatrix}

a = np.array([ [1,-1,1], [2,1, -3], [1,1,1] ])
a
b = np.array( [4,0,2])
b
x = np.linalg.solve(a,b)
print(x)
import sympy as sp
sp.init_printing()
x,y,z = sp.symbols('x,y,z')
sol = sp.solve([x+(-1)*y+ z-4, 

- Define function $f(w_1,w_2,w_3)= w_1^2+w_2^2+w_3^2$
- find$ \frac{\partial f}{\partial w_1},\frac{\partial f}{\partial w_2},\frac{\partial f}{\partial w_3}$

import sympy as sp
w1 = sp.symbols('w1')
w2 = sp.symbols('w2')
w3 = sp.symbols('w3')
def perform_op( w1, w2, w3):
    return w1*2+ w2*2+ w3*2
print(sp.diff(perform_op(w1, w2, w3), w1))
print(sp.diff(perform_op(w1, w2, w3), w2))
print(sp.diff(perform_op(w1, w2, w3), w3))
                2*x + y + (-3)*z -0 ,
                x+y+z -2 ], [x,y,z])
print(sol)

-Define the following function
$
f(x) =\begin{cases}
 & 0.5x , \text{ if }  x \leq0 \\
 & 1 ,\text{ if }  x > 0
\end{cases} \text { find }  f(-3) $ $ \text{and}$ $ f(2) $

import sympy as sp

x = sp.symbols('x')
def do_fn(x):
    if x<=0 :
        return 0.5*x
    else :
        return 1

print(do_fn(-3))
print(do_fn(2))
