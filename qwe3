import pandas as pd
import numpy as np
from sklearn.utils import shuffle
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
from nltk.stem import LancasterStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import RegexpTokenizer

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import metrics
 
from textblob import TextBlob
import matplotlib.pyplot as pltm
from tensorflow.keras import backend

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
A word cloud is a visual representation of data where the size of each word corresponds to its frequency or importance in the dataset. it helps quickly identify frequently occuring words, or patterns in textual data.

Word cloud used for sentiment analysis to identify common words for positive or negative reviews or comments. And also used in Topic analysis highlighting keywords or more frequent words from text data. Alo used in providing Social Media insights by understanding trending words, comments.

Drawbacks of Wordcloud

Word cloud ignore the relationships between words,
Since words are treated individualy without considering relation grammer or structre of sentence
Its bais towards common words.
Word cloud will show high frequency words and ignoring important words , ie. stop words like is, the, and might appear if not removed properly.


N-grams are a type of tokenization when individual sentences are split into multiple word string or characters. Its a continous sequence of N words from a given sample of source text(corpus). N-gram is a continous sequence of N items(tokens) from text or speech in NLP.
N-grams are used in applicaions like text analysis machine learning and predictive search input etc. N-grams are different types

Unigram(1-gram ) A sequence of one item ( eg., cat, dog
Bigram (2-gram) a sequence of two items ( eg., deep learning, national language,)
Trigram(3-gram) s sequence of three items. (eg., Machine learning model, data science approach)
4-gram, 5-gram longer sequences of items, for or more consecutive words.
  
Applications of N-grams in NLP

Speech recognition - N-grams play crucial role in modeling and recognizing spoken language patterns, Improving the accuracy of speech recognition systems
Machine Translation - unerstanding and translating phrases with in a broader context.
Predictive Input - N-gram used to suggest next word based of the context of sequence
Named Entity Recognition (NER) - N-grams aid in identifying and extracting named entities from text.
Search Engine Algorithm - Search engine use N-grams to index and retrieve relavant documents based on User Queries and improves the quality of search results.
##Use the data_set.csv data.The “review” column contains the review of several e-shopping items and the “Star” column contains the rating given by the customers.
data = pd.read_csv('data_set.csv')
data.head()
Add a column named ‘Emotion’. If the “Star” (i.e the rating) column is more than 3, mention Emotion as ‘positive’ otherwise ‘negative’ (Marks – 2)¶
data['Emotion']= ['positive' if x>3 else 'negative' for x in data['Star']]
Tokenize (based on space) (1 Marks)
2. Convert to lower Case (1 marks)
3. Remove Punctuations (2 Marks)
4. Remove Stopwords (2 marks)
5. Perform Lemitization (2 Marks)

#1. Tokenise based on space
review_text = data.Review
data_list = list()
for txt in review_text:
    data_list.append(word_tokenize(txt))
print(data_list)

def clean_text(text):
    # tokenizaition 
    words = word_tokenize(text)
    # remove puctuations
    words = [word for word in words if word.isalnum()]
    # remove stop words 
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in words  if word not in stop_words]
    # lemmatization 
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word, pos ='v') for word in tokens]
    return  " ".join(tokens)

# apply Text cleaning 
data["Clean_review"] = data['Review'].apply( lambda x : clean_text(str(x)))
#data['Clean_text']=data['Review'].apply(func_clean_text)
data.head(10)
df = data.copy()
df = df.drop(columns = "Review")
df.head()

df['Review'] = df['Clean_review']
df = df.drop(columns = "Clean_review")
df.head(10)
df['Review'].value_counts()

4.i. Find out the 10 most frequent words used while providing the review for `Refrigerator` & `Mobile`.  (2 +2 )
from collections import Counter

def get_top_N_words(reviews, n=10):
    all_words = " ".join(reviews).split()
    word_counts = Counter(all_words)
    return word_counts.most_common(n);
refrigerator_review = df[df["Item_Review"] =='refrigerator']['Review']
top_refrigerator_words = get_top_N_words(refrigerator_review, 10)
mobile_review = df[df["Item_Review"] =='Mobile']['Review']
print(f'10 Most frequent Word used in Review For Mobile ') 
print(top_mobile_words)
4. ii. Find out the essential/meaningfull words (top 5) which were used in the review for both Refrigerator & Mobile
refrigerator_words = [word for tokens in  refrigerator_review for word in tokens]
mobile_words = [word for tokens in  mobile_review for word in tokens]


refrigerator_set = set(refrigerator_words)
mobile_set = set(mobile_words)
common_words = refrigerator_set.intersection(mobile_set)
common_word_counts = [ (word , refrigerator_words.count(word) + mobile_words.count(word))
                      for word in common_words ]
common_word_counts.sort( key=lambda x:x[1], reverse=True )
top_essential_words = common_word_counts[:5]
print('Top 5 essential/meaningfull words  common to both  Refrigerator & Mobile : \n' , top_essential_words)
5. Build a basic logistic regression model which will categorise the review as per Item_Review
hint:Steps to use
Encode Item_Review  to numeric
Set cleaned text as feature
Segreegate train and test set
vectorize text using count-vectorizer
Build logistic regression model on train set
Measure accuracy on test set
df['label'] = df['Item_Review'].apply(lambda x: 1 if x=='Mobile' else 0)
df['label'].value_counts()
cnt_vct = CountVectorizer(max_features=1000)
cv = cnt_vct.fit_transform(df.Review)
df2 = pd.DataFrame(cv.todense(), columns= cnt_vct.get_feature_names_out())
df2['label'] = df['label']
df2.head()
X = df2.drop(columns='label')
Y = df2['label']
X.shape,Y.shape
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape
lr = LogisticRegression()
model = lr.fit(X_train, y_train)
lr.score(X_train,y_train)
lr.score(X_test,y_test)
pred = lr.predict(X_test)
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, pred))
print(classification_report(y_test, pred))
5. ii. Categorise the following review into categories using the model prepared above
processed_review1 = clean_text(Review1[0])
processed_review2 = clean_text(Review2[0])
processed_review3 = clean_text(Review3[0])
processed_review4 = clean_text(Review4[0])
reviews_vec = cnt_vct.transform(
[ processed_review1,
 processed_review2,
 processed_review3,
 processed_review4 
])
predicted_labels = model.predict(reviews_vec)
predicted_cat = [df['Item_Review'].astype('category').cat.categories[label] for label in predicted_labels]
print('Category for Reviews ')
print(f'Review1 : {predicted_cat[3]}' )
print(f'Review2 : {predicted_cat[2]}' )
print(f'Review3 : {predicted_cat[0]}' )
print(f'Review4 : {predicted_cat[1]}' )


# Question 6 : Sentiment Analysis 
### 6.i. : Assign the subjetivity response to the content
review_df = pd.DataFrame()
review_df['Review'] = df['Review']
review_df.head(5)

def getTextSubjectivity(text):
    return TextBlob(text).sentiment.subjectivity

review_df['subjectivity'] = review_df['Review'].transform(lambda x: getTextSubjectivity(str(x)))
review_df['subjectivity']
### 6.ii : Assign the polarity response to the content 
def getTextPolarity(text):
    return TextBlob(text).sentiment.polarity

review_df['polarity'] = review_df['Review'].transform(lambda x: getTextPolarity(str(x)))
review_df['polarity'
### 6. iii. : Assing sentiment to each of the Review 
hint : 
- if polarity <  0 then sentiment ->"Negative"
- if polarity == 0 then sentiment ->"Neutral"
- if polarity >  0 then sentiment ->"Neutral"

def getTextAnalysis(a):
    if a< 0:
        return 'Negative'
    elif a == 0:
        return 'Neutral'
    else:
        return 'Positive'
    
review_df['sentiment'] = review_df['polarity'].apply(getTextAnalysis)
positive = review_df[review_df['sentiment']== 'Positive']
positivePercentage = (positive.shape[0]/review_df.shape[0])*100
print(f'{positivePercentage}% of Positive Review') 
plt.figure(figsize=(10,6))
labels = review_df.groupby('sentiment').count().index.values
values = review_df.groupby('sentiment').size().values
plt.bar(labels, values)
plt.show()
-----
corpus =  data['review']
<a id="TextCleaning"> </a>
#### 1.2 Cleaning of Corpus
#from spellchecker import SpellChecker
import pandas as pd
from nltk.corpus import stopwords 
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer 
import nltk 
import re
import numpy as np 
 
def remove_punctuation(text) :
    """Remove punctuations from a string"""
    import string
    text = text.translate(str.maketrans('', '', string.punctuation))
    return text
lemmatizer = WordNetLemmatizer()
wordnet_map = {"N":wordnet.NOUN, "V":wordnet.VERB, "J":wordnet.ADJ, "R":wordnet.ADV }
def clean_text(text):
    """
        text: a string
        
        return: modified initial string
  """
    text = text.lower() # lowercase text
    #remove_html_tags(text)
    text =re.sub('<[^<]+?>', '', text)
    remove_punctuation(text)
    # remove urls
    text  = re.sub(r"https?://\S+|www\.\S+", "", text )
    # remove digits
    text= re.sub(r'[0-9]',' ',text)
    text = lemma_words(text)
    text= re.sub(r'[^\w\s]',' ',text) #Removing every thing other than space, word and hash
    text = re.sub(' +', ' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text    
    return text

data['review'] = data['review'].apply(clean_text)
cleaned_corpus = data['review']
#### 1.3 Tokenization of Corpus
from nltk.tokenize import RegexpTokenizer
token_list = list()
for word in cleaned_corpus :
    token_list.append(RegexpTokenizer('\w+').tokenize(word))
#token_list
len(token_list)
Vocabulary of Corpus
- Vocabulary of Corpus refers to unique words of corpus 
- Vocabluary count of Corpus refers to no of unique words of corpus
vocabulary = set(x for lst in token_list for x in lst)
vocabulary
vocabluary_count = len(vocabulary)
vocabluary_count
Text to Numeric Convesion
# toy example 
from sklearn.feature_extraction.text import CountVectorizer
# list of text documents
text = ["The quick brown fox jumped over the lazy dog."]
# create the transform
vectorizer = CountVectorizer()
# tokenize and build vocab
vectorizer.fit(text)
# summarize
print(vectorizer.vocabulary_)
# encode document
vector = vectorizer.transform(text)
# summarize encoded vector
print(vector.shape)
print(type(vector))
print(vector.toarray())
# encode another document
text2 = ["the puppy"]
vector = vectorizer.transform(text2)
print(vector.toarray())


from sklearn.feature_extraction.text import CountVectorizer
count_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))
# analyzer should be word/ character
#ngram_range lower and upper boundary of the range of n-values
## let'sconvert text to vectors
data_countvectors = count_vectorizer.fit_transform(cleaned_corpus)

# converting sparce to dense vector 
print(data_countvectors.shape)
print(data_countvectors[0])
print(data_countvectors[0].todense())
#TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
# tfidf toy example
# list of text documents
text = ["The quick brown fox jumped over the lazy dog.",
        "The dog.",
        "The fox"]
# create the transform
vectorizer = TfidfVectorizer()
# tokenize and build vocab
vectorizer.fit(text)
# summarize
print(vectorizer.vocabulary_)
print(vectorizer.idf_)
# encode document
vector = vectorizer.transform([text[0]])
# summarize encoded vector
print(vector.shape)

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(analyzer='word',stop_words='english', ngram_range=(1, 1))
# training tfidf on corpus
tfidf_vectorizer.fit(cleaned_corpus)

data_tfidfvectors = tfidf_vectorizer.transform(cleaned_corpus)
# can do fit_tranform at once alos like countvectorizer conversion 
print(data_tfidfvectors.shape)
print(data_tfidfvectors) 
print(data_tfidfvectors.todense().shape)
print(data_tfidfvectors[0].todense())
 Word2Vec

3.1.1. SkipGram¶
#Build the model for our tokenized corpus
import gensim 
from time import time
t = time()
# Option1 :  intializa and then  build  skip gram embedding  model
# initialize skipgram model
sg_model = gensim.models.Word2Vec(min_count=10,
                    window=5, sg = 1,
                    sample=5e-5, alpha=0.05, 
                    min_alpha=0.0005,negative=20 )

# build model vocabulary
sg_model.build_vocab(token_list)

# train the model
sg_model.train(token_list, total_examples=len(token_list), epochs=10, report_delay=1)

print('Time to build & Train Skip-Gram model vocab: {} mins'.format(round((time() - t) / 60, 2)))

## option2 : intialize &  build together
# sg_model_2 = gensim.models.Word2Vec(token_list, #Word list       
#                                min_count=10, #Ignore all words with total frequency lower than this                           
#                                workers=4, #Number of CPU Cores
#                                window=5, #Neighbours on the left and right
#                                sg = 1 # 1 skipgram , 0 cbow                          
#                               ) 

#Model size
sg_model.wv.vectors.shape
# total no of words in model : 1052 
# dimension 100
sg_model.wv['etc']
# another way to get embeeding of a word 
sg_model.wv.__getitem__('cup')
len(sg_model.wv.__getitem__('early'))
type(sg_model.wv.__getitem__('cup'))
# top 5 similar words
sg_model.wv.most_similar('early')[:5]
Continuous Bag of Words (CBOW)¶
from gensim.models import Word2Vec
from time import time
t = time()
# initialize
cbow_model = Word2Vec(min_count=2,window=2, sg = 0,sample=5e-5, alpha=0.05, min_alpha=0.0005, 
                     negative=20 )
# note just change  sg= 0 for cbow  every other argument remains same
# build model vocabulary
cbow_model.build_vocab(token_list)

# train the model
cbow_model.train(token_list, total_examples=cbow_model.corpus_count, epochs=30, report_delay=1)

print('Time to build & Train CBOW model vocab: {} mins'.format(round((time() - t) / 60, 2)))
cbow_model.wv.__getitem__('cup')
# top 5 similar words
cbow_model.wv.most_similar('early')[:5]
Global Vector
## The glove_python library is not updated lately by its developers, hence the installation may throw error,
## This code is optional
#!pip install glove_python

# # importing the glove library
# # glove toy implementtion 
# from glove import Corpus, Glove

# # creating a corpus object
# corpus = Corpus() 

# #training the corpus to generate the co occurence matrix which is used in GloVe
# corpus.fit(tokens_list_of_lists, window=3)

# #creating a Glove object which will use the matrix created in the above lines to create embeddings
# #we can set the learning rate as glove uses Gradient Descent
# glove = Glove(no_components=300, learning_rate=0.05)
# glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)
# glove.add_dictionary(corpus.dictionary)
# glove.save('glove.model')
#option1 :
from gensim.models import FastText
from time import time
t = time()

#Initialize
fasttext_model = FastText(vector_size=4, window=3, min_count=1)  # instantiate
# build 
fasttext_model.build_vocab(corpus_iterable=token_list)
# train 
fasttext_model.train(corpus_iterable=token_list, total_examples=len(token_list), epochs=10) 
print('Time to build & Train Skip-Gram model vocab: {} mins'.format(round((time() - t) / 60, 2)))
# ## option2 : intialize,  build  & train together
# fasttext_model = FastText(tokens_list, window=5, min_count=5, workers=4, sg=1)
fasttext_model.wv.__getitem__('people')
# fasttext word similarity measure
fasttext_model.wv.similarity('evacuation','shelter' )
# most similar words
fasttext_model.wv.most_similar('fire')[:5]
#fetching  pretrain wordvector
from gensim.models.keyedvectors import load_word2vec_format
pretrained_w2vec_embedding = load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
pretrained_w2vec_embedding['people']
# shape of pretrain w2vec embedding
pretrained_w2vec_embedding.vectors.shape
import gensim.downloader as api

#genism is not working properly  for Python3.7+.
#Load Glove model  pretrained word embedding  
# pip3 install --upgrade certifi
pretrained_glove_model = api.load('glove-wiki-gigaword-50')
#Size of the model
pretrained_glove_model.vectors.shape
#Embedding for word  early
pretrained_glove_model['early']
#Embedding length based on selected model - we are using 50d here.
pretrained_glove_model.vector_size
### 4.3. Pre Trained FastText

def get_coefs(word, *arr): 
    return word, np.asarray(arr, dtype='float32')
EMBEDDING_FILE = 'wiki-news-300d-1M-002.vec'
pretrained_fasttext_embedding = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in (open(EMBEDDING_FILE)))
## no of words in pretrained fasttext embedding
len(pretrained_fasttext_embedding)
pretrained_fasttext_embedding['fire']
len(pretrained_fasttext_embedding['fire'])
---------------

Text Processing Using Keras library
import tensorflow as tf
# how to decided on vocabulary size ?
# do you remember vocabluary_count was 30916 ?
desired_vocab_size = 20000 #Vocablury size
# initialzing keras tokenizer
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=desired_vocab_size, oov_token='OOV') 
 
 token_list = list()
  for word in cleaned_corpus :
      token_list.append(RegexpTokenizer('\w+').tokenize(word))
#Fit tokenizer with actual training data
tokenizer.fit_on_texts(token_list)
#Vocabulary
tokenizer.word_index

# Sequences & Pad-Sequences Get the word index for each of the word in the review data
data_sequence = tokenizer.texts_to_sequences(token_list)
data_sequence
# Pad Sequences ->to make all list of same length 
max_review_length = 200
data_sequence_padded = tf.keras.preprocessing.sequence.pad_sequences(data_sequence,
                                                        maxlen=max_review_length,padding='pre')
type(data_sequence_padded)
data_sequence_padded.shape
Embedding Layer(with Embedding Matrix)

embedding_vector_length = dimension = 100
embedding_matrix = np.zeros((desired_vocab_size + 1, embedding_vector_length))
# create Embedding Matrix Load word vectors for each word in our vocabulary from from Glove pre-trained model

for word, i in sorted(tokenizer.word_index.items(), key=lambda x:x[1]):
    if i > (desired_vocab_size+1):
        break
    try:
        embedding_vector = pretrained_glove_model[word] #Reading word's embedding from Glove model for a given word
        embedding_matrix[i] = embedding_vector
    except:
        pass
Model Definition
#Initialize model
tf.keras.backend.clear_session()
model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(desired_vocab_size + 1, #Vocablury size
                                    embedding_vector_length, #Embedding size
                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model
                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.
                                    input_length=max_review_length) #Number of words in each review
          )
model.output
model.summary()
Use Case 1 : Building Sentiment Classification Model using TFIDF vectorizer and XGBoost
# we have already vectorize review data with 
data_tfidfvectors.shape
X = data_tfidfvectors
y = data['sentiment']
import xgboost as xgb
xgb_clf = xgb.XGBClassifier()
from sklearn.model_selection import cross_val_score
print(cross_val_score(xgb_clf, X, y, cv=5,scoring='accuracy'))
Use Case 2 : Building Sentiment Classification Model using GloVe Pre-trained Embeeding and DNN
#Initialize model

#Initialize model
# building on to previous model 
tf.keras.backend.clear_session()
dnn_model = tf.keras.Sequential()
dnn_model.add(tf.keras.layers.Embedding(desired_vocab_size + 1, #Vocablury size
                                    embedding_vector_length, #Embedding size
                                    weights=[embedding_matrix], #Embeddings taken from GloVe pre-trained
                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.
                                    input_length=max_review_length) #Number of words in each review
          )
 

#Flatten the data as we will use Dense layers
dnn_model.add(tf.keras.layers.Flatten())


#Add Hidden layers (Dense layers)
 
dnn_model.add(tf.keras.layers.Dense(100, activation='relu', input_shape=()))
dnn_model.add(tf.keras.layers.BatchNormalization())
dnn_model.add(tf.keras.layers.Dense(50, activation='relu'))
dnn_model.add(tf.keras.layers.BatchNormalization())
dnn_model.add(tf.keras.layers.Dense(25, activation='relu'))
dnn_model.add(tf.keras.layers.Dropout(0.25))

# output layer
dnn_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

#Compile the model
dnn_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

dnn_model.summary()

train Model 
target = data['sentiment'] 

my_callbacks = [tf.keras.callbacks.EarlyStopping(patience=5) ]

dnn_model.fit(data_sequence_padded,target,
          epochs=5,
          batch_size=32, validation_split=0.2, callbacks= my_callbacks)
Looks like TF-IDF model performance is better than DNN 
=========================

df17 = pd.read_csv(path17) ; 
df1 = df17.copy()
df1 = df1.rename(columns=({'Text':'Review'}))
df1=df1.drop(columns=['Id', 'Unnamed: 0'] , axis=1)
#df1 = df1.drop_duplicates(())
df1.head(2)

############## STEP 0 : Clean Text, Preprocessing  ################

############# 1 Clean the Text ##################
stop_words=set(stopwords.words('english'))
stop_words.discard('no')
stop_words.discard('not')
def func_clean_text(text):
    text   = text.lower()
    text   = unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8')
    text   = re.sub(r'http\S+|www\S+', '', text)  # Remove URLs
    text   = re.sub(r'\d','',text)                # Remove digits
    text   = text.translate(str.maketrans('','',string.punctuation))    # Remove punctuation
    tokens = [word for word in nltk.word_tokenize(text)                            if word not in stop_words]
    #tokens = [nltk.PorterStemmer().stem(word) for word in nltk.word_tokenize(text) if word not in stop_words]
    #tokens = [nltk.stem.WordNetLemmatizer().lemmatize(word) for word in nltk.word_tokenize(text)         if word not in stop_words]
    text   = ' '.join(tokens)                     
    text   = re.sub(r'\s+',' ',text).strip()       # Eliminate multiple spaces
    return text
df1['Clean_text']=df1['Review'].apply(func_clean_text)


################# 2  Polarity & Subjectivity & Sentiment #####
def getTextSubjectivity(txt):
    return TextBlob(txt).sentiment.subjectivity

def getTextPolarity(txt):
    return TextBlob(txt).sentiment.polarity

def getTextSentiment(a):
    if a < 0:
        return "Negative"
    elif a == 0:
        return "Neutral"
    else:
        return "Positive"

df1['Polarity'    ]   = df1['Clean_text'].transform(lambda x: getTextPolarity(str(x)))
df1['Subjectivity']   = df1['Clean_text'].transform(lambda x: getTextSubjectivity(str(x)))
df1['Sentiment'   ]   = df1['Polarity'].apply(getTextSentiment)
df1['ScoreEncoded']   = df1['Score']-1
df1['ScoreEncoded']   = df1['ScoreEncoded'].astype(int)
display(df1.head(2))


#df1['Emotion'] = df1['Polarity'].apply(lambda x: 'Positive' if x>3 else 'Negative') ; #data.head()

############ 3 Questions on Sentiment Analysis ########
a= df1[df1['Sentiment'] == 'Positive']
b = a.shape[0]/(df1.shape[0])*100
print(b , " % of positive Review")

#Visualize the frequency distribution of the sentiment on each content
plt.figure(figsize = (4,2))
labels = df1.groupby('Sentiment').count().index.values
values = df1.groupby('Sentiment').size().values
plt.bar(labels, values); plt.show();

#################### 4 Similar words to food [CBOW, Word to vec] ##########################
df_tokenize = df1['Clean_text'].apply(nltk.word_tokenize)          ## tokenized words
# df_tokenize = [text.split() for text in df1['Clean_text']]

model=Word2Vec(df_tokenize, window=5,sg=1,vector_size=100,min_count=1,epochs=300) ;  
similar_words = model.wv.most_similar(positive=['food'],topn=5) ; print(similar_words)


############## STEP 1 : Feature Engineering  ###############

#TF-IDF values are not raw counts but normalized weights,
############## 5 Count Vectoriser : Top 6 most frequent words  ################
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

#model          = CountVectorizer()                       # common words dominate    #SPAM/HAM
model           = TfidfVectorizer()                       # reduce importance common words dominate   
model_fit       = model.fit_transform(df1['Clean_text'])  # tv.toarray()[:2]
vocabulary      = model.get_feature_names_out()           # Retrieve the feature names (words)
term_importance = model_fit.sum(axis=0)                   # Sum the occurrences of each word across all documents
word_freq_df    = pd.DataFrame({'Word': vocabulary, 'Frequency': term_importance .A1})
word_freq_df    = word_freq_df.sort_values(by='Frequency', ascending=False)

model_shape     = word_freq_df.shape           ; print(model_shape)      # top 5 word
top_5_words     = word_freq_df.head(6) ; print(top_5_words)      # top 5 word
importance_of_food = word_freq_df[word_freq_df['Word'] =='food'] # checking importance of word "food"
print(importance_of_food)

####################### Section C ################################
####################### Train Test ###############################
# Convert text into numerical representations (CountVectorizer & TF-IDF)
vectorizer_count = CountVectorizer()
vectorizer_tfidf = TfidfVectorizer()

X_count = vectorizer_count.fit_transform(df1['Clean_text'])
X_tfidf = vectorizer_tfidf.fit_transform(df1['Clean_text'])
y = df1['Review']

# Split dataset (Ensuring stratified sampling)
X_train_count, X_test_count, y_train, y_test = train_test_split(X_count, df1['Sentiment'] , test_size=0.25, stratify= df1['Sentiment'], random_state=42)
X_train_tfidf, X_test_tfidf,  _     , _      = train_test_split(X_tfidf, df1['Sentiment'] , test_size=0.25, stratify= df1['Sentiment'], random_state=42)

####################### LR, Naive Bayas ###############################
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

# Logistic Regression Model
def train_model(model_class, X_train, X_test, y_train, y_test, model_type):
    model = model_class
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\n{model_type} {model_class} Accuracy:", accuracy)
    return model  

# Train & Evaluate Logistic Regression
train_model(LogisticRegression(), X_train_count, X_test_count, y_train, y_test, "Count-Vectorized")
train_model(LogisticRegression(), X_train_tfidf, X_test_tfidf, y_train, y_test, "TF-IDF")
train_model(MultinomialNB()     , X_train_count, X_test_count, y_train, y_test, "Count-Vectorized")
#train_model(MultinomialNB()    , X_train_tfidf, X_test_tfidf, y_train, y_test, "TF-IDF")

#---------------------------------------------------------------------------------------------------------------------------------------------------




import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Flatten, BatchNormalization, Bidirectional, SpatialDropout1D
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import regularizers
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Load dataset
df  = pd.read_csv('/content/drive/MyDrive/dataset/july24_data_set.csv')
df = df.rename(columns=({'Text':'Review'}))
df = df.drop(columns=['Id', 'Unnamed: 0'] , axis=1)
#df = df.drop_duplicates(())


############# 1 Clean the Text ##################
stop_words=set(stopwords.words('english'))
stop_words.discard('no')
stop_words.discard('not')
def func_clean_text(text):
    text   = text.lower()
    text   = unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8')
    text   = re.sub(r'http\S+|www\S+', '', text)  # Remove URLs
    text   = re.sub(r'\d','',text)                # Remove digits
    text   = text.translate(str.maketrans('','',string.punctuation))    # Remove punctuation
    tokens = [word for word in nltk.word_tokenize(text)                            if word not in stop_words]
    #tokens = [nltk.PorterStemmer().stem(word) for word in nltk.word_tokenize(text) if word not in stop_words]
    #tokens = [nltk.stem.WordNetLemmatizer().lemmatize(word) for word in nltk.word_tokenize(text)         if word not in stop_words]
    text   = ' '.join(tokens)
    text   = re.sub(r'\s+',' ',text).strip()       # Eliminate multiple spaces
    return text
df['Clean_text']=df['Review'].apply(func_clean_text)

# Tokenization
print("Tokenizing text...")
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(df['Clean_text'])
sequences = tokenizer.texts_to_sequences(df['Clean_text'])

# Sequence Padding
max_len = max([len(seq) for seq in sequences])
embed_dim = 100  # Own embedding size
vocab_size = len(tokenizer.word_index) + 1
print(f'The max sentence length: {max_len}, Unique words: {vocab_size}')

# Padding Sequences
padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')

# Label Encoding
y = df['Score']
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Splitting Data
xtrain, xtest, ytrain, ytest = train_test_split(padded_sequences, y_encoded, test_size=0.3, random_state=48, stratify=y_encoded)

# Encoding Labels for LSTM
ytrain_en = to_categorical(ytrain, num_classes=len(np.unique(y_encoded)))
ytest_en = to_categorical(ytest, num_classes=len(np.unique(y_encoded)))

# Building LSTM Model
print("Building LSTM Model...")
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=max_len))

model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))

#model.add(LSTM(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01), return_sequences=True))
#model.add(Dropout(0.2))
#model.add(LSTM(32, activation='tanh'))
#model.add(Flatten())

model.add(Dense(64, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dense(len(np.unique(y_encoded)), activation='softmax'))


# Compile Model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train Model with Early Stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = model.fit(xtrain, ytrain_en, batch_size=32, epochs=10, validation_data=(xtest, ytest_en), callbacks=[early_stopping])

# Predict
test_predictions = model.predict(xtest)
ypred_pr = np.argmax(test_predictions, axis=1)

# Model Evaluation
print(f"Length of X_test: {len(xtest)}")
print(f"Length of y_test: {len(ytest)}")
print(f"Length of predictions: {len(ypred_pr)}")
print(f"Model Accuracy: {accuracy_score(ytest, ypred_pr) * 100:.2f}%")

# Confusion Matrix & Classification Report
plt.figure(figsize=(6, 5))
sns.heatmap(confusion_matrix(ytest, ypred_pr), annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_.astype(str), yticklabels=label_encoder.classes_.astype(str))
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

print("Classification Report:")
print(classification_report(ytest, ypred_pr, target_names=label_encoder.classes_.astype(str)))

-----
LSTM Drawbacks & Need for Transformer

LSTMs when combined with an input Neural Word Embedding mechanism such as Word2Vec, were  able to achieve the best performance across various NLP tasks upto 2016-17 
- with comfortably  better performance than the earlier statistical text encoding ideas such as TF-IDF. LSTMs and their  variants were used by every major industry player, 
for applications ranging from Alexa’s Voice  Assistant, to Google Search, Google Translate, and even the Autosuggest in Smartphone Keyboards.

However, LSTMs also had some key limitations - especially relating to their Sequential Text  Processing mechanism - processing one word at a 
time could not computationally scale well to  large documents and corpuses of text, and there was a need for a more parallelizable computing  
architecture that could accept an entire corpus at once and send each word / token of text through its own pathway independent of the rest.
 There was also a realization that despite the smart hack of  creating a Gated Mechanism, LSTMs still couldn’t resolve the fundamental RNN problem of 
 remembering the very long-range dependencies that are common in flowing paragraphs of text
 import re
import unicodedata
import nltk
import string
import gensim
from nltk.corpus import stopwords
from collections import Counter
from gensim.models import Word2Vec
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from keras.utils import to_categorical

nltk.download('stopwords')
nltk.download('punkt')

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from nltk.tokenize import word_tokenize

import unicodedata
from nltk.corpus import stopwords
from gensim.parsing.preprocessing import remove_stopwords
import pandas as pd
import numpy as np
import string

from time import time
from gensim.models import Word2Vec

from transformers import pipeline

# Load the dataset
df = pd.read_csv('reviews.csv')
print(df.head())
# Remove duplicates
# Remove duplicate reviews
df.drop_duplicates(subset=['Review'], keep='first')
df.head()

# Get English stopwords
stop_words = set(stopwords.words('english'))

# Function to clean text
def clean_text(text):
    # Remove punctuation
    #use translate function
    # hint: refer string.punctuation to fetch all punctuations
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Remove stopwords
    words = text.split()
    text = ' '.join([word for word in words if word not in stop_words])
    
    # you need to import word_tokenize
    from nltk.tokenize import word_tokenize # import word_tokenize
    return text

# Apply the cleaning function

df['Cleaned_Review'] = df['Review'].apply(clean_text)
df.head()

# Word Embedding
# Initialize TF-IDF Vectorizer
vectorizer = TfidfVectorizer()

tfidf_matrix = vectorizer.fit_transform(df['Cleaned_Review'])
# Display the shape of the TF-IDF matrix
tfidf_shape = tfidf_matrix.shape
# tfidf vocabulaory
#Hint: get feature names
tfidf_vocabulary = vectorizer.get_feature_names_out()
# Display results
tfidf_shape, tfidf_vocabulary[:10]  # Showing first 10 words from the vocabulary for preview
# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, df['Sentiment'], test_size=0.2, random_state=42)
# Train Naive Bayes classifier
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train, y_train)

# Evaluate the model
y_pred = nb_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
# Display the accuracy
accuracy

# Tokenize the text
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(df['Cleaned_Review'])

#Hint : Use texts_to_sequences function

# Convert texts to sequences
X_seq = tokenizer.texts_to_sequences(df['Cleaned_Review'])

# Pad sequences to ensure equal length
X_padded = pad_sequences(X_seq, maxlen=100, padding='post')
# Encode the labels
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(df['Sentiment'])


# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)

# Build the LSTM model
model = Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=100),
    SpatialDropout1D(0.2),
    LSTM(100, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='softmax')  # 3 classes: Positive, Neutral, Negative
])

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=5, batch_size=16, validation_data=(X_test, y_test), verbose=1)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
# Display the accuracy
accuracy

----
# Load english language model
nlp = spacy.load('en_core_web_sm')

# Define custom tokenizer (remove stop words and punctuation and apply lemmatization)
def custom_tokenizer(doc):
    return [t.lemma_ for t in nlp(doc) if (not t.is_punct) and (not t.is_stop)]
The tokenizer is then passed as a callback function inside the count vectorizer. We also set binary equal to true to produce a binary bag-of-words.

# Pass tokenizer as callback function to countvectorizer
vectorizer = CountVectorizer(tokenizer=custom_tokenizer, binary=True)

# Fit vectorizer to corpus
bow = vectorizer.fit_transform(corpus)
We can view the resulting vocabulary and matrix the same way as before.

# Vocabulary
vectorizer.vocabulary_

# Dense matrix representation
bow.toarray()
# Sparse slice
print(bow[:,0:4])
# Cosine similarity using numpy
def cosine_sim(a,b):
    return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))
# Similarity between two documents
print(corpus[1])
print(corpus[3])
print(f'Similarity score: {cosine_sim(bow[1].toarray().squeeze(),bow[3].toarray().squeeze()):.3f}')
# Similarity between two documents
print(corpus[0])
print(corpus[2])
print(f'Similarity score: {cosine_sim(bow[0].toarray().squeeze(),bow[2].toarray().squeeze()):.3f}')
# cosine_similarity takes either array-likes or sparse matrices
print(cosine_similarity(bow))

Finally, we will build a bag-of-words matrix using n-grams. To do this, we can pass the ngram_range parameter in countvectorizer. It takes in a tuple, with the first entry indicating the minimum chunk size and the second entry indicating the maximum chunk size.

# Unigrams and bigrams with ngram_range=(1,2)
vectorizer = CountVectorizer(tokenizer=custom_tokenizer, lowercase=False, binary=True, ngram_range=(1,2))

# Fit vectorizer to corpus
unibigrams = vectorizer.fit_transform(corpus)

# Print vocabulary size
print(f'Size of vocabulary: {len(vectorizer.get_feature_names_out())}')

# Print vocabulary
print(vectorizer.vocabulary_

 Only bigrams with ngram_range=(2,2)
vectorizer = CountVectorizer(tokenizer=custom_tokenizer, lowercase=False, binary=True, ngram_range=(2,2))

# Fit vectorizer to corpus
bigrams = vectorizer.fit_transform(corpus)

# Print vocabulary size
print(f'Size of vocabulary: {len(vectorizer.get_feature_names_out())}')

# Print vocabulary
print(vectorizer.vocabulary_)


==============
#Hugging Face Pipeline
pipelines provides a high-level, easy to use,API for doing inference over a variety of downstream-tasks, including:
Sentence Classification _(Sentiment Analysis)_: Indicate if the overall sentence is either positive or negative, i.e. binary classification task or logitic regression task.
Token Classification (Named Entity Recognition, Part-of-Speech tagging): For each sub-entities (*tokens*) in the input, assign them a label, i.e. classification task.
Question-Answering: Provided a tuple (question, context) the model should find the span of text in content answering the question.
Mask-Filling: Suggests possible word(s) to fill the masked input with respect to the provided context.
Summarization: Summarizes the input article to a shorter article.
Feature Extraction: Maps the input to a higher, multi-dimensional space learned from the data.
Pipelines encapsulate the overall process of every NLP process:

Tokenization: Split the initial input into multiple sub-entities with ... properties (i.e. tokens).
Inference: Maps every tokens into a more meaningful representation.
Decoding: Use the above representation to generate and/or extract the final output for the underlying task.
The overall API is exposed to the end-user through the pipeline() method with the following structure:
from transformers import pipeline
#Initialize Sentiment analysis pipeline
model_id = "cardiffnlp/twitter-roberta-base-sentiment-latest"
nlp_sentence_classif = pipeline('sentiment-analysis',model=model_id)
nlp_sentence_classif('Such a nice weather outside !')
#Feed the text for classification
import time
start_time = time.time()
print(nlp_sentence_classif('Such a nice weather outside !'))
print('Time taken', time.time() - start_time)

#Feed the text for classification
nlp_sentence_classif('The audio was good. But camera was not so good')

#Feed the text for classification
nlp_sentence_classif('That was not a nice movie')

#Initialize Sentiment analysis pipeline
#nlp_sentence_classif = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment')
#Initialize the pipeline for NER
nlp_token_class = pipeline('ner')
nlp_token_class.model
#Feed the text for NER
nlp_token_class('Hugging Face is a French company based in New-York.')

#Initialize pipeline for Question Answering
nlp_qa = pipeline('question-answering')
#Feed a Paragraph and ask questions from the same
paragraph = 'Hugging Face is a French company based in New-York.'
question = 'Where is Hugging Face based?'
nlp_qa(context=paragraph, question=question)
nlp_qa(context=article, question='Who is the CEO of Google?')
Text Generation - Mask Filling
#Initialize the pipeline
nlp_fill = pipeline('fill-mask')
#Provide a text with MASKed words that need to be filled up
nlp_fill('Hugging Face is a French company based in ' + nlp_fill.tokenizer.mask_token)
nlp_fill('In Machine Learning, Machine learns ' + nlp_fill.tokenizer.mask_token + ' and biases.')
#initialize pipeline
summarizer = pipeline('summarization')
summarizer(LONG_BORING_TENNIS_ARTICLE)
text_generator = pipeline("text-generation")
output = text_generator("The main reason for India's loss is ", )
print(output[0]['generated_text'])
import numpy as np
nlp_features = pipeline('feature-extraction')
output = nlp_features('Hugging Face is a French company based in Paris')
np.array(output).shape   # (Samples, Tokens, Vector Size)
----
Generative AI focuses on generating new data instances that resemble a given dataset. It learns the underlying data distribution to create new, realistic data points. Prominent generative AI models include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).

Core principles of generative AI
Generative AI models aim to learn the joint probability distribution of input data. They generate new data samples by modelling the entire data distribution, often requiring extensive computational resources and handling unlabeled data effectively.

How generative AI works
Generative AI works by training on existing data and learning its underlying distribution. For instance, GANs consist of two neural networks, a generator and a discriminator, which work together to create realistic images. VAEs, on the other hand, encode input data into a latent space and then decode it back to generate new data points.


What is discriminative AI?
Discriminative AI focuses on classifying existing data by learning the boundaries between different classes. It models the conditional probability distribution, making accurate predictions based on input data. Examples include Logistic Regression and Support Vector Machines (SVMs).


Core principles of discriminative AI
Discriminative models concentrate on the decision boundary between different data classes. They excel in supervised machine learning tasks, such as classification and regression, by focusing on labelled data to make predictions.

How discriminative AI works
Discriminative AI works by learning from labelled training data to identify patterns and make predictions. For example, SVMs find the optimal hyperplane that separates different classes, while logistic regression models the probability of a data point belonging to a particular class. While discriminative AI focuses on classifying data, the future of generative AI is advancing by enabling models to create new data based on learned patterns.

Applications of generative AI
Art and music creation
Generative AI creates unique artworks and music compositions. Tools like Jukedeck and Amper Music use these models to produce innovative and creative content, transforming artistic processes.

Synthetic data generation
Generative models create synthetic data for training other AI models, enhancing both the diversity and quantity of training datasets, which improves model robustness and performance.

Content creation
Generative AI generates text, images, and videos, revolutionising content creation in industries such as marketing and entertainment by automating and enhancing creative processes.


Applications of discriminative AI
Spam filtering
Discriminative AI models are crucial for email spam filtering systems. For example, Gmail uses these models to accurately classify and filter out spam emails, ensuring a cleaner inbox.

Facial recognition
Facial recognition systems utilise discriminative AI to identify individuals, enhancing security across various applications, from unlocking devices to surveillance systems in public spaces.

Credit scoring
In finance, discriminative AI models assess credit scores and loan eligibility, providing accurate and fair decision-making, which is vital for financial institutions and customers.

Creativity and innovation
Generative AI drives creativity by producing new and unique data, fostering innovation in fields like art and design, and pushing the boundaries of traditional creative processes.

Data augmentation
Generative models enhance training datasets, improving the performance of other AI models through increased data diversity, which is crucial for developing robust and accurate AI systems.

Handling unlabeled data
Generative AI excels at learning from unlabeled data, making it versatile for various unsupervised learning tasks, thus expanding its applicability across different domains.


Advantages of discriminative AI
High accuracy
Discriminative AI provides high accuracy in classification tasks, making it reliable for applications that require precise predictions, such as medical diagnoses and fraud detection.

Simplicity and speed
These models are simpler and faster to train compared to generative models, offering efficiency in practical applications and quicker deployment in real-world scenarios.

Robustness
Discriminative AI models are robust against outliers and noisy data, ensuring reliable performance across diverse environments and maintaining accuracy in challenging conditions
