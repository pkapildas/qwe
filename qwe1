Convert the following matrix to an orthogonal matrix using Gram Schmidt Process?
A = np.array( ((1,1,1), (-1, 0, 1), (1, 1, 2)))
A
u1, u2, u3 = A[:,0], A[:,1], A[:,2]
w1 =u1
# find magnitude of u1
mag_w1 = np.linalg.norm(w1) 
# do normalization 
v1= w1/mag_w1
print(v1.dot(v1))
# remove vector projection of u2 in v1 from u2
w2 =u2- (u2.dot(v1)) *v1 
mag_w2 = np.linalg.norm(w2) 
v2 = w2/mag_w2
v2.dot(v2)
# remove vector projection of u3 in v1 and v2 from u3
w3 = u3- ((u3.dot(v1)) *v1) -  ((u3.dot(v2)) *v2)
mag_w3 = np.linalg.norm(w3) 
v3 = w3/mag_w3
v3.dot(v3)

y = np.column_stack((v1,v2, v3))
print('result ')
print(y)

- Find Covariance matrix ?

x1 = np.array( [ [1.23, 2.12, 3.34, 4.5], [2.56, 2.89, 3.76, 3.95]])

print(x1)
np.cov(x1)

Define a loss function
Calculate derivative at each point
Run Gradient descent for Learning rate = 0.000001
Calculate the optimal value of w
For 60 Epoch plot cost

import matplotlib.pyplot as plt
height = np.array([167,145,170,180,189,155,163,178,173,176])
weight= np.array([83.5,72.5,85,90,94.5,77.5,81.5,89,86.5,88])

N =len(x)

learn_rate = 0.000001
error_rate =0.001
epochs = 60 
w=0
costs =[]
#Gradient Descent 
for epoch in range(epochs):
    predictions = w*height
    errors = weight - predictions
    cost = (errors **2 ).mean()
    costs.append(cost)
    gradient = -2 *(height *errors).mean()
    w-= learn_rate * gradient
    print(f"Epoch {epoch+1}/{epochs}, Cost :{cost}, w:{w}")
    

plt.title('Cost over Epochs ')
plt.xlabel('Epoch ')
plt.ylabel('Cost')
plt.plot( range(epochs), costs)
plt.show()

print(f"Optimal Value of w : {w}")


-Find singular value decomposition of 
from scipy.linalg import svd
A = np.array( ((1,2), (3 , 4), (5,6)))
A


Generate a dummy dataset using
np.random.seed(74)
X = np.random.randint(10,50,100).reshape(20,5)
Preprocess the data Preprocess the data by subtracting the mean and dividing by the standard deviation of each attribute value. The resulting data should be zero-mean with variance 1.
Compute the covriance matrix
obtain the eigenvalues and eigenvectors.
Find principle components.
X = np.random.randint(10,50,100).reshape(20,5)
mean = np.mean(X, axis =0)
std = np.std(X, axis =0)
Xcen = X-mean 
Xnorm = Xcen /std
XnormCov = np.cov(Xnorm, rowvar =False)
evals, evecs =np.linalg.eig(XnormCov)
s_index = np.argsort(evals)[::-1]
s_evals = evals[s_index]
s_devecs= evecs[s_index]
#np.transpose(s_devecs)
PCs =  np.transpose(np.dot(np.transpose(s_devecs), np.transpose(Xnorm)))

print(XnormCov)
print(evals)
print(evecs)
print(PCs)

Solve the following system $$\begin{pmatrix}

a = np.array([ [1,-1,1], [2,1, -3], [1,1,1] ])
a
b = np.array( [4,0,2])
b
x = np.linalg.solve(a,b)
print(x)
import sympy as sp
sp.init_printing()
x,y,z = sp.symbols('x,y,z')
sol = sp.solve([x+(-1)*y+ z-4, 

- Define function $f(w_1,w_2,w_3)= w_1^2+w_2^2+w_3^2$
- find$ \frac{\partial f}{\partial w_1},\frac{\partial f}{\partial w_2},\frac{\partial f}{\partial w_3}$

import sympy as sp
w1 = sp.symbols('w1')
w2 = sp.symbols('w2')
w3 = sp.symbols('w3')
def perform_op( w1, w2, w3):
    return w1*2+ w2*2+ w3*2
print(sp.diff(perform_op(w1, w2, w3), w1))
print(sp.diff(perform_op(w1, w2, w3), w2))
print(sp.diff(perform_op(w1, w2, w3), w3))
                2*x + y + (-3)*z -0 ,
                x+y+z -2 ], [x,y,z])
print(sol)

-Define the following function
$
f(x) =\begin{cases}
 & 0.5x , \text{ if }  x \leq0 \\
 & 1 ,\text{ if }  x > 0
\end{cases} \text { find }  f(-3) $ $ \text{and}$ $ f(2) $
sol
import sympy as sp

x = sp.symbols('x')
def do_fn(x):
    if x<=0 :
        return 0.5*x
    else :
        return 1

print(do_fn(-3))
print(do_fn(2))

----
(b) function optimization problem  or

PCA step by step on a iris dataset and capture 95 % variance(15 marks)
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

# Step 1: Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names

# Step 2: Standardize the Data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 3: Compute the Covariance Matrix
cov_matrix = np.cov(X_scaled.T)

# Step 4: Compute the Eigenvectors and Eigenvalues
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Step 5: Select Principal Components
# Sort eigenvalues and eigenvectors in descending order
sorted_indices = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[sorted_indices]
eigenvectors = eigenvectors[:, sorted_indices]

# Select the top k eigenvectors based on the explained variance ratio
total_variance = np.sum(eigenvalues)
explained_variance_ratio = eigenvalues / total_variance
cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)
k = np.argmax(cumulative_explained_variance_ratio >= 0.95) + 1

# Step 6: Project Data onto Principal Components
projection_matrix = eigenvectors[:, :k]
X_pca = X_scaled.dot(projection_matrix)

# Display the explained variance ratio and cumulative explained variance ratio
print("Explained Variance Ratio:", explained_variance_ratio[:k])
print("Cumulative Explained Variance Ratio:", cumulative_explained_variance_ratio[:k])
----
3. Consider the data given below and fit a linear regression line y=ax+b using gradient descent.

X
0
0.4
0.6
1

Y
0
1
0.48
0.95

Initialize the weights a and b to 0.8, 0.2 respectively.

Update the weights such that the error is minimum using gradient descent.

Use the function sum of squared errors $ y−y^2$ where $y^$ is the y-predicted value and y is the actual given y.

Plot the linear regression line after updating the values of a and b in two iterations.

---
import numpy as np
import matplotlib.pyplot as plt

# Define the dataset
X = np.array([0, 0.4, 0.6, 1])
Y = np.array([0, 1, 0.48, 0.95])

# Define initial weights
a = 0.8
b = 0.2

# Define learning rate and number of iterations
learning_rate = 0.1
iterations = 2

# Define function to calculate sum of squared errors
def sum_squared_errors(Y, Y_pred):
    return np.sum((Y - Y_pred) ** 2)

# Perform gradient descent
for i in range(iterations):
    # Calculate predicted values of y
    Y_pred = a * X + b

    # Calculate gradients
    gradient_a = -2 * np.sum((Y - Y_pred) * X)
    gradient_b = -2 * np.sum(Y - Y_pred)

    # Update weights
    a -= learning_rate * gradient_a
    b -= learning_rate * gradient_b

    # Print the updated weights and sum of squared errors
    print(f"Iteration {i+1}: a = {a}, b = {b}, Sum of Squared Errors = {sum_squared_errors(Y, Y_pred)}")

# Plot the original data points
plt.scatter(X, Y, color='blue', label='Original data points')

# Plot the linear regression line
plt.plot(X, a*X + b, color='red', label='Linear regression line')

# Add labels and legend
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Linear Regression using Gradient Descent')
plt.legend()

# Show plot
plt.grid(True)
plt.show()
====Imaage

from PIL import Image, ImageFilter, ImageOps
import numpy as np
import matplotlib.pyplot as plt

# Step 1: Load the image
image_path = "/content/drive/MyDrive/MF_sagarika_changes/MF ESA Jan24 model paper/Lena.jpg"
image = Image.open(image_path)

# Step 2: Display the original image
plt.figure(figsize=(10, 10))
plt.subplot(3, 3, 1)
plt.imshow(image)
plt.title("Original Image")
plt.axis("off")

# Step 3: Apply grayscale transformation
gray_image = ImageOps.grayscale(image)
plt.subplot(3, 3, 2)
plt.imshow(gray_image, cmap="gray")
plt.title("Grayscale Image")
plt.axis("off")

# Step 4: Apply Gaussian blur filter
blurred_image = gray_image.filter(ImageFilter.GaussianBlur(radius=5))
plt.subplot(3, 3, 3)
plt.imshow(blurred_image, cmap="gray")
plt.title("Blurred Image")
plt.axis("off")

# Step 5: Apply edge detection (Sobel filter)
sobel_image = gray_image.filter(ImageFilter.FIND_EDGES)
plt.subplot(3, 3, 4)
plt.imshow(sobel_image, cmap="gray")
plt.title("Edge Detection")
plt.axis("off")

# Step 6: Rotate the image by 45 degrees
rotated_image = image.rotate(45)
plt.subplot(3, 3, 5)
plt.imshow(rotated_image)
plt.title("Rotated Image")
plt.axis("off")

# Step 7: Scale the image
scaled_image = image.resize((int(image.width * 1.5), int(image.height * 1.5)))
plt.subplot(3, 3, 6)
plt.imshow(scaled_image)
plt.title("Scaled Image")
plt.axis("off")

# Step 8: Translate the image
translated_image = image.transpose(Image.FLIP_LEFT_RIGHT)
plt.subplot(3, 3, 7)
plt.imshow(translated_image)
plt.title("Translated Image")
plt.axis("off")

plt.tight_layout()
plt.show()
-------
A = np.array([ [5,6,2], [4,7, 1], [0,3,1] ])
A
B = np.array([ [1,-2,1], [4,4, 5], [5,5,1] ])
B
C = np.array([ [1,0,1], [4,1, 0], [2,0,1] ])
C
# (AB)T = AT.BT
AB  = A.dot(B)
print(AB)
AT=A.T
print(AT)
BT= B.T
print(BT)
BTAT == BT.dot(AT)
print(BTAT)
AB_T = AB.T
AB_T == BTAT
####
Ainv = np.linalg.inv(A)
print(Ainv)
Binv = np.linalg.inv(B)
print(Binv)
AB_inv = np.linalg.inv(AB)
print(AB_inv)
BinvAinv = Binv.dot(Ainv)
print(BinvAinv)
ABinv == Binv.dot(Ainv)

#####
BplusC = B+C
print(BplusC)
A_BplusC = A.dot(BplusC)
print(A_BplusC)
AB = A.dot(B)
print(AB)
AC = A.dot(C)
print(AC)
A_BplusCAC = AB +AC
print(A_BplusCAC)
A_BplusC==A_BplusCAC

#### AT =AT
AT =A.T
print(AT)
AT_inv =  np.linalg.inv(AT)
print(AT_inv)
Ainv = np.linalg.inv(A)
print(Ainv)
AinvT= Ainv.T
AT_inv ==AinvT
##
AT=  A.T
print(AT)
AA = A.dot(A)
print(AA)
ATAT = AT.dot(A).T
print(ATAT)
ATAT == AT.dot(A)
###

Cin = np.linalg.inv(C)
print(Cin)
DetCin = np.linalg.det(Cin)
print(DetCin)
DetC = np.linalg.det(C)
print(DetC)
onebyDetC = 1/DetC
print(onebyDetC)
DetCin == onebyDetC

(2) Find the critical points of the function  f(x)=x5−5x4+5x3−1
import sympy as sp

# Define the variable
x = sp.symbols('x')

# Define the function
f = x**5 - 5*x**4 + 5*x**3 - 1

# Calculate the derivative of the function
f_prime = sp.diff(f, x)

# Find the critical points by solving f'(x) = 0
critical_points = sp.solve(f_prime, x)

# Output the critical points
print("Critical points:", critical_points)

(3) The revenue generation function of an IT company is 3000x - 20x2 + 200   rupees where x is the number of employees. Find out the marginal revenue generation when 10 employees are hired.
# Define the revenue generation function
revenue_function = 3000*x - 20*x**2 + 200

# Calculate the derivative of the revenue generation function
marginal_revenue_function = sp.diff(revenue_function, x)

# Evaluate the marginal revenue function at x = 10
marginal_revenue_at_10_employees = marginal_revenue_function.subs(x, 10)

# Output the result
print("Marginal revenue generation when 10 employees are hired:", marginal_revenue_at_10_employees, "rupees")
(4) Calculate the angle between two given vectors. The two vectors are, a = i + 2j and b = 9 i + 3j
import numpy as np

# Define the vectors
a = np.array([1, 2])  # Vector a = i + 2j
b = np.array([9, 3])  # Vector b = 9i + 3j

# Calculate the dot product of the vectors
dot_product = np.dot(a, b)

# Calculate the magnitudes of the vectors
magnitude_a = np.linalg.norm(a)
magnitude_b = np.linalg.norm(b)

# Calculate the cosine of the angle between the vectors
cosine_theta = dot_product / (magnitude_a * magnitude_b)

# Calculate the angle in radians
angle_radians = np.arccos(cosine_theta)

# Convert radians to degrees
angle_degrees = np.degrees(angle_radians)

# Output the result
print("Angle between vectors a and b (in degrees):", angle_degrees)

np.linalg.norm(u) # Euclidean Distance L_2 norm

np.linalg.norm(u, ord=2)
np.linalg.norm(u, ord=1) # Manhattan Distance
Cosine Similarity
cos(θ)=u.v||u||||v||
a = np.array([12000,34])
b = np.array([174,1500000])
cos_theta = np.dot(a,b)/[np.linalg.norm(a)*np.linalg.norm(b)] # Euclidean Distance]
cos_theta
Scalar and Vector Projections
If u and v are two vectors then we define projv(u)=u.v||v||
We also define vector projection as
projv(u)=u.v||v||2v
sc_proj = np.dot(u, v) / np.linalg.norm(v)   # u.v/||v||
sc_proj
# Vector projection of u on v
vec_proj = (v * sc_proj)/ np.linalg.norm(v)   # (v * (np.dot(u,v)/np.linalg.norm(v)) / np.linalg.norm(v)  
vec_proj
# Vector Projection (vector * scalar projection) / v norm

u = np.array([1, 2, 3])   # vector u 
v = np.array([5, 6, 2])   # vector v: 
  
vec_proj = v * (np.dot(u,v)/np.linalg.norm(v)) / np.linalg.norm(v)

print("Projection of Vector u on Vector v is: ", vec_proj) 

def direction(x):
    return x/np.linalg.norm(x)

---
import numpy as np

A = np.array([[5, 6, 2], [4, 7, 19],[0, 3, 12]])
B = np.array([14, 4, 5])
C= np.array([1,2,3])
D=np.array([[1,2],[3,4]])
I= np.identity(3)
I2= np.identity(2)
det= np.linalg.det(A+I)     
print("|A+I|= :" , det)

BC= np.dot(B,C)
print("BC is :" , BC)
M= np.dot(A,B)
print("The Multiplication of  A and B is :" , M)
P= np.dot(A,A)
print("The A square is :" , P)
print('D^2-5D-2I=',np.dot(D,D)-5*D-2*I2)
Convert the following matrix to an orthogonal matrix using Gram Schmidt Process?
import numpy as np
u1= np.array([1,7,9])
u2= np.array([3,4,5])
u3= np.array([1,2,1])
v1=np.copy(u1)
#v1=np.array(v1.append(u1))
print(v1/(np.linalg.norm(v1)))
v2= u2-(np.dot(u2,v1)/(np.linalg.norm(v1))**2)*v1
print(v2/(np.linalg.norm(v2)))
v3=u3-(np.dot(u3,v1)/(np.linalg.norm(v1))**2)*v1 -(np.dot(u3,v2)/(np.linalg.norm(v2))**2)*v2
print(v3/(np.linalg.norm(v3)))
Find Scalar and Vector Projection of u on v
u = [6,7,8] v= [2,3,-1]

Verify Q is orthogonal
# Scalar Projection of u on v
u = np.array([6,7,8])
v = np.array([2,3,-1])
sc_proj = np.dot(u, v) / np.linalg.norm(v)   # u.v/||v||
sc_proj
# Vector projection of u on v
vec_proj = (v * sc_proj)/ np.linalg.norm(v)   # (v * (np.dot(u,v)/np.linalg.norm(v)) / np.linalg.norm(v)  
vec_proj
from numpy.linalg import inv
# define orthogonal matrix
Q =1/3* array([ 
[2,-2, 1],
[1, 2, 2],
[2,-1, -2]])
print(Q)
# inverse equivalence
V = inv(Q)
#print(Q.T)
print(V)
# identity equivalence
I = Q.dot(Q.T)
print(I)
---
The Following table lists the weight and heights of 5 boys Find the covariance matrix for the data.

x = [120,125,125,135,145]
y = [61,60,64,68,72]
X = np.stack((x, y), axis=0)
np.cov(X)
---
Find eigen values and eigen vectors for

from numpy import linalg as LA
import numpy as np

a = np.array([[5, 6, 2],[4,7,19],[0,3,12]])
w, v = np.linalg.eig(a)

# singular-value decomposition
from numpy import array
from scipy.linalg import svd
# define a matrix
A = array([
[1, 2],
[3, 4],
[5, 6]])
print(A)
# factorize
U, s, V = svd(A)
print(U)
print(s)
print(V)

----
principal_df
From sklearn.dataset import iris data
Preprocess the data Preprocess the data by subtracting the mean and dividing by the standard deviation of each attribute value. The resulting data should be zero-mean with variance 1.
Compute the covriance matrix
Factorize the covariance matrix using singular value decomposition (SVD) and obtain the eigenvalues and eigenvectors.
Find principle components
def PCA(X , num_components):
     
    #Step-1
    X_meaned = X - np.mean(X , axis = 0)
     
    #Step-2
    cov_mat = np.cov(X_meaned , rowvar = False)
     
    #Step-3
    eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)
     
    #Step-4
    sorted_index = np.argsort(eigen_values)[::-1]
    sorted_eigenvalue = eigen_values[sorted_index]
    sorted_eigenvectors = eigen_vectors[:,sorted_index]
     
    #Step-5
    eigenvector_subset = sorted_eigenvectors[:,0:num_components]
     
    #Step-6
    X_reduced = np.dot(eigenvector_subset.transpose() , X_meaned.transpose() ).transpose()
     
    return X_reduced

import pandas as pd
 
#Get the IRIS dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
data = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])
 
#prepare the data
x = data.iloc[:,0:4]
 
#prepare the target
target = data.iloc[:,4]
 
#Applying it to PCA function
mat_reduced = PCA(x , 2)
 
#Creating a Pandas DataFrame of reduced Dataset
principal_df = pd.DataFrame(mat_reduced , columns = ['PC1','PC2'])
 
#Concat it with target variable to create a complete Dataset
principal_df = pd.concat([principal_df , pd.DataFrame(target)] , axis = 1)


print(w)
print(v)
