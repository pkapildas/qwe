df = df17.copy()
#---------------------1 eda ----------------------------------------------
df = df.drop(columns='CUST_ID')
# df['MINIMUM_PAYMENTS'].sort_values(ascending = False).head()
df['MINIMUM_PAYMENTS'].replace(to_replace = '?' , value = np.nan, inplace = True) #, regex = True)
df['CREDIT_LIMIT'].replace(to_replace = '?' , value = np.NaN, inplace = True)  #, regex = True)
df['CREDIT_LIMIT'] = df['CREDIT_LIMIT'].astype('float64')
df['MINIMUM_PAYMENTS'] = df['MINIMUM_PAYMENTS'].astype('float64')
df = df.dropna()
df = df.reset_index(drop = True)
df.head(2)

#---------------------2 scale & pca ----------------------------------------------
snc = StandardScaler()
df_snc = pd.DataFrame( snc.fit_transform(df) , columns = df.columns, index = df.index)
df_snc.head(2)

pc1=PCA()
inp_pc1=pc1.fit_transform(df_snc)

#print(  'Top 5 Eigen values :                 ' , pc1.explained_variance_[:5] )
#print(  'Top 5 Eigen Vectors:                 ' , pc1.components_[:5]         )
#print("Principal Component Covariance Matrix:\n",  np.cov(pc1.components_) )

#---------------------2 90% variance ----------------------------------------------
explained_variance = np.cumsum(pc1.explained_variance_ratio_)
num_components_90 = np.argmax(explained_variance >= 0.90) + 1
print(f"Number of components to explain 90% variance: {num_components_90}")

#---------------------3 PCA with 90% Variance -------------------------------------
pca_90      = PCA(n_components = num_components_90)
pca_data_90 = pca_90.fit_transform(df_snc)
inp_df_pc_90   = pd.DataFrame(pca_data_90, columns=[f"PC{i+1}" for i in range(num_components_90)])
inp_df_pc_90.head(2)

#inp_df_pc_90=pd.DataFrame(inp_pc1[:,:num_pc],columns=[f'PCA{i+1}' for i in range(num_pc)])
inp_df_pc_90.head(2)

#---------------------4 K means -------------------------------------

# - - #### 2. (b).Find the optimal number of clusters for the K-means clustering model [Note: Use the PCs, which are explaining the 90% variance].  (6 marks)
wcss=[]
sc=[]
for i in range(2,9):
    kmeans1=KMeans(n_clusters=i,random_state=48)
    kmeans1.fit(inp_df_pc_90)
    wcss.append(kmeans1.inertia_)
    sc.append(silhouette_score(inp_df_pc_90,kmeans1.labels_))

plt.figure(figsize=(4,4))
plt.plot(range(2,9),wcss,c='r',marker='o')   # 2 to 20 is clusture si
plt.twinx()
plt.plot(range(2,9),sc,marker='*')
plt.axvline(3,c='orange')
plt.show()

# Final K-Means Model with 3 Clusters ------------------ only if wcss asked
optimal_k = 3
kmeans_final = KMeans(n_clusters=3, random_state=42)
kmeans_final.fit(pca_data_90)
df["Cluster"] = kmeans_final.labels_

# Business Inferences from Clusters
cluster_summary = df.groupby("Cluster").mean()
print("Cluster Summary Statistics:\n")
cluster_summary

# cluster_summary
# Cluster 0 (High Balance, Frequent Cash Advances):  Balance: Very high average balance ($4020), suggesting customers with high credit utilization
# Cluster 1 (Moderate Purchases, Low Cash Advances): Balance: Low average balance ($841), implying better control over credit utilization.
# Cluster 2 (High Purchases, High Credit Utilization): Balance: Moderate-high balance ($2236) with extremely high purchases ($4273), including a mix of one-off and installment-based purchases.

#Key Business Insights:
#Cluster 0: High-risk customers—monitor for potential credit defaults.
#Cluster 1: Balanced, low-risk customers—ideal for standard offers.
#Cluster 2: High-value customers—target for premium services and rewards.

#---------------------5 SilhouetteVisualizer -------------------------------------
# do only if asked
from yellowbrick.cluster import SilhouetteVisualizer
from sklearn.metrics import silhouette_score
#for i in range(3,4):
i = 3
kmeans = KMeans(n_clusters=i, random_state=48)
kmeans.fit(inp_df_pc_90)
silhouette_avg = silhouette_score(inp_df_pc_90, kmeans.labels_)
print('Silhouette Score:', silhouette_avg)
model = SilhouetteVisualizer(kmeans)
model.fit(inp_df_pc_90)
#model.show()

#--------------------- 6 # Plot the dendrograms [ Hierarchical Clustering ]
#Hint:Use the following functions and create the dendrograms for 100 nodes only
# linkage(pca_df, method='single',metric='euclidean')
# dendrogram(mergings,truncate_mode='lastp',p=100)
from scipy.cluster.hierarchy import dendrogram, linkage, cophenet
from scipy.spatial.distance import pdist
#  ['single','complete','average','centroid']

# Plot Dendrogram for Ward Linkage
linkage_matrix = linkage(inp_df_pc_90, method='ward')
plt.figure(figsize=(6, 2))
dendrogram(linkage_matrix, truncate_mode='lastp', p=100)
plt.title("Dendrogram (Ward Linkage)")
plt.xlabel("Data Points")
plt.ylabel("Distance")
#plt.show()

# Compute Cophenetic Correlation Coefficient
c, _ = cophenet(linkage_matrix, pdist(pca_data_90))
print(f"Cophenetic Correlation Coefficient: {c:.2f}")


#------------------------7 # Aglo clusturing
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import pdist

# Ward linkage clustering and get the cluster labels
# # Cluster to 4 groups. Then Predict clusters and add labels to DataFrame

lm = linkage(inp_df_pc_90, method='ward')  
cluster_labels = fcluster(lm, 4, criterion='maxclust')  
df['Cluster_Ward'] = cluster_labels  

# AgglomerativeClustering
ac = AgglomerativeClustering(n_clusters=4, linkage='ward')
df['Cluster_Agglo'] = ac.fit_predict(inp_df_pc_90) 


def func_wcss(df, cluster_labels_col, data):
    x = []  # List to store WCSS values

    for j in np.unique(df[cluster_labels_col]):
        a = data[df[cluster_labels_col] == j]
        b = a.mean(axis=0)
        c = ((a - b) ** 2).sum().sum()
        x.append((j, c))  

    x_sort = sorted(x, key=lambda x: x[1])  # Sort based on WCSS value

    print("Cluster WCSS (Ordered by Quality):")
    for cluster_label, wcss in x_sort:  # Use x_sort instead of wcss_per_cluster_sorted
        print(f"Cluster {cluster_label}: WCSS = {wcss:.2f}")

# Call the function for Ward and Agglomerative clustering
func_wcss(df,  'Cluster_Ward', inp_df_pc_90)
func_wcss(df, 'Cluster_Agglo', inp_df_pc_90)


-----------------------clusturing -------------------------------------------------

from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import pdist

lm = linkage(inp_df_pc_90, method='ward')  
cluster_labels = fcluster(lm, 4, criterion='maxclust')  
df['Cluster_Ward'] = cluster_labels  

ac = AgglomerativeClustering(n_clusters=4, linkage='ward')
df['Cluster_Agglo'] = ac.fit_predict(inp_df_pc_90) 


def func_wcss(df, cluster_labels_col, data):
    x = []  # List to store WCSS values

    for j in np.unique(df[cluster_labels_col]):
        a = data[df[cluster_labels_col] == j]
        b = a.mean(axis=0)
        c = ((a - b) ** 2).sum().sum()
        x.append((j, c))  

    x_sort = sorted(x, key=lambda x: x[1])  # Sort based on WCSS value

    print("\n\n ----Cluster WCSS (Ordered by Quality):")
    for cluster_label, wcss in x_sort:  # Use x_sort instead of wcss_per_cluster_sorted
        print(f"Cluster {cluster_label}: WCSS = {wcss:.2f}")

# Call the function for Ward and Agglomerative clustering
func_wcss(df,  'Cluster_Ward', inp_df_pc_90)
func_wcss(df, 'Cluster_Agglo', inp_df_pc_90)


<<<Cheat Sheet>
 import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 from sklearn.preprocessing import StandardScaler
 from sklearn.decomposition import PCA
 from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN
 from sklearn.metrics import silhouette_score
 from scipy.cluster.hierarchy import linkage , dendrogram, fcluster,cophenet
 from scipy.spatial.distance  import pdist
 #from sklearn.model_selection import train_test_split
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.metrics import classification_report
 from sklearn.decomposition import TruncatedSVD
 from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
 from surprise import KNNWithMeans,SVDpp
 from surprise import Dataset
 from surprise import accuracy
 from surprise import Reader
 from surprise.model_selection import train_test_split,cross_validate
 from mlxtend.frequent_patterns import apriori
 from mlxtend.frequent_patterns import association_rules
 from surprise import KNNBasic
 import warnings
 warnings.filterwarnings("ignore")


#  df['sex'].value_counts() #-------Check ? or.
 #  df['CREDIT_LIMIT'].sort_values(ascending=False).head()
 #  df['MINIMUM_PAYMENTS'].sort_values(ascending=False).head()
 df['CREDIT_LIMIT'].replace(to_replace='?',value=np.nan,inplace=True)
 df['MINIMUM_PAYMENTS'].replace(to_replace='?',value=np.nan,inplace=True)
 print(df.columns)
 #d  f['cul']=df['culmen_length_mm'].fillna(df['cul'].median())
 #   df1=pd.get_dummies(df,drop_first=True)
 #   df.isnull().sum()/df.shape[0]*100
 df = df.dropna()
 df = df.reset_index(drop = True)
 df.head(2)
 # df.isnull().sum()

df['CREDIT_LIMIT']    =df['CREDIT_LIMIT'].astype('float64')
 df['MINIMUM_PAYMENTS']=df['MINIMUM_PAYMENTS'].astype('float64')

snc=StandardScaler()
 df_snc=pd.DataFrame(snc.fit_transform(df),columns=df.columns,index=df.index)
 df_snc.head(2)

pc1=PCA()
 inp_pc1=pc1.fit_transform(df_snc)
 print('Top 5 Eigen values: \n' , pc1.explained_variance_[:5])
 print('Top 5 Eigen vectors:\n' ,pc1.components_[:2])

cum_var=np.cumsum(pc1.explained_variance_ratio_)
 num_pc=np.sum((cum_var<0.9).astype('int'))+1;
 print(num_pc)

#10 till 90
 inp_red_pc_90=pd.DataFrame(inp_pc1[:,:num_pc],
                           columns=[f'PCA{i+1}' for i in range(num_pc)])
 inp_red_pc_90.head(2)

# 2. (b).Find the optimal number of clusters for the K-means clustering model 
#        [Note: Use the PCs, which are explaining the 90% variance].  (6 marks)
 wcss =[]
 sc   =[]
 for i in range(2,20):
    kmeans1=KMeans(n_clusters=i,random_state=48)
    kmeans1.fit(inp_red_pc_90)
    wcss.append(kmeans1.inertia_)
    sc.append(silhouette_score(inp_red_pc_90,kmeans1.labels_))
 plt.figure(figsize=(6,3))
 plt.plot(range(2,20),wcss,marker='o',c='r',label='wcss')
 plt.legend(loc=1)
 plt.twinx()
 plt.plot(range(2,20),sc,marker='*',c='b',label='silhouette_score')
 plt.legend(loc=3)
 plt.axvline(3,c='orange')
 plt.show()
 kmeans2=KMeans(n_clusters=3,random_state=48)
 kmeans2.fit(inp_red_pc_90)
 df2=inp_red_pc_90.copy()
 df2['Cluster_Labels']=kmeans2.labels_
 df2.head(2)

 # 2. (c). Plot the dendrograms using 4 linkage methods for the PCA transformed d
 #         [Note: Use the PCs, which are explaining the 90% variance] (6 marks)
 #         Hint:Use the following functions and create the dendrograms for 100 no
 #              linkage(pca_df, method='single',metric='euclidean')
 #              dendrogram(mergings,truncate_mode='lastp',p=100)
 linkage1=['ward']   #'single','complete','average','centroid','ward']
 for lk in linkage1:
    link1=linkage(inp_red_pc_90,method=lk)
    c,_=cophenet(link1,pdist(inp_red_pc_90))
    print(f'Cophenet correlation for linkage method {lk} is {c}')
    plt.figure(figsize=(10,1))
    dendrogram(link1,truncate_mode='lastp',p=100)
    plt.show()

# 2 (d)Cluster the data into 4 groups and order the cluster quality in terms of 
#      [Use ward linkage metric] (6 marks)
 df3=inp_red_pc_90.copy()
 agg1=AgglomerativeClustering(n_clusters=4)
 agg1.fit(inp_red_pc_90)
 df3['Cluster_Labels']=agg1.labels_
 df3.head(2)

# Extracting clusters
 df_clus0 = df3[df3['Cluster_Labels'] == 0].drop(columns='Cluster_Labels')
 df_clus1 = df3[df3['Cluster_Labels'] == 1].drop(columns='Cluster_Labels')
 df_clus2 = df3[df3['Cluster_Labels'] == 2].drop(columns='Cluster_Labels')
# Converting to arrays
 df_clus0_ar = np.array(df_clus0)
 df_clus1_ar = np.array(df_clus1)
 df_clus2_ar = np.array(df_clus2)

# Calculating centroids
 df_clust_cent0 = df_clus0.mean(axis=0).values
 df_clust_cent1 = df_clus1.mean(axis=0).values
 df_clust_cent2 = df_clus2.mean(axis=0).values

 # Calculating WCSS for each cluster
 wcss1 = []
 wcss1.append(np.sum((df_clus0_ar - df_clust_cent0) ** 2))  # WCSS for cluster 0
 wcss1.append(np.sum((df_clus1_ar - df_clust_cent1) ** 2))  # WCSS for cluster 1
 wcss1.append(np.sum((df_clus2_ar - df_clust_cent2) ** 2))  # WCSS for cluster 2

for i, wcss in enumerate(wcss1):                 # Display WCSS for each cluster
    print(f"Cluster {i}: WCSS = {wcss:.2f}")
 print(np.argsort(wcss1))

 2 (e)Compare the quality of clusters for K-means clustering algorithm on origi
 #      data and PCA transformed data. (8 marks)
wcss=[]
 sc=[]
 for i in range(2,20):
    kmeans3=KMeans(n_clusters=i,random_state=48)
    kmeans3.fit(df_snc)
    wcss.append(kmeans3.inertia_)
    sc.append(silhouette_score(df_snc,kmeans3.labels_))
 plt.figure(figsize=(6,3))
 plt.plot(range(2,20),wcss,marker='o',c='r',label='wcss')
 plt.legend(loc=1)
 plt.twinx()
 plt.plot(range(2,20),sc,marker='*',c='b',label='silhouette_score')
 plt.legend(loc=3)
 plt.axvline(3,c='orange')
 plt.show()
 kmeans4=KMeans(n_clusters=3,random_state=48)
 kmeans4.fit(df_snc)
 var_2e1 = silhouette_score(df_snc,kmeans4.labels_)
 var_2e2 = silhouette_score(inp_red_pc_90,kmeans2.labels_)
 print(f'Silhouette_score of Kmeans clustering with PCA is {var_2e1}')
 print(f'Silhouette_score of Kmeans clustering without PCA is {var_2e2}')

# 3 (a) Develop a popularity-driven recommendation system, 
# print Total no of ratings,Total No of Users,Total No of products
 # and recommend the top 5 items. (10 marks )
 # Use the dataset: Book_ratings.csv
 # Variables:
 # book_id - ID of the book.
 # user_id - ID of the user rated
 # ratings - ratings of the book by the user.
 df_p=pd.read_csv(path18)   #'Book_ratings.csv')
 df_p=df_p.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])
 df_p=df_p[['user_id','book_id','rating']]
 df_p.head()
 df_r=df_p.groupby('book_id')['rating'].agg(['count','mean'])
 df_r.columns=['Rating_count','Average_Rating']
df_r.sort_values('Rating_count',ascending=False).head()
 print(f'Total no of ratings: {df_r.Rating_count.count()}')
 print(f'Total No of Users: {df_p.user_id.nunique()}')
 print(f'Total No of products: {df_p.book_id.nunique()}')

## Recommendation Engine
df_c=df_p.copy()                                                          
# Load the dataset
 reader1=Reader(rating_scale=(df_c['rating'].min(),df_c['rating'].max()))  
# Create a Reader object
 dataset=Dataset.load_from_df(df_c,reader=reader1)                         
# Load the dataset into Surprise's Dataset format
 [trainset,testset]=train_test_split(dataset,test_size=0.2,random_state=48) 
# Split the data into training and testing sets
 #svd1=SVD() ; svd1.fit(trainset)
 model=KNNBasic(sim_options={'name':'cosine','user_based':True})            
# Build the collaborative filtering model (user-based in this case)
 model.fit(trainset)                                  # Train the model
 ypred_knn=model.test(testset)                        # Make predictions on the t
 print(accuracy.rmse(ypred_knn) )                     # Calculate RMSE
 # Perform cross-validation
 cv_km = cross_validate(model, dataset, measures=['RMSE'], cv=5)
 np.mean(cv_km['test_rmse']) # Print the average RMSE across folds

 df_p.head(2)

 user1='116'
 item_visited=set(df_p[df_p['user_id']==user1]['book_id'])
 item_all=set(df_p['book_id'])
 item_not_visted=list(item_all.difference(item_visited))
 ls=[]
 for i in item_not_visted:
 #   ls.append({'book_id':i,'rating':cv_knn.predict(uid=user1,iid=i).est})
    ls.append({'book_id':i,'rating': model.predict(uid=user1,iid=i).est})
 df_pred=pd.DataFrame(ls)
 df_pred.sort_values('rating',ascending=False); df_pred.head(2)

 #3 (c) Create association rule mining using apriori algorithm.
 #             Perform basic pre-processing operations required by algorithm
 #            (drop missing values, drop unnecessary columns). 
#      Create the basket only for France. Run algorithm
 #             with minimum support 0.07, tune with lift. (3 marks)
 #             Use Dataset:Online Retail.csv
 import pandas as pd
 import numpy as np
 from mlxtend.frequent_patterns import apriori
 from mlxtend.frequent_patterns import association_rules
 df_ap=pd.read_csv(path19) #'Online Retail.csv')
 df_ap.head()
 df=df_ap[df_ap['Country']=='France']
 df1=df[['InvoiceNo','Description','Quantity']]
 df2=df1[~df1['InvoiceNo'].str.startswith('C')]
 df3=df2.groupby(['InvoiceNo','Description'])['Quantity'].sum()
 df3.head(3)

df4=df3.unstack()
 df5=df4.reset_index()
 df6=df5.fillna(0)
 df7=df6.set_index('InvoiceNo')
 df7.head(2)

def encode(x):
    return 1 if x>0.0 else 0
 df8=df7.applymap(encode)
 df8.head()
 frequent_itemsets=apriori(df8,min_support=0.07,use_colnames=True)
 rules=association_rules(frequent_itemsets,metric='lift',min_threshold=1)
 rules=rules.sort_values(by='lift',ascending=False)
 rules.head(2)

## APIORI 2nd Example
 #3 (c) dataset for stores - apiori
 df_ap=pd.read_csv(path20) #'Online Retail.csv')
 df_ap.head(2)
 df_ap=df_ap.fillna(0)
 df_ap.head(3)
 # Convert the dataset to a list of transactions
 transactions = []
 for i in range(0, df_ap.shape[0]):
    transaction = df_ap.iloc[i].dropna().tolist()  # Remove NaN values and conve
    filtered_transaction = [str(item) for item in transaction if not str(item).i
    transactions.append(frozenset(filtered_transaction))  # Use frozenset to mak
 # Remove duplicates by converting to a set and back to a list


unique_transactions = [set(map(str, transaction)) for transaction in list(set(tr
 # One-hot encode the list of transactions
 from mlxtend.preprocessing import TransactionEncoder
 te = TransactionEncoder()
 te_array = te.fit(unique_transactions).transform(unique_transactions)
 df8 = pd.DataFrame(te_array, columns=te.columns_).astype(int)  # Convert True/Fa
 # Display the one-hot encoded dataframe
 print("One-Hot Encoded Data (0/1):")
 df8.head(1)

# Apply the Apriori algorithm with a minimum support threshold of 0.01
 frequent_itemsets = apriori(df8, min_support=0.01, use_colnames=True)
 # Generate association rules using lift as the metric
 rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)
 rules=rules.sort_values(by='lift',ascending=False)
 # Display frequent itemsets
 print("\nFrequent Itemsets:") ; print(frequent_itemsets.head(10))
 # Display the top 10 rules
 print("\nAssociation Rules:")
 rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10)

5. Displays the associated results of items = ['Rule','Support','Confidence','Lift']
association_rules = apriori(records, min_support=0.1, min_confidence=0.4,min_lift=1)
association_results = list(association_rules)
# Initialize an empty list to collect rows
rows = []

# Iterate over the association results
for item in association_results:
    pair = item[2]
    for i in pair:
        if i[3] != 1:
            row = {
                'Rule': str([x for x in i[0]]) + " -> " + str([x for x in i[1]]),
                'Support': item[1],
                'Confidence': i[2],
                'Lift': i[3]
            }
            rows.append(row)

# Create the DataFrame from the list of rows
Result = pd.DataFrame(rows, columns=['Rule', 'Support', 'Confidence', 'Lift'])

print(Result)
---------------------


# Step 1: Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, cohen_kappa_score, confusion_matrix, roc_curve, auc
from scipy.stats import zscore
from imblearn.over_sampling import SMOTE
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Step 2: Load Dataset
df = pd.read_csv("loan.csv")

# -------------------- (A) Data Exploration --------------------
print("\nDataset Shape:", df.shape)
print("\nData Info:\n", df.info())
print("\nMissing Values:\n", df.isnull().sum())

# Identify Numerical & Categorical Columns
numerical_cols = df.select_dtypes(include=['number']).columns.tolist()
categorical_cols = df.select_dtypes(exclude=['number']).columns.tolist()

print("\nNumerical Columns:", numerical_cols)
print("\nCategorical Columns:", categorical_cols)

data_clust.isnull().sum().sum()
#Keep the input and output column seperate
inp_data_dime=data_clust.drop('M3',axis=1)
out=data_clust['M3']
For dimensionality reduction scaling is the must do pre-processing step. Students must perform scaling prior to dimensionality reduction. 
(Students may also performed outlier treatment, which is good to consider for awarding more marks). If they have not performed scaling, then mark must be reduced drastically.


from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
data_sc = scaler.fit_transform(inp_data_dime)
data_sc=pd.DataFrame(data_sc,columns=inp_data_dime.columns)
#1.b Perform univariate and bivariate analysis and remove any variable which is absolutely 
#    insignificant. (2 Marks)
inp_data_dime.describe()
out.value_counts().plot(kind='bar')
--
import seaborn as sb
from matplotlib import pyplot as plt
plt.figure(figsize=(18,8))
sb.heatmap(inp_data_dime.corr(),annot=True)

for i in inp_data_dime.columns:
    sb.boxplot(data_clust['M3'],data_clust[i])
    plt.show()

# 2. Apply K means clustering and identify the ideal value of K using elbow and silhoutee method
from sklearn.cluster import KMeans
wcss=[]
cl=[1,2,3,4,5,6,7,8]
for i in cl:
    mod=KMeans(n_clusters=i,random_state=10)
    mod.fit(data_sc)
    print(mod.inertia_)
    wcss.append(mod.inertia_)
plt.plot(cl,wcss) 
sil=[]
from sklearn.metrics import silhouette_score
cl=[2,3,4,5,6,7,8]
for i in cl:
    mod=KMeans(n_clusters=i)
    mod.fit(data_sc)
    sil.append(silhouette_score(data_sc,mod.labels_))
res=pd.DataFrame({'k':cl,'silhoutee':sil})
res
Apply PCA on the data. How many PCs are required to reproduce the 95% charecteristics of original data. What is the top 5 features contributing in PC1 ?
from sklearn.decomposition import PCA 
pca = PCA(n_components = data_sc.shape[1])
pca_data = pca.fit_transform(data_sc)
exp_var_ratio= pca.explained_variance_ratio_
exp_var_ratio.round(3)

cum_var=exp_var_ratio[0]
itr=2 # defined as two as first pc1 variance defined outside the loop
for j in exp_var_ratio[1:]:
    cum_var=cum_var+j
    if cum_var >= 0.95:
        break
    itr=itr+1

print('The number of principle components capturing 95 percent varaition is data is : ',itr,' Varaince explained is ', cum_var)
# Variance Ratio bar plot for each PCA components.
ax = plt.bar(range(1,len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)
plt.xlabel("PCA Components",fontweight = 'bold')
plt.ylabel("Variance Ratio",fontweight = 'bold')
# PC1 is derived from first eigen vector
e1=pd.DataFrame(pca.components_[0,:]) # first eigen vector
e1.index=data_sc.columns
e11=np.abs(e1)
e11.sort_values(0,ascending=False).head(5) # Top 5 features contributing in PC1
### 5. Build the following ML model and compare its performace: (5 Marks)
    #a. ML model with original inp_data_dime and out
    #b. ML model with inp_data_dime_pca and out
    #(Note: For pca and svd use the number of components which captures the 95 percent of variance) 
    
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

xtrain,xtest,ytrain,ytest=train_test_split(data_sc,out,test_size=0.2,random_state=48)
rf=RandomForestClassifier(random_state=48)
rf.fit(xtrain,ytrain)
ypred=rf.predict(xtest)
print('The number of input feature ',xtrain.shape[1])
print(classification_report(ytest,ypred))

xtrain,xtest,ytrain,ytest=train_test_split(pca_data[:,:5],out,test_size=0.2,random_state=48)
rf=RandomForestClassifier(random_state=48)
rf.fit(xtrain,ytrain)
ypred=rf.predict(xtest)
print('The number of PCA components ',xtrain.shape[1])
print(classification_report(ytest,ypred))
# Recomendation 
data_recom.head(2)
from surprise import KNNWithMeans,SVDpp
from surprise import Dataset
from surprise import accuracy
from surprise import Reader
from surprise.model_selection import train_test_split,cross_validate
#data_recom=pd.read_csv('recommendation_mini.csv')
ratings = data_recom
reader = Reader(rating_scale=(1, 5))

trainsetfull = rating_data.build_full_trainset()
print('Number of users: ', trainsetfull.n_users, '\n')
print('Number of items: ', trainsetfull.n_items, '\n')

alg=SVDpp()
alg.fit(trainsetfull) 
alg.predict(uid = 'A2CX7LUOHB2NDG', iid ='059400232X')

results = cross_validate(
    algo = alg, data = rating_data, measures=['RMSE'], 
    cv=3)
results['test_rmse'].mean()
---
3 (a) Build the popularity based recommendation system and suggest top 5 items. (5 Marks)
data_recom=data.iloc[:,24:28]
data_recom.shape
# Top 5 Items based on Average Rating
pd.DataFrame(data.groupby('ItemID')['Rating'].mean().sort_values(ascending=False))
popularity_table = data.groupby('ItemID').agg({'Rating' : 'mean'})
popularity_table.head(5)
popularity_table.sort_values('Rating', ascending=False).head(5)
#Build collaborative recommendation engine to recommend a top 5 items to the specific user. Measure the model quality in terms of RMSE
reader = Reader(rating_scale =(1,5))
rating_data = Dataset.load_from_df(data_recom[['UserID', 'ItemID', 'Rating']], reader)
rating_data
[train_set, test_set] = train_test_split(rating_data, test_size=.15, shuffle=True)
trainsetfull = rating_data.build_full_trainset()
print('Number of Users  : ',trainsetfull.n_users, '\n')
print('Number of Items  : ',trainsetfull.n_items, '\n')
algo = KNNWithMeans(k= 15, min_k = 5 , sim_options = {'name':'pearson', 'user_based' :False}, verbose =True)
results = cross_validate( algo = algo, data =rating_data , measures =['RMSE'], 
                        cv =5, return_train_measures=True)
print(results['test_rmse'].mean())

alg = SVDpp()
alg.fit(trainsetfull)
alg.predict(uid =76, iid =2)
item_id = data_recom['ItemID'].unique()
item_id76 = data_recom.loc[data_recom['UserID']==76, 'ItemID']
item_id_pred = np.setdiff1d(item_id, item_id76)
testset= [[76, iid, 4] for iid in item_id_pred]
pred = alg.test(testset)
pred
--
pred_ratings = np.array([pred1.est for pred1 in pred])
i_max= pred_ratings.argmax()
iid = item_id_pred[i_max]
print('Top item for user has iid {0} with predicted rating{1}'.
      format(iid, pred_ratings[i_max]))
r_df = data_recom.pivot(index ='UserID', columns ='ItemID', values='Rating').fillna(0)



model = SVD(random_state =42)
model.fit(trainsetfull)
pred = model.test(testset)
print("Collabrative Filter Model RMSE ", accuracy.rmse(pred))
# for a user
num_recomendations =5
userId =76
user_row_number = userId -1 
all_items= data_recom['ItemID'].unique()
pred_ratings = [(item, model.predict(userId, item).est) for item in all_items]
top_5_reco = sorted(pred_ratings, key = lambda x :x[1], reverse =True)[:5]
print(f' Top 5 Recomended items for User id {userId}')
for item , rating in top_5_reco :
    print(f'item : {item}, Predicted Rating {rating}')



#Imabalanced data
# -------------------- (B) Data Cleaning --------------------

#First run a check for the presence of missing values and their percentage for each column. Then choose the right approach to treat them.
Total = df_admissions.isnull().sum().sort_values(ascending=False)          
Percent = (df_admissions.isnull().sum()*100/df_admissions.isnull().count()).sort_values(ascending=False)   
missing_data = pd.concat([Total, Percent], axis = 1, keys = ['Total', 'Percentage of Missing Values'])    
missing_data
df_num = df_feature.select_dtypes(include = [np.number])
df_cat = df_feature.select_dtypes(include = [np.object])

 

"""
imputer_num = SimpleImputer(strategy="median")
df[numerical_cols] = imputer_num.fit_transform(df[numerical_cols])

imputer_cat = SimpleImputer(strategy="most_frequent")
df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])
"""

for col in numerical_cols:
    if df[col].isnull().sum() > 0:  
        median_value = df[col].median()  # Compute median dynamically
        df[col].fillna(median_value, inplace=True)  # Replace missing values with median

for col in categorical_cols:
    if df[col].isnull().sum() > 0:
        mode_value = df[col].mode()[0]  # Compute mode dynamically
        df[col].fillna(mode_value, inplace=True)  # Replace missing values with mode
#Examine Outliers
for i in df_f.columns:
    sb.boxplot(df_f[i])
    plt.show()

for i in df_f.columns:
    sb.kdeplot(stats.zscore(df_f[i]))
    plt.axvline(3, ymin=0, ymax=0.01, c='r')
    plt.axvline(-3, ymin=0, ymax=0.01, c='r')
    plt.show()

# Handling Outliers using Z-score
z_scores = np.abs(zscore(df[numerical_cols]))
df = df[(z_scores < 3).all(axis=1)]  

 # removal of outliers
Q1=data[num_cols].quantile(0.25)
Q3=data[num_cols].quantile(0.75)
IQR=Q3-Q1
lower_bound=Q1-1.5*IQR
upper_bound=Q3+1.5*IQR
data_fix=data[~((data[num_cols]<lower_bound) | (data[num_cols]>upper_bound)).any(axis=1)]
print(data.shape)
print(data_fix.shape)

# for the independent numeric variables, we plot the histogram to check the distribution of the variables
# Note: the hist() function considers the numeric variables only, by default
# we drop the target variable using drop()
# 'axis=1' drops the specified column
df_admissions.drop('Chance of Admit', axis = 1).hist()
plt.tight_layout()
# skew() returns the coefficient of skewness for each variable
df_admissions.drop('Chance of Admit', axis = 1).skew()
#Correlation 
    sb.heatmap(df_n.corr(), annot=True, cmap='rainbow')

# Check skewness for numerical features
for col in df_num:
    skewness = skew(df[col])
    print(f"Skewness of {col}: {skewness}")

# -------------------- (C) Feature Engineering --------------------
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
Remove Insignificant Variables
df_cust = df_cust.drop('Cust_Number',axis=1)
Outlier Analysis and Treatment
Check the outliers in all the variables and treat them using appropriate techniques.
--------------------------------
Choosing the number of clusters 
The first step is to define the K number of clusters in which we will group the data. Let’s select K=3.
# consider the numeric variables
df_num = df_cust.drop(['Sex'], axis = 1)
fig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 4))
for variable, subplot in zip(df_num.columns, ax.flatten()):  
    # use boxplot() to plot the graph
    # pass the axes for the plot to the parameter, 'ax'
    sns.boxplot(df_cust[variable], ax = subplot)
# display the plot
plt.show()
# missing value & presence of missing values and their percentage for each column.
#Scale the Data
# consider the features 'Cust_Spend_Score' and 'Yearly_Income'
X_filtered = df_cust[['Cust_Spend_Score', 'Yearly_Income']]

# print top 5 observations of X
X_filtered.head()
# initialize the StandardScaler
X_norm = StandardScaler()
num_norm = X_norm.fit_transform(X_filtered)
X = pd.DataFrame(num_norm, columns = X_filtered.columns)
#K-Means Clustering
Optimal Value of K Using Elbow Plot
Elbow plot is plotted with the value of K on the x-axis and the WCSS (Within Cluster Sum of Squares) on the y-axis. The value of K corresponding to the elbow point represents the optimal value for K.
# create several cluster combinations ranging from 1 to 20 and observe the wcss (Within Cluster Sum of Squares) for each cluster
# consider an empty list to store the WCSS
wcss  = []

# use for loop to perform K-means with different values of K
# set the 'random_state' to obtain the same centroid initialization for each code run
# fit the model on scaled data
# append the value of WCSS for each K to the list 'wcss'
# the 'inertia_' retuns the WCSS for specific value of K
for i in range(1,21):
    kmeans = KMeans(n_clusters = i, random_state = 10)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
Let us plot the elbow plot and identify the elbow point.
# visualize the elbow plot to get the optimal value of K
plt.plot(range(1,21), wcss)

# set the axes and plot labels
# set the font size using 'fontsize'
plt.title('Elbow Plot', fontsize = 15)
plt.xlabel('No. of clusters (K)', fontsize = 15)
plt.ylabel('WCSS', fontsize = 15)

# display the plot
plt.show()
# visualize the elbow plot to get the optimal value of K
plt.plot(range(1,21), wcss)

# set the axes and plot labels
# set the font size using 'fontsize'
plt.title('Elbow Plot', fontsize = 15)
plt.xlabel('No. of clusters (K)', fontsize = 15)
plt.ylabel('WCSS', fontsize = 15)

# plot a vertical line at the elbow
plt.axvline(x = 5, color = 'red')

# display the plot
plt.show()
Interpretation: We can see that the for K = 5, there is an elbow in the plot. Before this elbow point, the WCSS is decreasing rapidly and after K = 5, the WCSS is decreasing slowly.
Now, let us use the silhouette score method to identify the optimal value of K.
#Optimal Value of K Using Silhouette Score
The Silhouette score can also be used to identify the optimal number of clusters. We plot the Silhouette score for different values of K. The K with the highest Silhouette score represents the optimal value for the number of clusters (K).

# create a list for different values of K
n_clusters = [2, 3, 4, 5, 6]
# use 'for' loop to build the clusters
# 'random_state' returns the same sample each time you run the code  
# fit and predict on the scaled data
# 'silhouette_score' function computes the silhouette score for each K
for K in n_clusters:
    cluster = KMeans (n_clusters= K, random_state= 10)
    predict = cluster.fit_predict(X)
    score = silhouette_score(X, predict, random_state= 10)
    print ("For {} clusters the silhouette score is {})".format(K, score))

##Visualize the silhouette scores
# consider the number of clusters
n_clusters = [2, 3, 4, 5, 6]

# consider an array of the data
X = np.array(X)

# for each value of K, plot the silhouette plot the clusters formed
for K in n_clusters:
    
    # create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    
    # set the figure size
    fig.set_size_inches(18, 7)

    # the 1st subplot is the silhouette plot
    # initialize the cluster with 'K' value and a random generator
    model = KMeans(n_clusters = K, random_state = 10)
    
    # fit and predict on the scaled data
    cluster_labels = model.fit_predict(X)

    # the 'silhouette_score()' gives the average value for all the samples
    silhouette_avg = silhouette_score(X, cluster_labels)
    
    # Compute the silhouette coefficient for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(K):
        
        # aggregate the silhouette scores for samples belonging to cluster i, and sort them
        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
        
        # sort the silhouette coefficient
        ith_cluster_silhouette_values.sort()
        
        # calculate the size of the cluster
        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        # color each cluster 
        color = cm.nipy_spectral(float(i) / K)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # compute the new y_lower for next plot
        y_lower = y_upper + 10 

    # set the axes and plot label
    ax1.set_title("Silhouette Plot")
    ax1.set_xlabel("Silhouette coefficient")
    ax1.set_ylabel("Cluster label")

    # plot the vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    # clear the y-axis ticks
    ax1.set_yticks([])  
    
    # set the ticks for x-axis 
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8])

    
    # 2nd plot showing the actual clusters formed
    # consider different color for each cluster
    colors = cm.nipy_spectral(cluster_labels.astype(float) / K)
    
    # plot a scatter plot to visualize the clusters
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k')

    # label the cluster centers
    centers = model.cluster_centers_
    
    # display the cluster center with cluster number
    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')
    
    # add the axes and plot title
    ax2.set_title("Clusters")
    ax2.set_xlabel("Spending Score")
    ax2.set_ylabel("Annual Income")
    
    # set the common title for subplots
    plt.suptitle(("Silhouette Analysis for K-Means Clustering with n_clusters = %d" % K), fontsize=14, 
                 fontweight='bold')

# display the plot
plt.show()

Interpretation: The above plot shows the silhouette plot and the clusters formed for each value of K. The plot shows that there are outliers (where the silhouette coefficient is less than 0) for K = 2,3,4. Also for K = 6, the 6th cluster has the silhouette score less than the average silhouette score. Thus we can not consider the K values as 2,3,4 and 6.

Also from the above output, we can see that the silhouette score is maximum for k = 5 and from the plot, we can see that there are no outliers for 5 clusters and all the clusters have silhouette coefficients greater than the average silhouette score. Thus we choose K = 5 as the optimal value of k.
3.3 Build the Clusters
Let us build the 5 clusters using K-menas clustering.

# build a K-Means model with 5 clusters
new_clusters = KMeans(n_clusters = 5, random_state = 10)

# fit the model
new_clusters.fit(X)

# append the cluster label for each point in the dataframe 'df_cust'
df_cust['Cluster'] = new_clusters.labels_
# head() to display top five rows
df_cust.head()
Check the size of each cluster
df_cust.Cluster.value_counts()
Plot a barplot to visualize the cluster sizes

# use 'seaborn' library to plot a barplot for cluster size
sns.countplot(data= df_cust, x = 'Cluster')

# set the axes and plot labels
# set the font size using 'fontsize'
plt.title('Size of Cluster', fontsize = 15)
plt.xlabel('Clusters', fontsize = 15)
plt.ylabel('Number of Customers', fontsize = 15)

# add values in the graph
# 'x' and 'y' assigns the position to the text
# 's' represents the text on the plot
plt.text(x = -0.05, y =39, s = np.unique(new_clusters.labels_, return_counts=True)[1][0])
plt.text(x = 0.95, y =24, s = np.unique(new_clusters.labels_, return_counts=True)[1][1])
plt.text(x = 1.95, y =37, s = np.unique(new_clusters.labels_, return_counts=True)[1][2])
plt.text(x = 2.95, y =22, s = np.unique(new_clusters.labels_, return_counts=True)[1][3])
plt.text(x = 3.95, y =81, s = np.unique(new_clusters.labels_, return_counts=True)[1][4])

# display the plot
plt.show()
The 5th cluster is the largest cluster containing 80 observations
3.4 Analyze the Clusters
Let us visualize the clusters by considering the variables 'Cust_Spend_Score' and 'Yearly_Income'.
# plot the lmplot to visualize the clusters
# pass the different markers to display the points in each cluster with different shapes
# the 'hue' parameter returns colors for each cluster
sns.lmplot(x = 'Cust_Spend_Score', y = 'Yearly_Income', data = df_cust, hue = 'Cluster', 
                markers = ['*', ',', '^', '.', '+'], fit_reg = False, size = 10)

# set the axes and plot labels
# set the font size using 'fontsize'
plt.title('K-means Clustering (for K=5)', fontsize = 15)
plt.xlabel('Spending Score', fontsize = 15)
plt.ylabel('Annual Income', fontsize = 15)

# display the plot
plt.show()
Cluster 1
Check the size of the cluster
# size of a cluster 1
len(df_cust[df_cust['Cluster'] == 0])
Compute the statistical summary for the customers in this cluster
# statistical summary of the numerical variables
df_cust[df_cust.Cluster==0].describe()

# summary of the categorical variable
df_cust[df_cust.Cluster==0].describe(include = object)


Initializing centroids
Centroid is the center of a cluster but initially, the exact center of data points will be unknown so, we select random data points and define them as centroids for each cluster.
Assign data points to the nearest cluster
Now that centroids are initialized, the next step is to assign data points Xn to their closest cluster centroid C
In this step, we will first calculate the distance between data point X and centroid C using Euclidean Distance metric.
And then choose the cluster for data points where the distance between the data point and the centroid is minimum. 
Re-initialize centroids 
Next, we will re-initialize the centroids by calculating the average of all data points of that cluster.
Repeat steps 3 and 4
We will keep repeating steps 3 and 4 until we have optimal centroids and the assignments of data points to correct clusters are not changing anymore.
Merits:○Easy to understand ○Simple implementation●Demerits:○Finding the optimal value of K can be computationally expensive○Initial centroid assignment affects the final output○Not efficient in presence of outliers
Dendrogram●It is a very useful technique to visualize the clusters
●It is a tree-based hierarchical structure that can be used to decide the required number of clusters
Different linkage methods result in the formation of different dendrograms
●Observations linked at a low height represents more similar observations
●Dissimilar observations fuse at a higher level in the dendrogram
Dendrogram●X-axis of the dendrogram represents the data point, each considered as a single cluster and the distance is given on the Y-axis
Each single cluster is known as ‘leaf’, The horizontal line is known as ‘clade’ which represents the merging of cluster

●DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is mostly used density-based clustering algorithm
●This technique can form clusters of non-linear shapes
●It considers a cluster as a continuous region of high density
●Regions of low density are identified as noise/ outliers
Dimensio Reduction 
The real-life dataset may contain a large number of features under study
●For example, while estimating the price of a mobile phone we need to consider various features like screen size, internal storage, camera quality, battery backup and so on
●The dataset with a large number of features needs more time for training the model. Also, it can cause the overfitting 
To avoid such issues, one can reduce the dimension of the dataset●The dimension reduction techniques remove the redundant variables/ noise in the original data, which reduces the training time●Reducing the dataset to 2 or 3 dimensions helps in visualization of the data●Various dimension reduction techniques:○Principal Component Analysis (PCA)○Linear Discriminant Analysis (LDA) ○Factor Analysis
●Two different approaches can be used for dimension reduction: Projection, Manifold learning●In the projection approach, the original dataset is projected onto the lower-dimensional plane●PCA uses the projection approach for dimension reduction
●This method is not effective if the dataset has different layers in the higher dimensions●In manifold learning, a manifold is created on which the dataset lies
PCA
It is one of the dimensionality reduction techniques that is used to reduce the dimensions of the large datasets●It transforms the large set of features into a small set such that it will contain the maximum information in the original data●The number of components is less than or equal to the number of independent variables●PCA projects the original dataset on the lower dimensional plane●It transoms the original data to a new set of uncorrelated variables

The first principal component (PC1) exhibits the direction of maximum variance in the data●It is used to remove the redundancy in the data●PCA reduces the multicollinearity (if present) in the original data●Principal components are always orthogonal to each other 
PCA steps 
1. Standardize the data
2. Compute the covariance matrix
3. Calculate the eigenvalues and eigenvectors
4. Sort the eigenvalues in the descending order 
5. Select the eigenvectors that explains the maximum variance in the data
Apllication PCA is mainly used in image compression, facial recognition models●It is also used in the exploratory analysis to reduce the dimension of data before applying machine learning methods●Used in the field of psychology, finance to identify the patterns high dimensional data



# instantiate linkage object with scaled data and consider 'ward' linkage method 
link_mat = linkage(features_scaled, method = 'ward')     

# print first 10 observations of the linkage matrix 'link_mat'
print(link_mat[0:10])

5. DBSCAN
DBSCAN is a density-based clustering method. It can create non-linear clusters. This method considers a high-density region as a cluster and the low-density points are considered as outliers. We do not need to provide the required number of clusters to the algorithm.

Let us cluster the scaled data.

# instantiate DBSCAN with epsilon and minimum points 
# pass the epsilon radius for neighbourhood
# pass the number of minimum points
model = DBSCAN(eps = 0.8, min_samples = 15)

# fit the model on the scaled data
model.fit(features_scaled)
# display the unique clusters formed by DBSCAN
(set(model.labels_)
interpretation: From the above output we can see that the DBSCAN algorithm has created 3 clusters. The data points labeled as -1 are the outliers identified by DBSCAN.
# add a column containing cluster number to the original data
df_prod['Cluster_DBSCAN'] = model.labels_

# print head() of the newly formed dataframe
df_prod.head()

# check the size of each cluster
df_prod['Cluster_DBSCAN'].value_counts()

# plot the countplot for the cluster size
sns.countplot(data = df_prod, x = 'Cluster_DBSCAN')

# set the axes and plot labels
# set the font size using 'fontsize'
plt.title('Size of Cluster', fontsize = 15)
plt.xlabel('Cluster', fontsize = 15)
plt.ylabel('No. of Products', fontsize = 15)

# display the plot
plt.show()
nterpretation: From the above output we can see that a cluster with 4875 is the largest cluster and other clusters are very small.

Now let us visualize the clusters. As we have more than 2 features, we consider only the variables Sales and Profit to visualize the clusters.
 plot the lmplot to visualize the clusters
# pass the 'Cluster_DBSCAN' to the hue parameter to display each cluster in a different color
# pass the different marker styles to visualize each cluster with a different marker
sns.lmplot(x = 'Sales', y = 'Profit', data = df_prod, hue = 'Cluster_DBSCAN', markers = ['o','+','^',','], 
           fit_reg = False, size = 12)

# set the axes and plot labels
# set the font size using 'fontsize'
plt.title('DBSCABN (eps = 0.8, min_samples = 15) ', fontsize = 15)
plt.xlabel('Sales', fontsize = 15)
plt.ylabel('Profit', fontsize = 15)

# display the plot
plt.show()

nterpretation: The above plot shows the clusters created by DBSCAN. The blue circles correspond to the outliers and the observations in the largest cluster are denoted by the orange '+'. Other clusters are too small compared to the largest cluster.

We can see some of the points are overlapped. This is because the dimension of the original data is greater than 2 and we have considered only 2 variables to plot the clusters.

Now let us check the products belonging to each cluster.

Cluster 1
Let us identify the products in the cluster 1.

# check the count of different products belonging to cluster_1
df_prod[df_prod.Cluster_DBSCAN==0].index.value_counts()

Outliers identified by DBSCAN
# check the count of different products identified as outliers
Interpretation: We can see that the algorithm has identified most of the technical products as the outliers.

Here we can see that the DBSCAN algorithm has not grouped the product like hierarchical clustering. Thus we can conclude that the DBSCAN algorithm is working poorly on this dataset.

df_prod[df_prod.Cluster_DBSCAN==-1].index.value_counts()
---
from sklearn import tree
model=tree.DecisionTreeClassifier()
model.fit(X_train,y_train)
DecisionTreeClassifier()
y_pred_DT = model.predict(X_test)
cm_DT= confusion_matrix(y_test, y_pred_DT)
cm_DT
array([[11,  0,  0],
       [ 0,  7,  6],
       [ 0,  4,  2]], dtype=int64)
sns.heatmap(cm_DT, annot=True)
plt.show()
classification=classification_report(y_test,y_pred_DT)
print(classification)
ac = accuracy_score(y_test, y_pred_DT)
print("Accuracy Score:", ac)
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.Set1,alpha = 1, edgecolor='k')
ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')
ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_pred_DT, cmap=plt.cm.Set1,alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Predicted label')
plt.show()
--
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0][y_test == 0], X_test[:, 1][y_test == 0], c='red',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 1], X_test[:, 1][y_test == 1], c='blue',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 2], X_test[:, 1][y_test == 2], c='yellow',alpha = 1, edgecolor='k')

ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')
ax[1].scatter(X_test[:, 0][y_pred_DT == 0], X_test[:, 1][y_pred_DT == 0], c='red',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred_DT == 1], X_test[:, 1][y_pred_DT == 1], c='blue',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred_DT == 2], X_test[:, 1][y_pred_DT == 2], c='yellow',alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Predicted label')
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
from sklearn.decomposition import PCA
pca = PCA()
X_train_2 = pca.fit_transform(X_train)
X_test_2 = pca.transform(X_test)
explained_variance = pca.explained_variance_ratio_  
explained_variance
--
from sklearn import tree
model2=tree.DecisionTreeClassifier()
model2.fit(X_train_2,y_train)
DecisionTreeClassifier()
y_pred_DT_2 = model2.predict(X_test_2)
from sklearn.metrics import confusion_matrix
cm_PCA= confusion_matrix(y_test, y_pred_DT_2)
sns.heatmap(cm_PCA, annot=True)
plt.show()
ac_PCA = accuracy_score(y_test, y_pred_DT_2)
print("Accuracy Score:", ac_PCA)
classification_2=classification_report(y_test,y_pred_DT_2)
print(classification_2)
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.Set1,alpha = 1, edgecolor='k')
ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')
ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_pred_DT_2, cmap=plt.cm.Set1,alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Predicted label')
plt.show()

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0][y_test == 0], X_test[:, 1][y_test == 0], c='red',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 1], X_test[:, 1][y_test == 1], c='blue',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 2], X_test[:, 1][y_test == 2], c='yellow',alpha = 1, edgecolor='k')

ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')
ax[1].scatter(X_test[:, 0][y_pred_DT_2 == 0], X_test[:, 1][y_pred_DT_2 == 0], c='red',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred_DT_2 == 1], X_test[:, 1][y_pred_DT_2 == 1], c='blue',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred_DT_2 == 2], X_test[:, 1][y_pred_DT_2 == 2], c='yellow',alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Predicted label')
plt.show()
--
 DT After Applying LDA
# create the lda model
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
model = LinearDiscriminantAnalysis()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
model.fit(X_train,y_train)
LinearDiscriminantAnalysis()
y_pred = model.predict(X_test)
from sklearn.metrics import confusion_matrix
cm_LDA= confusion_matrix(y_test, y_pred)
sns.heatmap(cm_LDA, annot=True)
plt.show()

ac_LDA = accuracy_score(y_test, y_pred)
print("Accuracy Score:", ac_LDA)
Accuracy Score: 0.7666666666666667
classification_2=classification_report(y_test,y_pred)
print(classification_2)

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
# define model
model = LinearDiscriminantAnalysis()
# define model evaluation method
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
# define grid
grid = dict()
grid['solver'] = ['svd', 'lsqr', 'eigen']
#grid['shrinkage'] = np.arange(0, 1, 0.01)
# define search
search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)
# perform the search
results = search.fit(X_train, y_train)
# summarize
print('Mean Accuracy: %.3f' % results.best_score_)
print('Config: %s' % results.best_params_)
 
model = LinearDiscriminantAnalysis(solver = 'svd')
model1 = model.fit(X_train,y_train)
y_pred = model.predict(X_test)
from sklearn.metrics import confusion_matrix
cm_LDA= confusion_matrix(y_test, y_pred)
sns.heatmap(cm_LDA, annot=True)
plt.show()

ac_LDA = accuracy_score(y_test, y_pred)
print("Accuracy Score:", ac_LDA)
Accuracy Score: 0.7666666666666667
classification_2=classification_report(y_test,y_pred)
print(classification_2)
---
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.Set1,alpha = 1, edgecolor='k')
ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')
ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap=plt.cm.Set1,alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Predicted label')
plt.show()

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0][y_test == 0], X_test[:, 1][y_test == 0], c='red',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 1], X_test[:, 1][y_test == 1], c='blue',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 2], X_test[:, 1][y_test == 2], c='yellow',alpha = 1, edgecolor='k')

ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')
ax[1].scatter(X_test[:, 0][y_pred == 0], X_test[:, 1][y_pred == 0], c='red',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred == 1], X_test[:, 1][y_pred == 1], c='blue',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred == 2], X_test[:, 1][y_pred == 2], c='yellow',alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Predicted label')
plt.show()

fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(15, 5))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0][y_test == 0], X_test[:, 1][y_test == 0], c='red',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 1], X_test[:, 1][y_test == 1], c='blue',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 2], X_test[:, 1][y_test == 2], c='yellow',alpha = 1, edgecolor='k')
ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')

ax[1].scatter(X_test[:, 0][y_pred_DT == 0], X_test[:, 1][y_pred_DT == 0], c='red',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred_DT == 1], X_test[:, 1][y_pred_DT == 1], c='blue',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred_DT == 2], X_test[:, 1][y_pred_DT == 2], c='yellow',alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Classification Before PCA')


ax[2].scatter(X_test[:, 0][y_pred_DT_2 == 0], X_test[:, 1][y_pred_DT_2 == 0], c='red',alpha = 1,  edgecolor='k')
ax[2].scatter(X_test[:, 0][y_pred_DT_2 == 1], X_test[:, 1][y_pred_DT_2 == 1], c='blue',alpha = 1,  edgecolor='k')
ax[2].scatter(X_test[:, 0][y_pred_DT_2 == 2], X_test[:, 1][y_pred_DT_2 == 2], c='yellow',alpha = 1,  edgecolor='k')
ax[2].set_xlabel('PC1')
ax[2].set_ylabel('PC2')
ax[2].set_title('Classification After PCA')


ax[3].scatter(X_test[:, 0][y_pred == 0], X_test[:, 1][y_pred == 0], c='red',alpha = 1,  edgecolor='k')
ax[3].scatter(X_test[:, 0][y_pred == 1], X_test[:, 1][y_pred == 1], c='blue',alpha = 1,  edgecolor='k')
ax[3].scatter(X_test[:, 0][y_pred == 2], X_test[:, 1][y_pred == 2], c='yellow',alpha = 1,  edgecolor='k')
ax[3].set_xlabel('PC1')
ax[3].set_ylabel('PC2')
ax[3].set_title('Classification After LDA')
plt.show()

2. (a) Perform EDA and pre-processing techniques required for PCA and clustering. (10 marks)

Print the top 5 Eigenvalues and Eigenvectors. (4 marks)
df['culmen_length_mm']=df['culmen_length_mm'].fillna(df['culmen_length_mm'].median())
df['culmen_depth_mm']=df['culmen_depth_mm'].fillna(df['culmen_depth_mm'].median())
df['flipper_length_mm']=df['flipper_length_mm'].fillna(df['flipper_length_mm'].median())
df['body_mass_g']=df['body_mass_g'].fillna(df['body_mass_g'].median())
df['sex']=df['sex'].fillna(df['sex'].mode()[0])

df['sex'].value_counts()
MALE      178
FEMALE    165
.           1
Name: sex, dtype: int64
df['sex']=df['sex'].replace(to_replace='.',value=df['sex'].mode()[0])
df_num=df.select_dtypes(include='number')
df_c=df.select_dtypes(include='object')
snc=StandardScaler()
df_snc=pd.DataFrame(snc.fit_transform(df_num),columns=df_num.columns,index=df_num.index)
df_snc.head()
pc1=PCA()
inp_pc1=pc1.fit_transform(df_snc)
print('Top 5 Eigen values: ')
print(pc1.explained_variance_[:5])
print('Top 5 Eigen Vectors: ')
print(pc1.components_[:5])

2. (b) Calculate the optimal number of clusters for K-means, using the principal components that represent 90% of the explained variance.(use SilhouetteVisualizer) (6 marks)
cum_var=np.cumsum(pc1.explained_variance_ratio_)
num_pc=np.sum((cum_var<0.9).astype('int'))+1
print(f'Number of PCA components required to explain 90 percent of the variance in data is {num_pc}')
inp_df_pc=pd.DataFrame(inp_pc1[:,:3],columns=[f'PCA{i+1}' for i in range(num_pc)])
inp_df_pc.head()
wcss=[]
sc=[]
for i in range(2,20):
    kmeans1=KMeans(n_clusters=i,random_state=48)
    kmeans1.fit(inp_df_pc)
    wcss.append(kmeans1.inertia_)
    sc.append(silhouette_score(inp_df_pc,kmeans1.labels_))
plt.figure(figsize=(12,8))
plt.plot(range(2,20),wcss,c='r',marker='o')
plt.twinx()
plt.plot(range(2,20),sc,marker='*')
plt.axvline(3,c='orange')
plt.show()

for i in range(2,10):
    kmeans1=KMeans(n_clusters=i,random_state=48)
    kmeans1.fit(inp_df_pc)
    print(f'silhouette_score={silhouette_score(inp_df_pc,kmeans1.labels_)}')
    model=SilhouetteVisualizer(kmeans1)
    model.fit(inp_df_pc)
    model.show()
2. (c) Create dendrograms (top 100 cluster) using five different linkage methods and compare their performance, utilizing the principal components that explain 90% of the variance and calculate the cophenetic correlation coefficient for each linkage, display the results. (6 marks)
inp_df_pc.head()
linkage1=['single','complete','average','centroid','ward']
for lk in linkage1:
    lk1=linkage(inp_df_pc,method=lk)
    c,_=cophenet(lk1,pdist(inp_df_pc))
    print(f'Cophenet correlation for the linkage method {lk} is {c}')
    plt.figure(figsize=(16,10))
    dendrogram(lk1,truncate_mode='lastp',p=100)
    plt.show()

2 (d)Group the data into the optimal number of clusters and rank the clusters based on their quality, using the within-cluster sum of squares (WCSS) for each cluster. (6 marks)
df_clus0=df2[df2['Cluster_label']==0].drop(columns='Cluster_label')
df_clus1=df2[df2['Cluster_label']==1].drop(columns='Cluster_label')
df_clus2=df2[df2['Cluster_label']==2].drop(columns='Cluster_label')
df2['Cluster_label'].value_counts()
df_clus0_ar=np.array(df_clus0)
df_clus1_ar=np.array(df_clus1)
df_clus2_ar=np.array(df_clus2)
df_cent=np.array(kmeans2.cluster_centers_)
wcss1=[]
wcss1.append(((df_clus0_ar - df_cent[0,:])**2).sum())
wcss1.append(np.sum((df_clus1_ar - df_cent[1,:]) ** 2))
wcss1.append(np.sum((df_clus2_ar - df_cent[2,:]) ** 2))
wcss1
np.argsort(np.array(wcss1))
2 (e)Cluster the data using both K-Means and Agglomerative methods without applying PCA, and analyze the differences in their performance. (8 marks)
wcss3=[]
sc1=[]
for i in range(2,20):
    kmeans3=KMeans(n_clusters=i,random_state=48)
    kmeans3.fit(df_snc)
    wcss3.append(kmeans3.inertia_)
    sc1.append(silhouette_score(df_snc,kmeans3.labels_))
plt.figure(figsize=(12,8))
plt.plot(range(2,20),wcss3,c='r',marker='o')
plt.twinx()
plt.plot(range(2,20),sc1,marker='*')
plt.axvline(3,c='orange')
plt.show()

for i in range(2,10):
    kmeans4=KMeans(n_clusters=i,random_state=48)
    kmeans4.fit(df_snc)
    print(f'silhouette_score={silhouette_score(df_snc,kmeans4.labels_)}')
    model1=SilhouetteVisualizer(kmeans4)
    model1.fit(df_snc)
    model1.show()
print('The best number of cluster from the above analysis is 3 clusters')
The best number of cluster from the above analysis is 3 clusters
df3=df_snc.copy()
kmeans5=KMeans(n_clusters=3,random_state=48)
kmeans5.fit(df3)

df3['Cluster_label']=kmeans5.labels_
 
linkage1=['single','complete','average','centroid','ward']
for lk in linkage1:
    lk1=linkage(df_snc,method=lk)
    c,_=cophenet(lk1,pdist(df_snc))
    print(f'Cophenet correlation for the linkage method {lk} is {c}')
    plt.figure(figsize=(16,10))
    dendrogram(lk1,truncate_mode='lastp',p=100)
    plt.show()
agg1=AgglomerativeClustering(n_clusters=3)
agg1.fit(df_snc)
df4=df3.copy()
df4['Cluster_label']=agg1.labels_
agg2=AgglomerativeClustering(n_clusters=3)
agg2.fit(inp_df_pc)
print(f'Silhouette_score of Kmeans clustering with PCA is {silhouette_score(inp_df_pc,kmeans2.labels_)}')
print(f'Silhouette_score of Kmeans clustering without PCA is {silhouette_score(df_snc,kmeans5.labels_)}')
print(f'Silhouette_score of hierarchal clustering with PCA is {silhouette_score(inp_df_pc,agg2.labels_)}')
print(f'Silhouette_score of  hierarchal clustering without PCA is {silhouette_score(df_snc,agg1.labels_)}')
====
 Build the Apriori ML model with minimum support 10% and print values of Lift greater than 2. 
store_Data  = pd.readCSV()
final_data = pd.DataFrame(columns=['Transaction','Items'])
store_Data = df.T
for col in store_Data.columns:
    col_data = list(store_Data[col].dropna())
    temp_dict = {'Transaction':[int(col)]*len(col_data),  'Items':col_data }
    temp_df = pd.DataFrame(temp_dict)
    final_data = pd.concat([final_data,temp_df,],ignore_index=True)
final_data.head()

frequent_itemsets  =  apriori(format_data, min_support = 0.01, use_colnames = True)
frequent_itemsets.head()

final_data_dummy = pd.get_dummies(final_data['Items'])
final_data_dummy['Transaction'] = final_data['Transaction']
# i don't want the quantity, I want just that product is bought or #not(after doing dummy yow will get 2 if same item bought twice in a #transaction) so i will use encode_units function
def encode_units(x):
    if x <=  0:
        return 0
    if x >=  1:
        return 1
format_data= final_data_dummy.groupby('Transaction').sum()
format_data = format_data.applymap(encode_units)
format_data.head()

 result = association_rules(frequent_itemsets, metric = "lift" )
result.sort_values('lift',ascending=False)



#Content Based Recommendation system
df_cb=pd.read_csv('movie_metadata-2.csv')
df_cb.head()

df_cb1=df_cb[['imdb_score','movie_title','genres']]
geners=df_cb['genres'].str.split('|',expand=True)
geners.head()

## Now I choose only first 3 columns of geners
geners1=geners.iloc[:,:3]
geners1.head()

geners1.columns=['genre1','genre2','genre3']
geners1.head()

df_cb2=pd.concat([df_cb1,geners1],axis=1)
df_cb2.drop(columns='genres',inplace=True)
df_cb2.head()

df_cb3=df_cb2.set_index('movie_title')
df_cb3.head()

df_cb3.index=df_cb3.index.str.strip()
df_inp=pd.get_dummies(df_cb3,drop_first=True,dtype='int')
df_inp.head()

from sklearn.neighbors import NearestNeighbors
rknn=NearestNeighbors()
rknn.fit(df_inp)
vie_near='The Dark Knight Rises'
input1=df_inp.loc[movie_near].to_numpy().reshape(1,-1)
ndist,nindex=rknn.kneighbors(input1,n_neighbors=6)
print(f'Movies near to {movie_near} is : ')
print()
for i in nindex[:,1:].tolist()[0]:
    print(df_inp.index[i])
---
Need for Linkage Methods:
•	Used in Hierarchical Clustering to determine how clusters are merged.
•	Different methods affect the shape and size of final clusters.
Types of Linkage Methods:
1.	Single Linkage: Distance between closest points in clusters.
o	Can form long chains of clusters.
2.	Complete Linkage: Distance between farthest points in clusters.
o	Forms compact clusters.
3.	Average Linkage: Average distance between all points in clusters.
o	Balanced between single and complete linkage.
4.	Centroid Linkage: Distance between the centroids of clusters.
o	Less sensitive to outliers.

Supervised Learning:
1.	Definition: Uses labeled data where input-output pairs are known.
2.	Objective: Learns a function to map inputs to outputs.
3.	Common Algorithms: Decision Trees, SVM, Neural Networks.
4.	Example: Email classification (spam or not spam).
5.	Training Required: Needs labeled data for training.
6.	Accuracy: Usually more accurate as it learns from explicit labels.
7.	Use Case: Fraud detection, image recognition.
Unsupervised Learning:
1.	Definition: Works with unlabeled data to find patterns.
2.	Objective: Groups similar data points into clusters.
3.	Common Algorithms: K-Means, DBSCAN, PCA.
4.	Example: Customer segmentation in marketing.
5.	Training: No explicit labels are given.
6.	Accuracy: May be lower but useful for exploratory analysis.
7.	Use Case: Anomaly detection, recommendation systems.

SVD vs PCA
SVD is a factorization method that decomposes a matrix into three other matrices: U, Σ (sigma), and V^T (transpose of V). Here’s a simplified breakdown:

Matrix A (m x n) is decomposed into U (m x m), Σ (m x n), and V^T (n x n).
U contains orthogonal columns that represent the left singular vectors.
Σ is a diagonal matrix containing the singular values.
V^T contains orthogonal rows representing the right singular vectors.
Applications of SVD
Matrix Approximation: SVD is used to approximate a matrix with lower rank, retaining the most significant singular values.
Recommendation Systems: Collaborative filtering in recommendation systems leverages SVD to find latent factors that describe user preferences and item characteristics.
Image Compression: SVD can compress images by capturing the most significant singular values and vectors.
Principal Component Analysis (PCA)
PCA is a statistical procedure that aims to transform data into a new coordinate system where the axes are the principal components. These components are orthogonal and capture the maximum variance in the data. Here’s a simplified overview:

Standardize the Data: Normalize the data to have zero mean and unit variance.
Calculate Covariance Matrix: Compute the covariance matrix of the standardized data.
Compute Eigenvectors and Eigenvalues: Compute the eigenvectors and eigenvalues of the covariance matrix.
Select Principal Components: Sort eigenvalues in descending order and choose the top-k eigenvalues to form principal components.
Transform Data: Project the original data onto the principal components to create a lower-dimensional representation.
Applications of PCA
Data Visualization: PCA helps visualize high-dimensional data in 2D or 3D plots while retaining important patterns.
Noise Reduction: By focusing on the principal components, PCA can reduce noise and redundancy in the data.
Dimensionality Reduction: PCA transforms data into a lower-dimensional space, preserving most of the variability.
Comparing PCA and SVD
Mathematical Relationship: PCA can be seen as a specific application of SVD. The principal components obtained through PCA are essentially the left singular vectors of the data matrix.
Purpose: PCA is primarily used for dimensionality reduction and visualization, while SVD has broader applications, including matrix approximation and recommendation systems.
Orthogonality: Both PCA and SVD produce orthogonal vectors (components) that capture uncorrelated directions of maximum variance.
Eigenvalues vs. Singular Values: PCA involves computing eigenvectors and eigenvalues of the covariance matrix, while SVD directly deals with singular values.
* silhouette score
The silhouette score is specialized for measuring cluster quality when the clusters are convex-shaped, and may not perform well if the data clusters have irregular shapes or are of varying sizes. The silhouette can be calculated with any distance metric, such as the Euclidean distance or the Manhattan distance.

Most clustering approaches use distance measures to assess the similarities or differences between a pair of objects, the most popular distance measures used are:
Euclidean Distance: ...
Manhattan Distance: ...
Jaccard Index: ...
Minkowski distance: ..


Let us calculate the Cophenetic correlation coefficient to study the quality of clusters formed using dendrogram.
from sklearn.metrics.pairwise import euclidean_distances
from scipy.cluster.hierarchy import cophenet

# instantiate linkage object with scaled data and consider 'average' linkage method 
# calculate the euclidean distance between the observations 
eucli_dist = euclidean_distances(X)

# the above code will return the matrix of 3756x3756
# consider only the array of upper triangular matrix
# k=1 considers the upper triangular values without the diagonal elements
dist_array = eucli_dist[np.triu_indices(3756, k = 1)]

# pass the linkage matrix and actual distance
# 1st output of the cophenet() is the correlation coefficient
coeff, cophenet_dist = cophenet(link_mat, dist_array)
Task 2: Prepare a segregation model to justify the rating category of anew movie.
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(data2['content_rating'])
print(y)

from sklearn.metrics import confusion_matrix
from sklearn import tree
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(np.array(X), y, test_size = 0.2, random_state = 0)
3.1 DT Classifier Before Applying PCA
from sklearn.ensemble import BaggingClassifier
model = BaggingClassifier()
model = model.fit(X_train, y_train)
y_pred_DT = model.predict(X_test)
cm_DT= confusion_matrix(y_test, y_pred_DT)
cm_DT
sns.heatmap(cm_DT, annot=True)
plt.show()
classification=classification_report(y_test,y_pred_DT)
print(classification)
ac = accuracy_score(y_test, y_pred_DT)
print("Accuracy Score:", ac)
y_test[:5]

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.Set1,alpha = 1, edgecolor='k')
ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')
ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_pred_DT, cmap=plt.cm.Set1,alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Predicted label')
plt.show()

3.2 DT After Applying PCA
from sklearn.decomposition import PCA
pca = PCA()
X_train_2 = pca.fit_transform(X_train)
X_test_2 = pca.transform(X_test)
explained_variance = pca.explained_variance_ratio_  
explained_variance
sns.barplot(np.arange(len(pca.explained_variance_ratio_ )),pca.explained_variance_ratio_ )
plt.show()

from sklearn import tree
model2 = BaggingClassifier()

model2.fit(X_train_2,y_train)
BaggingClassifier()
y_pred_DT_2 = model2.predict(X_test_2)
from sklearn.metrics import confusion_matrix
cm_PCA= confusion_matrix(y_test, y_pred_DT_2)
sns.heatmap(cm_PCA, annot=True)
plt.show()

ac_PCA = accuracy_score(y_test, y_pred_DT_2)
print("Accuracy Score:", ac_PCA)

classification_2=classification_report(y_test,y_pred_DT_2)
print(classification_2)

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.Set1,alpha = 1, edgecolor='k')
ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')
ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_pred_DT_2, cmap=plt.cm.Set1,alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Predicted label')
plt.show()


3.3 DT After Applying LDA
# create the lda model
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
model = LinearDiscriminantAnalysis()
model.fit(X_train,y_train)

y_pred = model.predict(X_test)
from sklearn.metrics import confusion_matrix
cm_LDA= confusion_matrix(y_test, y_pred)
sns.heatmap(cm_LDA, annot=True)
plt.show()
ac_LDA = accuracy_score(y_test, y_pred)
print("Accuracy Score:", ac_LDA)
classification_2=classification_report(y_test,y_pred)
print(classification_2)

3.3.2Hyperparameter tunning of LDA
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
# define model
model = LinearDiscriminantAnalysis()
# define model evaluation method
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
# define grid
grid = dict()
grid['solver'] = ['svd', 'lsqr', 'eigen']
#grid['shrinkage'] = np.arange(0, 1, 0.01)
# define search
search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)
# perform the search
results = search.fit(X_train, y_train)
# summarize
print('Mean Accuracy: %.3f' % results.best_score_)
print('Config: %s' % results.best_params_)

model = LinearDiscriminantAnalysis(solver = 'svd')
model1 = model.fit(X_train,y_train)
y_pred = model.predict(X_test)
from sklearn.metrics import confusion_matrix
cm_LDA= confusion_matrix(y_test, y_pred)
sns.heatmap(cm_LDA, annot=True)
plt.show()
ac_LDA = accuracy_score(y_test, y_pred)
print("Accuracy Score:", ac_LDA)
classification_2=classification_report(y_test,y_pred)
print(classification_2)

ig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.Set1,alpha = 1, edgecolor='k')
ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')
ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap=plt.cm.Set1,alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Predicted label')
plt.show()

----

from sklearn.cluster import KMeans
# create several cluster combinations ranging from 2 to 80 and observe the wcss (Within Cluster Sum of Squares) for each cluster
# consider an empty list to store the WCSS
wcss  = []

# use for loop to perform K-means with different values of K
# set the 'random_state' to obtain the same centroid initialization for each code run
# fit the model on scaled data
# append the value of WCSS for each K to the list 'wcss'
# the 'inertia_' retuns the WCSS for specific value of K
for i in range(1,41):
    kmeans = KMeans(n_clusters = i, random_state = 10)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
Let us plot the elbow plot and identify the elbow point.

plt.figure(figsize = (20,10))
# visualize the elbow plot to get the optimal value of K
plt.plot(range(1,41), wcss)

# set the axes and plot labels
# set the font size using 'fontsize'
plt.title('Elbow Plot', fontsize = 15)
plt.xlabel('No. of clusters (K)', fontsize = 15)
plt.ylabel('WCSS', fontsize = 15)
plt.yticks(np.arange(0,65000,5000))

# display the plot
plt.show()

plt.figure(figsize = (20,10))
# visualize the elbow plot to get the optimal value of K
plt.plot(range(1,41), wcss)

# set the axes and plot labels
# set the font size using 'fontsize'
plt.title('Elbow Plot', fontsize = 15)
plt.xlabel('No. of clusters (K)', fontsize = 15)
plt.ylabel('WCSS', fontsize = 15)
plt.yticks(np.arange(0,65000,5000))
plt.xticks(range(1,41))

# plot a vertical line at the elbow
plt.axvline(x = 11 , color = 'red')

# display the plot
plt.show()

om sklearn.metrics import silhouette_score

# create a list for different values of K
n_clusters = np.arange(2,81)
sil = []
# use 'for' loop to build the clusters
# 'random_state' returns the same sample each time you run the code  
# fit and predict on the scaled data
# 'silhouette_score' function computes the silhouette score for each K
for K in n_clusters:
    cluster = KMeans (n_clusters= K, random_state= 10)
    predict = cluster.fit_predict(X)
    score = silhouette_score(X, predict, random_state= 10)
    sil.append(score)
    print ("For {} clusters the silhouette score is {})".format(K, score))


plt.figure(figsize = (20,10))

# visualize the elbow plot to get the optimal value of K
plt.plot(np.arange(2,81), sil)

# set the axes and plot labels
# set the font size using 'fontsize'
plt.title('Elbow Plot', fontsize = 20)
plt.xlabel('No. of clusters (K)', fontsize = 20)
plt.ylabel('silhouette score', fontsize = 20)
plt.xticks(np.arange(2,81))
# plot a vertical line at the elbow
plt.axvline(x = 11, color = 'red')

# display the plot
plt.show()

2. (b).Find the optimal number of clusters for the K-means clustering model [Note: Use the PCs, which are explaining the 90% variance]. (6 marks)

pca_90= PCA(n_components=0.9)
pca_df=pca_90.fit_transform(df_scaled)

sse=[]

for i in range(1,11):
  Kmeans=KMeans(n_clusters=i,init='k-means++',random_state=42)
  Kmeans.fit(pca_df)
  sse.append(Kmeans.inertia_)

# Plot the elbow curve
plt.figure(figsize=(10, 6))
plt.plot(range(1,11), sse, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Sum of Squared Errors (SSE)')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.show()

--
silhouette_scores = []
max_k = 10  # Maximum number of clusters to try

for k in range(2, max_k + 1):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(pca_df)
    labels = kmeans.labels_
    silhouette_scores.append(silhouette_score(pca_df, labels))

best_k = np.argmax(silhouette_scores) + 2

print(f"Optimal k (Silhouette Score): {best_k}")

# Plotting Silhouette Scores
plt.figure(figsize=(8, 6))
plt.plot(range(2, max_k + 1), silhouette_scores, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Analysis for Optimal k')
plt.xticks(range(2, max_k + 1))
plt.grid(True)
plt.show()


linkage_methods = ['single', 'complete', 'average', 'ward']
plt.figure(figsize=(20, 15))

for i, method in enumerate(linkage_methods, 1):
    plt.subplot(2, 2, i)
    mergings = linkage(pca_df, method=method, metric='euclidean')
    dendrogram(mergings, truncate_mode='lastp', p=100)
    plt.title(f'Dendrogram ({method} linkage)')
plt.show()

2 (d)Cluster the data into 4 groups and order the cluster quality in terms of the inertia (WCSS) of each cluster. [Use ward linkage metric] (6 marks)

from sklearn.cluster import AgglomerativeClustering

# Cluster the data into 4 groups using ward linkage
agg_clustering = AgglomerativeClustering(n_clusters=4, linkage='ward')
df['Cluster'] = agg_clustering.fit_predict(pca_df)

# Calculate WCSS for each cluster
wcss = []
for i in range(4):
    cluster_data = df[df['Cluster'] == i]
    wcss.append(np.sum(((cluster_data - cluster_data.mean(axis=0)).values)**2))

# Order clusters by WCSS
ordered_clusters = sorted(range(4), key=lambda k: wcss[k])
print("Ordered Clusters by WCSS (Inertia):")
print(ordered_clusters)
#### 2 (e)Compare the quality of clusters for K-means clustering algorithm on original data and PCA transformed data. (8 marks)

# K-means clustering on original data
kmeans_original = KMeans(n_clusters=3, random_state=42)
clusters_original = kmeans_original.fit_predict(df_scaled)
wcss_original = kmeans_original.inertia_

# K-means clustering on PCA transformed data
kmeans_pca = KMeans(n_clusters=3, random_state=42)
clusters_pca = kmeans_pca.fit_predict(pca_df)
wcss_pca = kmeans_pca.inertia_

# Compare the quality of clusters
print("WCSS for Original Data:", wcss_original)
print("WCSS for PCA Transformed Data:", wcss_pca)
2. (c). Plot the dendrograms using 4 linkage methods for the PCA transformed data and identify which one is the best. [Note: Use the PCs, which are explaining the 90% variance] (6 marks)
linkage_methods = ['single', 'complete', 'average', 'ward']
plt.figure(figsize=(20, 15))

ince the WCSS for the PCA transformed data (100496.43030580929) is lower than that for the original data (112609.01071221931), the clustering on the PCA transformed data is better. This suggests that reducing the dimensionality using PCA has helped in creating more compact and well-defined clusters.

#### 3 (a) Develop a popularity-driven recommendation system, print Total no of ratings,Total No of Users,Total No of products  and recommend the top 5 items. (10 marks )

Use the dataset: Book_ratings.csv
Variables:
book_id - ID of the book.
user_id - ID of the user rated
ratings - ratings of the book by the user.
df=pd.read_csv('/content/drive/MyDrive/ML3 ESA/ML3 ESA/Question-ML3-Mar2024/Question/Book_ratings.csv')
Build collaborative recommendation engine. Use KNNBasic library with cosine similarity and measure the model quality by performing cross validation in terms of RMSE.
Use the dataset: Book_ratings.csv (20 Marks)
reader= Reader(rating_scale=(1,10))
data=Dataset.load_from_df(df[['user_id','book_id','rating']],reader)
trainset, testset= train_test_split(data,test_size=0.2,random_state=42)

sim_options = {
    'name': 'cosine',
    'user_based': True
}
model= KNNBasic(sim_options=sim_options)
model.fit(trainset)
predictions=model.test(testset)

rmse = accuracy.rmse(predictions)
print("RMSE:", rmse)


3 (c) Create association rule mining using apriori algorithm.Perform basic pre-processing operations required by algorithm ( drop missing values, drop unnecessary columns). (7 marks)

Create the basket only for France. Run algorithm with minimum support 0.07, tune with lift. (3 marks)
Use Dataset:Online Retail.csv
df = df.drop(columns=['Country', 'CustomerID', 'InvoiceDate'])
basket = df.groupby(['InvoiceNo', 'Description'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')
basket = basket.applymap(lambda x: 1 if x > 0 else 0)

# Run Apriori algorithm with minimum support 0.07
frequent_itemsets = apriori(basket, min_support=0.07, use_colnames=True)

# Create association rules and tune with lift
rules = association_rules(frequent_itemsets, metric='lift')

print("Frequent Itemsets:")
print(frequent_itemsets)
print("Association Rules:")
print(rules)

---Sept21--
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import linkage , dendrogram, fcluster,cophenet
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.decomposition import TruncatedSVD
from surprise import KNNWithMeans,SVDpp
from surprise import Dataset
from surprise import accuracy
from surprise.model_selection import train_test_split,cross_validate
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
from surprise import KNNBasic
from sklearn.metrics import silhouette_score
from yellowbrick.cluster import SilhouetteVisualizer
from csv import reader
import warnings

corr_matrix = df.corr()

# 3. Find pairs of variables with correlation greater than 0.8
correlation_pairs = np.sum(np.triu(np.abs(corr_matrix) > 0.8, k=1))

print(f"Number of variable pairs with correlation > 0.8: {correlation_pairs}")
---PCA 90%--
pca= PCA(n_components=90)
pca_scaled= pca.fit_transform(df_scaled)
print("Eigenvalues",pca.explained_variance_[:5])
print("Eigenvectors",pca.components_[:5])

Build the K-means clustering model with reduced PCA features (PCs which are explaining 90 percent variance) and compute the optimal value of clusters. Make the business inferences using the cluster groups.
---Build the K-means clustering model with reduced PCA features-----
inertia = []
for k in range(1, 11):  # Test for 1 to 10 clusters
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(pca_scaled)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8, 6))
plt.plot(range(1, 11), inertia, marker='o')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.show()

---silhouette_scores---
silhouette_scores = []
for k in range(2, 11):  # Silhouette score requires at least 2 clusters
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(pca_scaled)
    score = silhouette_score(pca_scaled, kmeans.labels_)
    silhouette_scores.append(score)

plt.figure(figsize=(8, 6))
plt.plot(range(2, 11), silhouette_scores, marker='o')
plt.title('Silhouette Score for Optimal K')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.show()
--Build/Plot the top 100 cluster dendogram using 4 different linkages and compare its performance.--
linkages = ['ward', 'complete', 'average', 'single']
plt.figure(figsize=(15, 10))

for i, method in enumerate(linkages):
    plt.subplot(2, 2, i+1)
    Z = linkage(pca_scaled[:100], method=method)
    dendrogram(Z,p=100)
    plt.title(f'Dendrogram ({method} linkage)')

plt.tight_layout()
plt.show()

--Cluster kmeans--- 
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(pca_scaled)
inertia = kmeans.inertia_
print(f"Inertia (WCSS) of the clusters: {inertia}")
clusters_ordered_by_inertia = sorted(range(len(kmeans.cluster_centers_)), key=lambda k: kmeans.inertia_) # Sort by inertia of the model
print(f"Clusters ordered by inertia (WCSS): {clusters_ordered_by_inertia}")
from sklearn.preprocessing import LabelEncoder
LableEncoder 
label_encoder= LabelEncoder()
y=label_encoder.fit_transform(df['activity'])
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)
--train_evaluate_model--
def train_evaluate_model(X_train, X_test, y_train, y_test):
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return accuracy_score(y_test, y_pred)

from sklearn.metrics import accuracy_score
accuracy_original = train_evaluate_model(X_train, X_test, y_train, y_test)

pca = PCA(n_components=0.95)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)
accuracy_pca = train_evaluate_model(X_train_pca, X_test_pca, y_train, y_test)

n_components= pca.n_components_
svd = TruncatedSVD(n_components=n_components)
X_train_svd = svd.fit_transform(X_train)
X_test_svd = svd.transform(X_test)
accuracy_svd = train_evaluate_model(X_train_svd, X_test_svd, y_train, y_test)

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=1)
X_train_lda = lda.fit_transform(X_train, y_train)
X_test_lda = lda.transform(X_test)
accuracy_lda = train_evaluate_model(X_train_lda, X_test_lda, y_train, y_test)


print(f"Accuracy with original data: {accuracy_original}")
print(f"Accuracy with PCA data: {accuracy_pca}")
print(f"Accuracy with SVD data: {accuracy_svd}")
print(f"Accuracy with LDA data: {accuracy_lda}")
