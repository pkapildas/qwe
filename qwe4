<<<Cheat Sheet>
 import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 from sklearn.preprocessing import StandardScaler
 from sklearn.decomposition import PCA
 from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN
 from sklearn.metrics import silhouette_score
 from scipy.cluster.hierarchy import linkage , dendrogram, fcluster,cophenet
 from scipy.spatial.distance  import pdist
 #from sklearn.model_selection import train_test_split
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.metrics import classification_report
 from sklearn.decomposition import TruncatedSVD
 from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
 from surprise import KNNWithMeans,SVDpp
 from surprise import Dataset
 from surprise import accuracy
 from surprise import Reader
 from surprise.model_selection import train_test_split,cross_validate
 from mlxtend.frequent_patterns import apriori
 from mlxtend.frequent_patterns import association_rules
 from surprise import KNNBasic
 import warnings
 warnings.filterwarnings("ignore")


#  df['sex'].value_counts() #-------Check ? or.
 #  df['CREDIT_LIMIT'].sort_values(ascending=False).head()
 #  df['MINIMUM_PAYMENTS'].sort_values(ascending=False).head()
 df['CREDIT_LIMIT'].replace(to_replace='?',value=np.nan,inplace=True)
 df['MINIMUM_PAYMENTS'].replace(to_replace='?',value=np.nan,inplace=True)
 print(df.columns)
 #d  f['cul']=df['culmen_length_mm'].fillna(df['cul'].median())
 #   df1=pd.get_dummies(df,drop_first=True)
 #   df.isnull().sum()/df.shape[0]*100
 df = df.dropna()
 df = df.reset_index(drop = True)
 df.head(2)
 # df.isnull().sum()

df['CREDIT_LIMIT']    =df['CREDIT_LIMIT'].astype('float64')
 df['MINIMUM_PAYMENTS']=df['MINIMUM_PAYMENTS'].astype('float64')

snc=StandardScaler()
 df_snc=pd.DataFrame(snc.fit_transform(df),columns=df.columns,index=df.index)
 df_snc.head(2)

pc1=PCA()
 inp_pc1=pc1.fit_transform(df_snc)
 print('Top 5 Eigen values: \n' , pc1.explained_variance_[:5])
 print('Top 5 Eigen vectors:\n' ,pc1.components_[:2])

cum_var=np.cumsum(pc1.explained_variance_ratio_)
 num_pc=np.sum((cum_var<0.9).astype('int'))+1;
 print(num_pc)

#10 till 90
 inp_red_pc_90=pd.DataFrame(inp_pc1[:,:num_pc],
                           columns=[f'PCA{i+1}' for i in range(num_pc)])
 inp_red_pc_90.head(2)

# 2. (b).Find the optimal number of clusters for the K-means clustering model 
#        [Note: Use the PCs, which are explaining the 90% variance].  (6 marks)
 wcss =[]
 sc   =[]
 for i in range(2,20):
    kmeans1=KMeans(n_clusters=i,random_state=48)
    kmeans1.fit(inp_red_pc_90)
    wcss.append(kmeans1.inertia_)
    sc.append(silhouette_score(inp_red_pc_90,kmeans1.labels_))
 plt.figure(figsize=(6,3))
 plt.plot(range(2,20),wcss,marker='o',c='r',label='wcss')
 plt.legend(loc=1)
 plt.twinx()
 plt.plot(range(2,20),sc,marker='*',c='b',label='silhouette_score')
 plt.legend(loc=3)
 plt.axvline(3,c='orange')
 plt.show()
 kmeans2=KMeans(n_clusters=3,random_state=48)
 kmeans2.fit(inp_red_pc_90)
 df2=inp_red_pc_90.copy()
 df2['Cluster_Labels']=kmeans2.labels_
 df2.head(2)

 # 2. (c). Plot the dendrograms using 4 linkage methods for the PCA transformed d
 #         [Note: Use the PCs, which are explaining the 90% variance] (6 marks)
 #         Hint:Use the following functions and create the dendrograms for 100 no
 #              linkage(pca_df, method='single',metric='euclidean')
 #              dendrogram(mergings,truncate_mode='lastp',p=100)
 linkage1=['ward']   #'single','complete','average','centroid','ward']
 for lk in linkage1:
    link1=linkage(inp_red_pc_90,method=lk)
    c,_=cophenet(link1,pdist(inp_red_pc_90))
    print(f'Cophenet correlation for linkage method {lk} is {c}')
    plt.figure(figsize=(10,1))
    dendrogram(link1,truncate_mode='lastp',p=100)
    plt.show()

# 2 (d)Cluster the data into 4 groups and order the cluster quality in terms of 
#      [Use ward linkage metric] (6 marks)
 df3=inp_red_pc_90.copy()
 agg1=AgglomerativeClustering(n_clusters=4)
 agg1.fit(inp_red_pc_90)
 df3['Cluster_Labels']=agg1.labels_
 df3.head(2)

# Extracting clusters
 df_clus0 = df3[df3['Cluster_Labels'] == 0].drop(columns='Cluster_Labels')
 df_clus1 = df3[df3['Cluster_Labels'] == 1].drop(columns='Cluster_Labels')
 df_clus2 = df3[df3['Cluster_Labels'] == 2].drop(columns='Cluster_Labels')
# Converting to arrays
 df_clus0_ar = np.array(df_clus0)
 df_clus1_ar = np.array(df_clus1)
 df_clus2_ar = np.array(df_clus2)

# Calculating centroids
 df_clust_cent0 = df_clus0.mean(axis=0).values
 df_clust_cent1 = df_clus1.mean(axis=0).values
 df_clust_cent2 = df_clus2.mean(axis=0).values

 # Calculating WCSS for each cluster
 wcss1 = []
 wcss1.append(np.sum((df_clus0_ar - df_clust_cent0) ** 2))  # WCSS for cluster 0
 wcss1.append(np.sum((df_clus1_ar - df_clust_cent1) ** 2))  # WCSS for cluster 1
 wcss1.append(np.sum((df_clus2_ar - df_clust_cent2) ** 2))  # WCSS for cluster 2

for i, wcss in enumerate(wcss1):                 # Display WCSS for each cluster
    print(f"Cluster {i}: WCSS = {wcss:.2f}")
 print(np.argsort(wcss1))

 2 (e)Compare the quality of clusters for K-means clustering algorithm on origi
 #      data and PCA transformed data. (8 marks)
wcss=[]
 sc=[]
 for i in range(2,20):
    kmeans3=KMeans(n_clusters=i,random_state=48)
    kmeans3.fit(df_snc)
    wcss.append(kmeans3.inertia_)
    sc.append(silhouette_score(df_snc,kmeans3.labels_))
 plt.figure(figsize=(6,3))
 plt.plot(range(2,20),wcss,marker='o',c='r',label='wcss')
 plt.legend(loc=1)
 plt.twinx()
 plt.plot(range(2,20),sc,marker='*',c='b',label='silhouette_score')
 plt.legend(loc=3)
 plt.axvline(3,c='orange')
 plt.show()
 kmeans4=KMeans(n_clusters=3,random_state=48)
 kmeans4.fit(df_snc)
 var_2e1 = silhouette_score(df_snc,kmeans4.labels_)
 var_2e2 = silhouette_score(inp_red_pc_90,kmeans2.labels_)
 print(f'Silhouette_score of Kmeans clustering with PCA is {var_2e1}')
 print(f'Silhouette_score of Kmeans clustering without PCA is {var_2e2}')

# 3 (a) Develop a popularity-driven recommendation system, 
# print Total no of ratings,Total No of Users,Total No of products
 # and recommend the top 5 items. (10 marks )
 # Use the dataset: Book_ratings.csv
 # Variables:
 # book_id - ID of the book.
 # user_id - ID of the user rated
 # ratings - ratings of the book by the user.
 df_p=pd.read_csv(path18)   #'Book_ratings.csv')
 df_p=df_p.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])
 df_p=df_p[['user_id','book_id','rating']]
 df_p.head()
 df_r=df_p.groupby('book_id')['rating'].agg(['count','mean'])
 df_r.columns=['Rating_count','Average_Rating']
df_r.sort_values('Rating_count',ascending=False).head()
 print(f'Total no of ratings: {df_r.Rating_count.count()}')
 print(f'Total No of Users: {df_p.user_id.nunique()}')
 print(f'Total No of products: {df_p.book_id.nunique()}')

## Recommendation Engine
df_c=df_p.copy()                                                          
# Load the dataset
 reader1=Reader(rating_scale=(df_c['rating'].min(),df_c['rating'].max()))  
# Create a Reader object
 dataset=Dataset.load_from_df(df_c,reader=reader1)                         
# Load the dataset into Surprise's Dataset format
 [trainset,testset]=train_test_split(dataset,test_size=0.2,random_state=48) 
# Split the data into training and testing sets
 #svd1=SVD() ; svd1.fit(trainset)
 model=KNNBasic(sim_options={'name':'cosine','user_based':True})            
# Build the collaborative filtering model (user-based in this case)
 model.fit(trainset)                                  # Train the model
 ypred_knn=model.test(testset)                        # Make predictions on the t
 print(accuracy.rmse(ypred_knn) )                     # Calculate RMSE
 # Perform cross-validation
 cv_km = cross_validate(model, dataset, measures=['RMSE'], cv=5)
 np.mean(cv_km['test_rmse']) # Print the average RMSE across folds

 df_p.head(2)

 user1='116'
 item_visited=set(df_p[df_p['user_id']==user1]['book_id'])
 item_all=set(df_p['book_id'])
 item_not_visted=list(item_all.difference(item_visited))
 ls=[]
 for i in item_not_visted:
 #   ls.append({'book_id':i,'rating':cv_knn.predict(uid=user1,iid=i).est})
    ls.append({'book_id':i,'rating': model.predict(uid=user1,iid=i).est})
 df_pred=pd.DataFrame(ls)
 df_pred.sort_values('rating',ascending=False); df_pred.head(2)

 #3 (c) Create association rule mining using apriori algorithm.
 #             Perform basic pre-processing operations required by algorithm
 #            (drop missing values, drop unnecessary columns). 
#      Create the basket only for France. Run algorithm
 #             with minimum support 0.07, tune with lift. (3 marks)
 #             Use Dataset:Online Retail.csv
 import pandas as pd
 import numpy as np
 from mlxtend.frequent_patterns import apriori
 from mlxtend.frequent_patterns import association_rules
 df_ap=pd.read_csv(path19) #'Online Retail.csv')
 df_ap.head()
 df=df_ap[df_ap['Country']=='France']
 df1=df[['InvoiceNo','Description','Quantity']]
 df2=df1[~df1['InvoiceNo'].str.startswith('C')]
 df3=df2.groupby(['InvoiceNo','Description'])['Quantity'].sum()
 df3.head(3)

df4=df3.unstack()
 df5=df4.reset_index()
 df6=df5.fillna(0)
 df7=df6.set_index('InvoiceNo')
 df7.head(2)

def encode(x):
    return 1 if x>0.0 else 0
 df8=df7.applymap(encode)
 df8.head()
 frequent_itemsets=apriori(df8,min_support=0.07,use_colnames=True)
 rules=association_rules(frequent_itemsets,metric='lift',min_threshold=1)
 rules=rules.sort_values(by='lift',ascending=False)
 rules.head(2)

## APIORI 2nd Example
 #3 (c) dataset for stores - apiori
 df_ap=pd.read_csv(path20) #'Online Retail.csv')
 df_ap.head(2)
 df_ap=df_ap.fillna(0)
 df_ap.head(3)
 # Convert the dataset to a list of transactions
 transactions = []
 for i in range(0, df_ap.shape[0]):
    transaction = df_ap.iloc[i].dropna().tolist()  # Remove NaN values and conve
    filtered_transaction = [str(item) for item in transaction if not str(item).i
    transactions.append(frozenset(filtered_transaction))  # Use frozenset to mak
 # Remove duplicates by converting to a set and back to a list


unique_transactions = [set(map(str, transaction)) for transaction in list(set(tr
 # One-hot encode the list of transactions
 from mlxtend.preprocessing import TransactionEncoder
 te = TransactionEncoder()
 te_array = te.fit(unique_transactions).transform(unique_transactions)
 df8 = pd.DataFrame(te_array, columns=te.columns_).astype(int)  # Convert True/Fa
 # Display the one-hot encoded dataframe
 print("One-Hot Encoded Data (0/1):")
 df8.head(1)

# Apply the Apriori algorithm with a minimum support threshold of 0.01
 frequent_itemsets = apriori(df8, min_support=0.01, use_colnames=True)
 # Generate association rules using lift as the metric
 rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)
 rules=rules.sort_values(by='lift',ascending=False)
 # Display frequent itemsets
 print("\nFrequent Itemsets:") ; print(frequent_itemsets.head(10))
 # Display the top 10 rules
 print("\nAssociation Rules:")
 rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10)


---------------------


# Step 1: Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, cohen_kappa_score, confusion_matrix, roc_curve, auc
from scipy.stats import zscore
from imblearn.over_sampling import SMOTE
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Step 2: Load Dataset
df = pd.read_csv("loan.csv")

# -------------------- (A) Data Exploration --------------------
print("\nDataset Shape:", df.shape)
print("\nData Info:\n", df.info())
print("\nMissing Values:\n", df.isnull().sum())

# Identify Numerical & Categorical Columns
numerical_cols = df.select_dtypes(include=['number']).columns.tolist()
categorical_cols = df.select_dtypes(exclude=['number']).columns.tolist()

print("\nNumerical Columns:", numerical_cols)
print("\nCategorical Columns:", categorical_cols)

data_clust.isnull().sum().sum()
#Keep the input and output column seperate
inp_data_dime=data_clust.drop('M3',axis=1)
out=data_clust['M3']
For dimensionality reduction scaling is the must do pre-processing step. Students must perform scaling prior to dimensionality reduction. 
(Students may also performed outlier treatment, which is good to consider for awarding more marks). If they have not performed scaling, then mark must be reduced drastically.


from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
data_sc = scaler.fit_transform(inp_data_dime)
data_sc=pd.DataFrame(data_sc,columns=inp_data_dime.columns)
#1.b Perform univariate and bivariate analysis and remove any variable which is absolutely 
#    insignificant. (2 Marks)
inp_data_dime.describe()
out.value_counts().plot(kind='bar')
--
import seaborn as sb
from matplotlib import pyplot as plt
plt.figure(figsize=(18,8))
sb.heatmap(inp_data_dime.corr(),annot=True)

for i in inp_data_dime.columns:
    sb.boxplot(data_clust['M3'],data_clust[i])
    plt.show()

# 2. Apply K means clustering and identify the ideal value of K using elbow and silhoutee method
from sklearn.cluster import KMeans
wcss=[]
cl=[1,2,3,4,5,6,7,8]
for i in cl:
    mod=KMeans(n_clusters=i,random_state=10)
    mod.fit(data_sc)
    print(mod.inertia_)
    wcss.append(mod.inertia_)
plt.plot(cl,wcss) 
sil=[]
from sklearn.metrics import silhouette_score
cl=[2,3,4,5,6,7,8]
for i in cl:
    mod=KMeans(n_clusters=i)
    mod.fit(data_sc)
    sil.append(silhouette_score(data_sc,mod.labels_))
res=pd.DataFrame({'k':cl,'silhoutee':sil})
res
Apply PCA on the data. How many PCs are required to reproduce the 95% charecteristics of original data. What is the top 5 features contributing in PC1 ?
from sklearn.decomposition import PCA 
pca = PCA(n_components = data_sc.shape[1])
pca_data = pca.fit_transform(data_sc)
exp_var_ratio= pca.explained_variance_ratio_
exp_var_ratio.round(3)

cum_var=exp_var_ratio[0]
itr=2 # defined as two as first pc1 variance defined outside the loop
for j in exp_var_ratio[1:]:
    cum_var=cum_var+j
    if cum_var >= 0.95:
        break
    itr=itr+1

print('The number of principle components capturing 95 percent varaition is data is : ',itr,' Varaince explained is ', cum_var)
# Variance Ratio bar plot for each PCA components.
ax = plt.bar(range(1,len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)
plt.xlabel("PCA Components",fontweight = 'bold')
plt.ylabel("Variance Ratio",fontweight = 'bold')
# PC1 is derived from first eigen vector
e1=pd.DataFrame(pca.components_[0,:]) # first eigen vector
e1.index=data_sc.columns
e11=np.abs(e1)
e11.sort_values(0,ascending=False).head(5) # Top 5 features contributing in PC1
### 5. Build the following ML model and compare its performace: (5 Marks)
    #a. ML model with original inp_data_dime and out
    #b. ML model with inp_data_dime_pca and out
    #(Note: For pca and svd use the number of components which captures the 95 percent of variance) 
    
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

xtrain,xtest,ytrain,ytest=train_test_split(data_sc,out,test_size=0.2,random_state=48)
rf=RandomForestClassifier(random_state=48)
rf.fit(xtrain,ytrain)
ypred=rf.predict(xtest)
print('The number of input feature ',xtrain.shape[1])
print(classification_report(ytest,ypred))

xtrain,xtest,ytrain,ytest=train_test_split(pca_data[:,:5],out,test_size=0.2,random_state=48)
rf=RandomForestClassifier(random_state=48)
rf.fit(xtrain,ytrain)
ypred=rf.predict(xtest)
print('The number of PCA components ',xtrain.shape[1])
print(classification_report(ytest,ypred))
# Recomendation 
data_recom.head(2)
from surprise import KNNWithMeans,SVDpp
from surprise import Dataset
from surprise import accuracy
from surprise import Reader
from surprise.model_selection import train_test_split,cross_validate
#data_recom=pd.read_csv('recommendation_mini.csv')
ratings = data_recom
reader = Reader(rating_scale=(1, 5))

trainsetfull = rating_data.build_full_trainset()
print('Number of users: ', trainsetfull.n_users, '\n')
print('Number of items: ', trainsetfull.n_items, '\n')

alg=SVDpp()
alg.fit(trainsetfull) 
alg.predict(uid = 'A2CX7LUOHB2NDG', iid ='059400232X')

results = cross_validate(
    algo = alg, data = rating_data, measures=['RMSE'], 
    cv=3)
results['test_rmse'].mean()
---
3 (a) Build the popularity based recommendation system and suggest top 5 items. (5 Marks)
data_recom=data.iloc[:,24:28]
data_recom.shape
# Top 5 Items based on Average Rating
pd.DataFrame(data.groupby('ItemID')['Rating'].mean().sort_values(ascending=False))
popularity_table = data.groupby('ItemID').agg({'Rating' : 'mean'})
popularity_table.head(5)
popularity_table.sort_values('Rating', ascending=False).head(5)
#Build collaborative recommendation engine to recommend a top 5 items to the specific user. Measure the model quality in terms of RMSE
reader = Reader(rating_scale =(1,5))
rating_data = Dataset.load_from_df(data_recom[['UserID', 'ItemID', 'Rating']], reader)
rating_data
[train_set, test_set] = train_test_split(rating_data, test_size=.15, shuffle=True)
trainsetfull = rating_data.build_full_trainset()
print('Number of Users  : ',trainsetfull.n_users, '\n')
print('Number of Items  : ',trainsetfull.n_items, '\n')
algo = KNNWithMeans(k= 15, min_k = 5 , sim_options = {'name':'pearson', 'user_based' :False}, verbose =True)
results = cross_validate( algo = algo, data =rating_data , measures =['RMSE'], 
                        cv =5, return_train_measures=True)
print(results['test_rmse'].mean())

alg = SVDpp()
alg.fit(trainsetfull)
alg.predict(uid =76, iid =2)
item_id = data_recom['ItemID'].unique()
item_id76 = data_recom.loc[data_recom['UserID']==76, 'ItemID']
item_id_pred = np.setdiff1d(item_id, item_id76)
testset= [[76, iid, 4] for iid in item_id_pred]
pred = alg.test(testset)
pred
--
pred_ratings = np.array([pred1.est for pred1 in pred])
i_max= pred_ratings.argmax()
iid = item_id_pred[i_max]
print('Top item for user has iid {0} with predicted rating{1}'.
      format(iid, pred_ratings[i_max]))
r_df = data_recom.pivot(index ='UserID', columns ='ItemID', values='Rating').fillna(0)



model = SVD(random_state =42)
model.fit(trainsetfull)
pred = model.test(testset)
print("Collabrative Filter Model RMSE ", accuracy.rmse(pred))
# for a user
num_recomendations =5
userId =76
user_row_number = userId -1 
all_items= data_recom['ItemID'].unique()
pred_ratings = [(item, model.predict(userId, item).est) for item in all_items]
top_5_reco = sorted(pred_ratings, key = lambda x :x[1], reverse =True)[:5]
print(f' Top 5 Recomended items for User id {userId}')
for item , rating in top_5_reco :
    print(f'item : {item}, Predicted Rating {rating}')



#Imabalanced data
# -------------------- (B) Data Cleaning --------------------

#First run a check for the presence of missing values and their percentage for each column. Then choose the right approach to treat them.
Total = df_admissions.isnull().sum().sort_values(ascending=False)          
Percent = (df_admissions.isnull().sum()*100/df_admissions.isnull().count()).sort_values(ascending=False)   
missing_data = pd.concat([Total, Percent], axis = 1, keys = ['Total', 'Percentage of Missing Values'])    
missing_data
df_num = df_feature.select_dtypes(include = [np.number])
df_cat = df_feature.select_dtypes(include = [np.object])

 

"""
imputer_num = SimpleImputer(strategy="median")
df[numerical_cols] = imputer_num.fit_transform(df[numerical_cols])

imputer_cat = SimpleImputer(strategy="most_frequent")
df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])
"""

for col in numerical_cols:
    if df[col].isnull().sum() > 0:  
        median_value = df[col].median()  # Compute median dynamically
        df[col].fillna(median_value, inplace=True)  # Replace missing values with median

for col in categorical_cols:
    if df[col].isnull().sum() > 0:
        mode_value = df[col].mode()[0]  # Compute mode dynamically
        df[col].fillna(mode_value, inplace=True)  # Replace missing values with mode
#Examine Outliers
for i in df_f.columns:
    sb.boxplot(df_f[i])
    plt.show()

for i in df_f.columns:
    sb.kdeplot(stats.zscore(df_f[i]))
    plt.axvline(3, ymin=0, ymax=0.01, c='r')
    plt.axvline(-3, ymin=0, ymax=0.01, c='r')
    plt.show()

# Handling Outliers using Z-score
z_scores = np.abs(zscore(df[numerical_cols]))
df = df[(z_scores < 3).all(axis=1)]  

 # removal of outliers
Q1=data[num_cols].quantile(0.25)
Q3=data[num_cols].quantile(0.75)
IQR=Q3-Q1
lower_bound=Q1-1.5*IQR
upper_bound=Q3+1.5*IQR
data_fix=data[~((data[num_cols]<lower_bound) | (data[num_cols]>upper_bound)).any(axis=1)]
print(data.shape)
print(data_fix.shape)

# for the independent numeric variables, we plot the histogram to check the distribution of the variables
# Note: the hist() function considers the numeric variables only, by default
# we drop the target variable using drop()
# 'axis=1' drops the specified column
df_admissions.drop('Chance of Admit', axis = 1).hist()
plt.tight_layout()
# skew() returns the coefficient of skewness for each variable
df_admissions.drop('Chance of Admit', axis = 1).skew()
#Correlation 
    sb.heatmap(df_n.corr(), annot=True, cmap='rainbow')

# Check skewness for numerical features
for col in df_num:
    skewness = skew(df[col])
    print(f"Skewness of {col}: {skewness}")

# -------------------- (C) Feature Engineering --------------------
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
Remove Insignificant Variables
df_cust = df_cust.drop('Cust_Number',axis=1)
Outlier Analysis and Treatment
Check the outliers in all the variables and treat them using appropriate techniques.
--------------------------------
Choosing the number of clusters 
The first step is to define the K number of clusters in which we will group the data. Let’s select K=3.
# consider the numeric variables
df_num = df_cust.drop(['Sex'], axis = 1)
fig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 4))
for variable, subplot in zip(df_num.columns, ax.flatten()):  
    # use boxplot() to plot the graph
    # pass the axes for the plot to the parameter, 'ax'
    sns.boxplot(df_cust[variable], ax = subplot)
# display the plot
plt.show()
# missing value & presence of missing values and their percentage for each column.
#Scale the Data
# consider the features 'Cust_Spend_Score' and 'Yearly_Income'
X_filtered = df_cust[['Cust_Spend_Score', 'Yearly_Income']]

# print top 5 observations of X
X_filtered.head()
# initialize the StandardScaler
X_norm = StandardScaler()
num_norm = X_norm.fit_transform(X_filtered)
X = pd.DataFrame(num_norm, columns = X_filtered.columns)
#K-Means Clustering
Optimal Value of K Using Elbow Plot
Elbow plot is plotted with the value of K on the x-axis and the WCSS (Within Cluster Sum of Squares) on the y-axis. The value of K corresponding to the elbow point represents the optimal value for K.
# create several cluster combinations ranging from 1 to 20 and observe the wcss (Within Cluster Sum of Squares) for each cluster
# consider an empty list to store the WCSS
wcss  = []

# use for loop to perform K-means with different values of K
# set the 'random_state' to obtain the same centroid initialization for each code run
# fit the model on scaled data
# append the value of WCSS for each K to the list 'wcss'
# the 'inertia_' retuns the WCSS for specific value of K
for i in range(1,21):
    kmeans = KMeans(n_clusters = i, random_state = 10)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
Let us plot the elbow plot and identify the elbow point.
# visualize the elbow plot to get the optimal value of K
plt.plot(range(1,21), wcss)

# set the axes and plot labels
# set the font size using 'fontsize'
plt.title('Elbow Plot', fontsize = 15)
plt.xlabel('No. of clusters (K)', fontsize = 15)
plt.ylabel('WCSS', fontsize = 15)

# display the plot
plt.show()
# visualize the elbow plot to get the optimal value of K
plt.plot(range(1,21), wcss)

# set the axes and plot labels
# set the font size using 'fontsize'
plt.title('Elbow Plot', fontsize = 15)
plt.xlabel('No. of clusters (K)', fontsize = 15)
plt.ylabel('WCSS', fontsize = 15)

# plot a vertical line at the elbow
plt.axvline(x = 5, color = 'red')

# display the plot
plt.show()
Interpretation: We can see that the for K = 5, there is an elbow in the plot. Before this elbow point, the WCSS is decreasing rapidly and after K = 5, the WCSS is decreasing slowly.
Now, let us use the silhouette score method to identify the optimal value of K.
#Optimal Value of K Using Silhouette Score
The Silhouette score can also be used to identify the optimal number of clusters. We plot the Silhouette score for different values of K. The K with the highest Silhouette score represents the optimal value for the number of clusters (K).

# create a list for different values of K
n_clusters = [2, 3, 4, 5, 6]
# use 'for' loop to build the clusters
# 'random_state' returns the same sample each time you run the code  
# fit and predict on the scaled data
# 'silhouette_score' function computes the silhouette score for each K
for K in n_clusters:
    cluster = KMeans (n_clusters= K, random_state= 10)
    predict = cluster.fit_predict(X)
    score = silhouette_score(X, predict, random_state= 10)
    print ("For {} clusters the silhouette score is {})".format(K, score))

##Visualize the silhouette scores
# consider the number of clusters
n_clusters = [2, 3, 4, 5, 6]

# consider an array of the data
X = np.array(X)

# for each value of K, plot the silhouette plot the clusters formed
for K in n_clusters:
    
    # create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    
    # set the figure size
    fig.set_size_inches(18, 7)

    # the 1st subplot is the silhouette plot
    # initialize the cluster with 'K' value and a random generator
    model = KMeans(n_clusters = K, random_state = 10)
    
    # fit and predict on the scaled data
    cluster_labels = model.fit_predict(X)

    # the 'silhouette_score()' gives the average value for all the samples
    silhouette_avg = silhouette_score(X, cluster_labels)
    
    # Compute the silhouette coefficient for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(K):
        
        # aggregate the silhouette scores for samples belonging to cluster i, and sort them
        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
        
        # sort the silhouette coefficient
        ith_cluster_silhouette_values.sort()
        
        # calculate the size of the cluster
        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        # color each cluster 
        color = cm.nipy_spectral(float(i) / K)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # compute the new y_lower for next plot
        y_lower = y_upper + 10 

    # set the axes and plot label
    ax1.set_title("Silhouette Plot")
    ax1.set_xlabel("Silhouette coefficient")
    ax1.set_ylabel("Cluster label")

    # plot the vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    # clear the y-axis ticks
    ax1.set_yticks([])  
    
    # set the ticks for x-axis 
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8])

    
    # 2nd plot showing the actual clusters formed
    # consider different color for each cluster
    colors = cm.nipy_spectral(cluster_labels.astype(float) / K)
    
    # plot a scatter plot to visualize the clusters
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k')

    # label the cluster centers
    centers = model.cluster_centers_
    
    # display the cluster center with cluster number
    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')
    
    # add the axes and plot title
    ax2.set_title("Clusters")
    ax2.set_xlabel("Spending Score")
    ax2.set_ylabel("Annual Income")
    
    # set the common title for subplots
    plt.suptitle(("Silhouette Analysis for K-Means Clustering with n_clusters = %d" % K), fontsize=14, 
                 fontweight='bold')

# display the plot
plt.show()

Interpretation: The above plot shows the silhouette plot and the clusters formed for each value of K. The plot shows that there are outliers (where the silhouette coefficient is less than 0) for K = 2,3,4. Also for K = 6, the 6th cluster has the silhouette score less than the average silhouette score. Thus we can not consider the K values as 2,3,4 and 6.

Also from the above output, we can see that the silhouette score is maximum for k = 5 and from the plot, we can see that there are no outliers for 5 clusters and all the clusters have silhouette coefficients greater than the average silhouette score. Thus we choose K = 5 as the optimal value of k.
3.3 Build the Clusters
Let us build the 5 clusters using K-menas clustering.

# build a K-Means model with 5 clusters
new_clusters = KMeans(n_clusters = 5, random_state = 10)

# fit the model
new_clusters.fit(X)

# append the cluster label for each point in the dataframe 'df_cust'
df_cust['Cluster'] = new_clusters.labels_
# head() to display top five rows
df_cust.head()
Check the size of each cluster
df_cust.Cluster.value_counts()
Plot a barplot to visualize the cluster sizes

# use 'seaborn' library to plot a barplot for cluster size
sns.countplot(data= df_cust, x = 'Cluster')

# set the axes and plot labels
# set the font size using 'fontsize'
plt.title('Size of Cluster', fontsize = 15)
plt.xlabel('Clusters', fontsize = 15)
plt.ylabel('Number of Customers', fontsize = 15)

# add values in the graph
# 'x' and 'y' assigns the position to the text
# 's' represents the text on the plot
plt.text(x = -0.05, y =39, s = np.unique(new_clusters.labels_, return_counts=True)[1][0])
plt.text(x = 0.95, y =24, s = np.unique(new_clusters.labels_, return_counts=True)[1][1])
plt.text(x = 1.95, y =37, s = np.unique(new_clusters.labels_, return_counts=True)[1][2])
plt.text(x = 2.95, y =22, s = np.unique(new_clusters.labels_, return_counts=True)[1][3])
plt.text(x = 3.95, y =81, s = np.unique(new_clusters.labels_, return_counts=True)[1][4])

# display the plot
plt.show()
The 5th cluster is the largest cluster containing 80 observations
3.4 Analyze the Clusters
Let us visualize the clusters by considering the variables 'Cust_Spend_Score' and 'Yearly_Income'.
# plot the lmplot to visualize the clusters
# pass the different markers to display the points in each cluster with different shapes
# the 'hue' parameter returns colors for each cluster
sns.lmplot(x = 'Cust_Spend_Score', y = 'Yearly_Income', data = df_cust, hue = 'Cluster', 
                markers = ['*', ',', '^', '.', '+'], fit_reg = False, size = 10)

# set the axes and plot labels
# set the font size using 'fontsize'
plt.title('K-means Clustering (for K=5)', fontsize = 15)
plt.xlabel('Spending Score', fontsize = 15)
plt.ylabel('Annual Income', fontsize = 15)

# display the plot
plt.show()
Cluster 1
Check the size of the cluster
# size of a cluster 1
len(df_cust[df_cust['Cluster'] == 0])
Compute the statistical summary for the customers in this cluster
# statistical summary of the numerical variables
df_cust[df_cust.Cluster==0].describe()

# summary of the categorical variable
df_cust[df_cust.Cluster==0].describe(include = object)


Initializing centroids
Centroid is the center of a cluster but initially, the exact center of data points will be unknown so, we select random data points and define them as centroids for each cluster.
Assign data points to the nearest cluster
Now that centroids are initialized, the next step is to assign data points Xn to their closest cluster centroid C
In this step, we will first calculate the distance between data point X and centroid C using Euclidean Distance metric.
And then choose the cluster for data points where the distance between the data point and the centroid is minimum. 
Re-initialize centroids 
Next, we will re-initialize the centroids by calculating the average of all data points of that cluster.
Repeat steps 3 and 4
We will keep repeating steps 3 and 4 until we have optimal centroids and the assignments of data points to correct clusters are not changing anymore.
Merits:○Easy to understand ○Simple implementation●Demerits:○Finding the optimal value of K can be computationally expensive○Initial centroid assignment affects the final output○Not efficient in presence of outliers
Dendrogram●It is a very useful technique to visualize the clusters
●It is a tree-based hierarchical structure that can be used to decide the required number of clusters
Different linkage methods result in the formation of different dendrograms
●Observations linked at a low height represents more similar observations
●Dissimilar observations fuse at a higher level in the dendrogram
Dendrogram●X-axis of the dendrogram represents the data point, each considered as a single cluster and the distance is given on the Y-axis
Each single cluster is known as ‘leaf’, The horizontal line is known as ‘clade’ which represents the merging of cluster

●DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is mostly used density-based clustering algorithm
●This technique can form clusters of non-linear shapes
●It considers a cluster as a continuous region of high density
●Regions of low density are identified as noise/ outliers
Dimensio Reduction 
The real-life dataset may contain a large number of features under study
●For example, while estimating the price of a mobile phone we need to consider various features like screen size, internal storage, camera quality, battery backup and so on
●The dataset with a large number of features needs more time for training the model. Also, it can cause the overfitting 
To avoid such issues, one can reduce the dimension of the dataset●The dimension reduction techniques remove the redundant variables/ noise in the original data, which reduces the training time●Reducing the dataset to 2 or 3 dimensions helps in visualization of the data●Various dimension reduction techniques:○Principal Component Analysis (PCA)○Linear Discriminant Analysis (LDA) ○Factor Analysis
●Two different approaches can be used for dimension reduction: Projection, Manifold learning●In the projection approach, the original dataset is projected onto the lower-dimensional plane●PCA uses the projection approach for dimension reduction
●This method is not effective if the dataset has different layers in the higher dimensions●In manifold learning, a manifold is created on which the dataset lies
PCA
It is one of the dimensionality reduction techniques that is used to reduce the dimensions of the large datasets●It transforms the large set of features into a small set such that it will contain the maximum information in the original data●The number of components is less than or equal to the number of independent variables●PCA projects the original dataset on the lower dimensional plane●It transoms the original data to a new set of uncorrelated variables

The first principal component (PC1) exhibits the direction of maximum variance in the data●It is used to remove the redundancy in the data●PCA reduces the multicollinearity (if present) in the original data●Principal components are always orthogonal to each other 
PCA steps 
1. Standardize the data
2. Compute the covariance matrix
3. Calculate the eigenvalues and eigenvectors
4. Sort the eigenvalues in the descending order 
5. Select the eigenvectors that explains the maximum variance in the data
Apllication PCA is mainly used in image compression, facial recognition models●It is also used in the exploratory analysis to reduce the dimension of data before applying machine learning methods●Used in the field of psychology, finance to identify the patterns high dimensional data



# instantiate linkage object with scaled data and consider 'ward' linkage method 
link_mat = linkage(features_scaled, method = 'ward')     

# print first 10 observations of the linkage matrix 'link_mat'
print(link_mat[0:10])

5. DBSCAN
DBSCAN is a density-based clustering method. It can create non-linear clusters. This method considers a high-density region as a cluster and the low-density points are considered as outliers. We do not need to provide the required number of clusters to the algorithm.

Let us cluster the scaled data.

# instantiate DBSCAN with epsilon and minimum points 
# pass the epsilon radius for neighbourhood
# pass the number of minimum points
model = DBSCAN(eps = 0.8, min_samples = 15)

# fit the model on the scaled data
model.fit(features_scaled)
# display the unique clusters formed by DBSCAN
(set(model.labels_)
interpretation: From the above output we can see that the DBSCAN algorithm has created 3 clusters. The data points labeled as -1 are the outliers identified by DBSCAN.
# add a column containing cluster number to the original data
df_prod['Cluster_DBSCAN'] = model.labels_

# print head() of the newly formed dataframe
df_prod.head()

# check the size of each cluster
df_prod['Cluster_DBSCAN'].value_counts()

# plot the countplot for the cluster size
sns.countplot(data = df_prod, x = 'Cluster_DBSCAN')

# set the axes and plot labels
# set the font size using 'fontsize'
plt.title('Size of Cluster', fontsize = 15)
plt.xlabel('Cluster', fontsize = 15)
plt.ylabel('No. of Products', fontsize = 15)

# display the plot
plt.show()
nterpretation: From the above output we can see that a cluster with 4875 is the largest cluster and other clusters are very small.

Now let us visualize the clusters. As we have more than 2 features, we consider only the variables Sales and Profit to visualize the clusters.
 plot the lmplot to visualize the clusters
# pass the 'Cluster_DBSCAN' to the hue parameter to display each cluster in a different color
# pass the different marker styles to visualize each cluster with a different marker
sns.lmplot(x = 'Sales', y = 'Profit', data = df_prod, hue = 'Cluster_DBSCAN', markers = ['o','+','^',','], 
           fit_reg = False, size = 12)

# set the axes and plot labels
# set the font size using 'fontsize'
plt.title('DBSCABN (eps = 0.8, min_samples = 15) ', fontsize = 15)
plt.xlabel('Sales', fontsize = 15)
plt.ylabel('Profit', fontsize = 15)

# display the plot
plt.show()

nterpretation: The above plot shows the clusters created by DBSCAN. The blue circles correspond to the outliers and the observations in the largest cluster are denoted by the orange '+'. Other clusters are too small compared to the largest cluster.

We can see some of the points are overlapped. This is because the dimension of the original data is greater than 2 and we have considered only 2 variables to plot the clusters.

Now let us check the products belonging to each cluster.

Cluster 1
Let us identify the products in the cluster 1.

# check the count of different products belonging to cluster_1
df_prod[df_prod.Cluster_DBSCAN==0].index.value_counts()

Outliers identified by DBSCAN
# check the count of different products identified as outliers
Interpretation: We can see that the algorithm has identified most of the technical products as the outliers.

Here we can see that the DBSCAN algorithm has not grouped the product like hierarchical clustering. Thus we can conclude that the DBSCAN algorithm is working poorly on this dataset.

df_prod[df_prod.Cluster_DBSCAN==-1].index.value_counts()
---
from sklearn import tree
model=tree.DecisionTreeClassifier()
model.fit(X_train,y_train)
DecisionTreeClassifier()
y_pred_DT = model.predict(X_test)
cm_DT= confusion_matrix(y_test, y_pred_DT)
cm_DT
array([[11,  0,  0],
       [ 0,  7,  6],
       [ 0,  4,  2]], dtype=int64)
sns.heatmap(cm_DT, annot=True)
plt.show()
classification=classification_report(y_test,y_pred_DT)
print(classification)
ac = accuracy_score(y_test, y_pred_DT)
print("Accuracy Score:", ac)
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.Set1,alpha = 1, edgecolor='k')
ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')
ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_pred_DT, cmap=plt.cm.Set1,alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Predicted label')
plt.show()
--
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0][y_test == 0], X_test[:, 1][y_test == 0], c='red',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 1], X_test[:, 1][y_test == 1], c='blue',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 2], X_test[:, 1][y_test == 2], c='yellow',alpha = 1, edgecolor='k')

ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')
ax[1].scatter(X_test[:, 0][y_pred_DT == 0], X_test[:, 1][y_pred_DT == 0], c='red',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred_DT == 1], X_test[:, 1][y_pred_DT == 1], c='blue',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred_DT == 2], X_test[:, 1][y_pred_DT == 2], c='yellow',alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Predicted label')
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
from sklearn.decomposition import PCA
pca = PCA()
X_train_2 = pca.fit_transform(X_train)
X_test_2 = pca.transform(X_test)
explained_variance = pca.explained_variance_ratio_  
explained_variance
--
from sklearn import tree
model2=tree.DecisionTreeClassifier()
model2.fit(X_train_2,y_train)
DecisionTreeClassifier()
y_pred_DT_2 = model2.predict(X_test_2)
from sklearn.metrics import confusion_matrix
cm_PCA= confusion_matrix(y_test, y_pred_DT_2)
sns.heatmap(cm_PCA, annot=True)
plt.show()
ac_PCA = accuracy_score(y_test, y_pred_DT_2)
print("Accuracy Score:", ac_PCA)
classification_2=classification_report(y_test,y_pred_DT_2)
print(classification_2)
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.Set1,alpha = 1, edgecolor='k')
ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')
ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_pred_DT_2, cmap=plt.cm.Set1,alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Predicted label')
plt.show()

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0][y_test == 0], X_test[:, 1][y_test == 0], c='red',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 1], X_test[:, 1][y_test == 1], c='blue',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 2], X_test[:, 1][y_test == 2], c='yellow',alpha = 1, edgecolor='k')

ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')
ax[1].scatter(X_test[:, 0][y_pred_DT_2 == 0], X_test[:, 1][y_pred_DT_2 == 0], c='red',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred_DT_2 == 1], X_test[:, 1][y_pred_DT_2 == 1], c='blue',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred_DT_2 == 2], X_test[:, 1][y_pred_DT_2 == 2], c='yellow',alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Predicted label')
plt.show()
--
 DT After Applying LDA
# create the lda model
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
model = LinearDiscriminantAnalysis()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
model.fit(X_train,y_train)
LinearDiscriminantAnalysis()
y_pred = model.predict(X_test)
from sklearn.metrics import confusion_matrix
cm_LDA= confusion_matrix(y_test, y_pred)
sns.heatmap(cm_LDA, annot=True)
plt.show()

ac_LDA = accuracy_score(y_test, y_pred)
print("Accuracy Score:", ac_LDA)
Accuracy Score: 0.7666666666666667
classification_2=classification_report(y_test,y_pred)
print(classification_2)

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
# define model
model = LinearDiscriminantAnalysis()
# define model evaluation method
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
# define grid
grid = dict()
grid['solver'] = ['svd', 'lsqr', 'eigen']
#grid['shrinkage'] = np.arange(0, 1, 0.01)
# define search
search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)
# perform the search
results = search.fit(X_train, y_train)
# summarize
print('Mean Accuracy: %.3f' % results.best_score_)
print('Config: %s' % results.best_params_)
 
model = LinearDiscriminantAnalysis(solver = 'svd')
model1 = model.fit(X_train,y_train)
y_pred = model.predict(X_test)
from sklearn.metrics import confusion_matrix
cm_LDA= confusion_matrix(y_test, y_pred)
sns.heatmap(cm_LDA, annot=True)
plt.show()

ac_LDA = accuracy_score(y_test, y_pred)
print("Accuracy Score:", ac_LDA)
Accuracy Score: 0.7666666666666667
classification_2=classification_report(y_test,y_pred)
print(classification_2)
---
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.Set1,alpha = 1, edgecolor='k')
ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')
ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap=plt.cm.Set1,alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Predicted label')
plt.show()

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0][y_test == 0], X_test[:, 1][y_test == 0], c='red',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 1], X_test[:, 1][y_test == 1], c='blue',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 2], X_test[:, 1][y_test == 2], c='yellow',alpha = 1, edgecolor='k')

ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')
ax[1].scatter(X_test[:, 0][y_pred == 0], X_test[:, 1][y_pred == 0], c='red',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred == 1], X_test[:, 1][y_pred == 1], c='blue',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred == 2], X_test[:, 1][y_pred == 2], c='yellow',alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Predicted label')
plt.show()

fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(15, 5))

# Plot the training points (scatter plot, all rows first and second column only)
ax[0].scatter(X_test[:, 0][y_test == 0], X_test[:, 1][y_test == 0], c='red',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 1], X_test[:, 1][y_test == 1], c='blue',alpha = 1, edgecolor='k')
ax[0].scatter(X_test[:, 0][y_test == 2], X_test[:, 1][y_test == 2], c='yellow',alpha = 1, edgecolor='k')
ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')
ax[0].set_title('Actual label')

ax[1].scatter(X_test[:, 0][y_pred_DT == 0], X_test[:, 1][y_pred_DT == 0], c='red',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred_DT == 1], X_test[:, 1][y_pred_DT == 1], c='blue',alpha = 1,  edgecolor='k')
ax[1].scatter(X_test[:, 0][y_pred_DT == 2], X_test[:, 1][y_pred_DT == 2], c='yellow',alpha = 1,  edgecolor='k')
ax[1].set_xlabel('PC1')
ax[1].set_ylabel('PC2')
ax[1].set_title('Classification Before PCA')


ax[2].scatter(X_test[:, 0][y_pred_DT_2 == 0], X_test[:, 1][y_pred_DT_2 == 0], c='red',alpha = 1,  edgecolor='k')
ax[2].scatter(X_test[:, 0][y_pred_DT_2 == 1], X_test[:, 1][y_pred_DT_2 == 1], c='blue',alpha = 1,  edgecolor='k')
ax[2].scatter(X_test[:, 0][y_pred_DT_2 == 2], X_test[:, 1][y_pred_DT_2 == 2], c='yellow',alpha = 1,  edgecolor='k')
ax[2].set_xlabel('PC1')
ax[2].set_ylabel('PC2')
ax[2].set_title('Classification After PCA')


ax[3].scatter(X_test[:, 0][y_pred == 0], X_test[:, 1][y_pred == 0], c='red',alpha = 1,  edgecolor='k')
ax[3].scatter(X_test[:, 0][y_pred == 1], X_test[:, 1][y_pred == 1], c='blue',alpha = 1,  edgecolor='k')
ax[3].scatter(X_test[:, 0][y_pred == 2], X_test[:, 1][y_pred == 2], c='yellow',alpha = 1,  edgecolor='k')
ax[3].set_xlabel('PC1')
ax[3].set_ylabel('PC2')
ax[3].set_title('Classification After LDA')
plt.show()

