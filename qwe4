
# Step 1: Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, cohen_kappa_score, confusion_matrix, roc_curve, auc
from scipy.stats import zscore
from imblearn.over_sampling import SMOTE
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Step 2: Load Dataset
df = pd.read_csv("loan.csv")

# -------------------- (A) Data Exploration --------------------
print("\nDataset Shape:", df.shape)
print("\nData Info:\n", df.info())
print("\nMissing Values:\n", df.isnull().sum())

# Identify Numerical & Categorical Columns
numerical_cols = df.select_dtypes(include=['number']).columns.tolist()
categorical_cols = df.select_dtypes(exclude=['number']).columns.tolist()

print("\nNumerical Columns:", numerical_cols)
print("\nCategorical Columns:", categorical_cols)

# -------------------- (B) Data Cleaning --------------------
# Handling Missing Values

"""
imputer_num = SimpleImputer(strategy="median")
df[numerical_cols] = imputer_num.fit_transform(df[numerical_cols])

imputer_cat = SimpleImputer(strategy="most_frequent")
df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])
"""

for col in numerical_cols:
    if df[col].isnull().sum() > 0:  
        median_value = df[col].median()  # Compute median dynamically
        df[col].fillna(median_value, inplace=True)  # Replace missing values with median

for col in categorical_cols:
    if df[col].isnull().sum() > 0:
        mode_value = df[col].mode()[0]  # Compute mode dynamically
        df[col].fillna(mode_value, inplace=True)  # Replace missing values with mode
        
# Handling Outliers using Z-score
z_scores = np.abs(zscore(df[numerical_cols]))
df = df[(z_scores < 3).all(axis=1)]  

# -------------------- (C) Feature Engineering --------------------
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Scaling
scaler = StandardScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

# -------------------- (D) Variance Inflation Factor (VIF) --------------------
X = df.drop(columns=["CustomerId", "Loan"])  
y = df["Loan"]

vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Remove features with high VIF
#X = X.loc[:, vif_data["VIF"] < 10]
X = X.loc[:, vif_data[vif_data["VIF"] < 10]["Feature"]]


# -------------------- (E) Train-Test Split --------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

y_train = y_train.astype(int)
y_test = y_test.astype(int)

# -------------------- (F) Handling Class Imbalance (SMOTE) --------------------
if len(y_train.unique()) > 1:  
    sm = SMOTE(random_state=42)
    X_train, y_train = sm.fit_resample(X_train, y_train)

# -------------------- (G) Model Training & Evaluation --------------------
# Random Forest
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# K-Nearest Neighbors (KNN)
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
y_pred_knn = knn_model.predict(X_test)

# AdaBoost Classifier
ada_model = AdaBoostClassifier(n_estimators=100, random_state=42)
ada_model.fit(X_train, y_train)
y_pred_ada = ada_model.predict(X_test)

# Model Evaluation
accuracy_rf = accuracy_score(y_test, y_pred_rf)
accuracy_knn = accuracy_score(y_test, y_pred_knn)
accuracy_ada = accuracy_score(y_test, y_pred_ada)

print("\nRandom Forest Accuracy:", accuracy_rf)
print("KNN Accuracy:", accuracy_knn)
print("AdaBoost Accuracy:", accuracy_ada)

# -------------------- (H) Model Comparison Visualizations --------------------
# Accuracy Comparison
plt.figure(figsize=(6, 4))
sns.barplot(x=["Random Forest", "KNN", "AdaBoost"], y=[accuracy_rf, accuracy_knn, accuracy_ada], palette="viridis")
plt.title("Accuracy Comparison of Models")
plt.ylabel("Accuracy Score")
plt.show()

# Confusion Matrices
plt.figure(figsize=(18, 5))

plt.subplot(1, 3, 1)
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt="d", cmap="Blues")
plt.title("Random Forest Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")

plt.subplot(1, 3, 2)
sns.heatmap(confusion_matrix(y_test, y_pred_knn), annot=True, fmt="d", cmap="Oranges")
plt.title("KNN Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")

plt.subplot(1, 3, 3)
sns.heatmap(confusion_matrix(y_test, y_pred_ada), annot=True, fmt="d", cmap="Greens")
plt.title("AdaBoost Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")

plt.show()

# -------------------- (I) ROC Curve for All Models --------------------
if len(np.unique(y_test)) == 2:  # ROC only for binary classification
    y_probs_rf = rf_model.predict_proba(X_test)[:, 1]
    y_probs_knn = knn_model.predict_proba(X_test)[:, 1]
    y_probs_ada = ada_model.predict_proba(X_test)[:, 1]

    fpr_rf, tpr_rf, _ = roc_curve(y_test, y_probs_rf)
    fpr_knn, tpr_knn, _ = roc_curve(y_test, y_probs_knn)
    fpr_ada, tpr_ada, _ = roc_curve(y_test, y_probs_ada)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr_rf, tpr_rf, label=f"Random Forest (AUC = {auc(fpr_rf, tpr_rf):.2f})", color="blue")
    plt.plot(fpr_knn, tpr_knn, label=f"KNN (AUC = {auc(fpr_knn, tpr_knn):.2f})", color="orange")
    plt.plot(fpr_ada, tpr_ada, label=f"AdaBoost (AUC = {auc(fpr_ada, tpr_ada):.2f})", color="green")

    plt.plot([0, 1], [0, 1], "k--")  
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve Comparison")
    plt.legend()
    plt.show()

# -------------------- (J) Feature Importance from Random Forest --------------------
feature_importance = pd.DataFrame({
    "Feature": X_train.columns,
    "Importance": rf_model.feature_importances_
}).sort_values(by="Importance", ascending=False)

print("\nTop Features Influencing Loan Payback:\n", feature_importance.head(10))

# Plot Feature Importance
plt.figure(figsize=(10, 5))
sns.barplot(x="Importance", y="Feature", data=feature_importance.head(10))
plt.title("Top Features Affecting Loan Payback")
plt.show()
# -------------------- END OF CODE --------------------
