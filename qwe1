Convert the following matrix to an orthogonal matrix using Gram Schmidt Process?
A = np.array( ((1,1,1), (-1, 0, 1), (1, 1, 2)))
A
u1, u2, u3 = A[:,0], A[:,1], A[:,2]
w1 =u1
# find magnitude of u1
mag_w1 = np.linalg.norm(w1) 
# do normalization 
v1= w1/mag_w1
print(v1.dot(v1))
# remove vector projection of u2 in v1 from u2
w2 =u2- (u2.dot(v1)) *v1 
mag_w2 = np.linalg.norm(w2) 
v2 = w2/mag_w2
v2.dot(v2)
# remove vector projection of u3 in v1 and v2 from u3
w3 = u3- ((u3.dot(v1)) *v1) -  ((u3.dot(v2)) *v2)
mag_w3 = np.linalg.norm(w3) 
v3 = w3/mag_w3
v3.dot(v3)

y = np.column_stack((v1,v2, v3))
print('result ')
print(y)

- Find Covariance matrix ?

x1 = np.array( [ [1.23, 2.12, 3.34, 4.5], [2.56, 2.89, 3.76, 3.95]])

print(x1)
np.cov(x1)

Define a loss function
Calculate derivative at each point
Run Gradient descent for Learning rate = 0.000001
Calculate the optimal value of w
For 60 Epoch plot cost

import matplotlib.pyplot as plt
height = np.array([167,145,170,180,189,155,163,178,173,176])
weight= np.array([83.5,72.5,85,90,94.5,77.5,81.5,89,86.5,88])

N =len(x)

learn_rate = 0.000001
error_rate =0.001
epochs = 60 
w=0
costs =[]
#Gradient Descent 
for epoch in range(epochs):
    predictions = w*height
    errors = weight - predictions
    cost = (errors **2 ).mean()
    costs.append(cost)
    gradient = -2 *(height *errors).mean()
    w-= learn_rate * gradient
    print(f"Epoch {epoch+1}/{epochs}, Cost :{cost}, w:{w}")
    

plt.title('Cost over Epochs ')
plt.xlabel('Epoch ')
plt.ylabel('Cost')
plt.plot( range(epochs), costs)
plt.show()

print(f"Optimal Value of w : {w}")


-Find singular value decomposition of 
from scipy.linalg import svd
A = np.array( ((1,2), (3 , 4), (5,6)))
A


Generate a dummy dataset using
np.random.seed(74)
X = np.random.randint(10,50,100).reshape(20,5)
Preprocess the data Preprocess the data by subtracting the mean and dividing by the standard deviation of each attribute value. The resulting data should be zero-mean with variance 1.
Compute the covriance matrix
obtain the eigenvalues and eigenvectors.
Find principle components.
X = np.random.randint(10,50,100).reshape(20,5)
mean = np.mean(X, axis =0)
std = np.std(X, axis =0)
Xcen = X-mean 
Xnorm = Xcen /std
XnormCov = np.cov(Xnorm, rowvar =False)
evals, evecs =np.linalg.eig(XnormCov)
s_index = np.argsort(evals)[::-1]
s_evals = evals[s_index]
s_devecs= evecs[s_index]
#np.transpose(s_devecs)
PCs =  np.transpose(np.dot(np.transpose(s_devecs), np.transpose(Xnorm)))

print(XnormCov)
print(evals)
print(evecs)
print(PCs)

Solve the following system $$\begin{pmatrix}

a = np.array([ [1,-1,1], [2,1, -3], [1,1,1] ])
a
b = np.array( [4,0,2])
b
x = np.linalg.solve(a,b)
print(x)
import sympy as sp
sp.init_printing()
x,y,z = sp.symbols('x,y,z')
sol = sp.solve([x+(-1)*y+ z-4, 

- Define function $f(w_1,w_2,w_3)= w_1^2+w_2^2+w_3^2$
- find$ \frac{\partial f}{\partial w_1},\frac{\partial f}{\partial w_2},\frac{\partial f}{\partial w_3}$

import sympy as sp
w1 = sp.symbols('w1')
w2 = sp.symbols('w2')
w3 = sp.symbols('w3')
def perform_op( w1, w2, w3):
    return w1*2+ w2*2+ w3*2
print(sp.diff(perform_op(w1, w2, w3), w1))
print(sp.diff(perform_op(w1, w2, w3), w2))
print(sp.diff(perform_op(w1, w2, w3), w3))
                2*x + y + (-3)*z -0 ,
                x+y+z -2 ], [x,y,z])
print(sol)

-Define the following function
$
f(x) =\begin{cases}
 & 0.5x , \text{ if }  x \leq0 \\
 & 1 ,\text{ if }  x > 0
\end{cases} \text { find }  f(-3) $ $ \text{and}$ $ f(2) $
sol
import sympy as sp

x = sp.symbols('x')
def do_fn(x):
    if x<=0 :
        return 0.5*x
    else :
        return 1

print(do_fn(-3))
print(do_fn(2))

----
(b) function optimization problem  or

PCA step by step on a iris dataset and capture 95 % variance(15 marks)
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

# Step 1: Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names

# Step 2: Standardize the Data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 3: Compute the Covariance Matrix
cov_matrix = np.cov(X_scaled.T)

# Step 4: Compute the Eigenvectors and Eigenvalues
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Step 5: Select Principal Components
# Sort eigenvalues and eigenvectors in descending order
sorted_indices = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[sorted_indices]
eigenvectors = eigenvectors[:, sorted_indices]

# Select the top k eigenvectors based on the explained variance ratio
total_variance = np.sum(eigenvalues)
explained_variance_ratio = eigenvalues / total_variance
cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)
k = np.argmax(cumulative_explained_variance_ratio >= 0.95) + 1

# Step 6: Project Data onto Principal Components
projection_matrix = eigenvectors[:, :k]
X_pca = X_scaled.dot(projection_matrix)

# Display the explained variance ratio and cumulative explained variance ratio
print("Explained Variance Ratio:", explained_variance_ratio[:k])
print("Cumulative Explained Variance Ratio:", cumulative_explained_variance_ratio[:k])
----
3. Consider the data given below and fit a linear regression line y=ax+b using gradient descent.

X
0
0.4
0.6
1

Y
0
1
0.48
0.95

Initialize the weights a and b to 0.8, 0.2 respectively.

Update the weights such that the error is minimum using gradient descent.

Use the function sum of squared errors $ y−y^2$ where $y^$ is the y-predicted value and y is the actual given y.

Plot the linear regression line after updating the values of a and b in two iterations.

---
import numpy as np
import matplotlib.pyplot as plt

# Define the dataset
X = np.array([0, 0.4, 0.6, 1])
Y = np.array([0, 1, 0.48, 0.95])

# Define initial weights
a = 0.8
b = 0.2

# Define learning rate and number of iterations
learning_rate = 0.1
iterations = 2

# Define function to calculate sum of squared errors
def sum_squared_errors(Y, Y_pred):
    return np.sum((Y - Y_pred) ** 2)

# Perform gradient descent
for i in range(iterations):
    # Calculate predicted values of y
    Y_pred = a * X + b

    # Calculate gradients
    gradient_a = -2 * np.sum((Y - Y_pred) * X)
    gradient_b = -2 * np.sum(Y - Y_pred)

    # Update weights
    a -= learning_rate * gradient_a
    b -= learning_rate * gradient_b

    # Print the updated weights and sum of squared errors
    print(f"Iteration {i+1}: a = {a}, b = {b}, Sum of Squared Errors = {sum_squared_errors(Y, Y_pred)}")

# Plot the original data points
plt.scatter(X, Y, color='blue', label='Original data points')

# Plot the linear regression line
plt.plot(X, a*X + b, color='red', label='Linear regression line')

# Add labels and legend
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Linear Regression using Gradient Descent')
plt.legend()

# Show plot
plt.grid(True)
plt.show()
====Imaage

from PIL import Image, ImageFilter, ImageOps
import numpy as np
import matplotlib.pyplot as plt

# Step 1: Load the image
image_path = "/content/drive/MyDrive/MF_sagarika_changes/MF ESA Jan24 model paper/Lena.jpg"
image = Image.open(image_path)

# Step 2: Display the original image
plt.figure(figsize=(10, 10))
plt.subplot(3, 3, 1)
plt.imshow(image)
plt.title("Original Image")
plt.axis("off")

# Step 3: Apply grayscale transformation
gray_image = ImageOps.grayscale(image)
plt.subplot(3, 3, 2)
plt.imshow(gray_image, cmap="gray")
plt.title("Grayscale Image")
plt.axis("off")

# Step 4: Apply Gaussian blur filter
blurred_image = gray_image.filter(ImageFilter.GaussianBlur(radius=5))
plt.subplot(3, 3, 3)
plt.imshow(blurred_image, cmap="gray")
plt.title("Blurred Image")
plt.axis("off")

# Step 5: Apply edge detection (Sobel filter)
sobel_image = gray_image.filter(ImageFilter.FIND_EDGES)
plt.subplot(3, 3, 4)
plt.imshow(sobel_image, cmap="gray")
plt.title("Edge Detection")
plt.axis("off")

# Step 6: Rotate the image by 45 degrees
rotated_image = image.rotate(45)
plt.subplot(3, 3, 5)
plt.imshow(rotated_image)
plt.title("Rotated Image")
plt.axis("off")

# Step 7: Scale the image
scaled_image = image.resize((int(image.width * 1.5), int(image.height * 1.5)))
plt.subplot(3, 3, 6)
plt.imshow(scaled_image)
plt.title("Scaled Image")
plt.axis("off")

# Step 8: Translate the image
translated_image = image.transpose(Image.FLIP_LEFT_RIGHT)
plt.subplot(3, 3, 7)
plt.imshow(translated_image)
plt.title("Translated Image")
plt.axis("off")

plt.tight_layout()
plt.show()
-------
A = np.array([ [5,6,2], [4,7, 1], [0,3,1] ])
A
B = np.array([ [1,-2,1], [4,4, 5], [5,5,1] ])
B
C = np.array([ [1,0,1], [4,1, 0], [2,0,1] ])
C
# (AB)T = AT.BT
AB  = A.dot(B)
print(AB)
AT=A.T
print(AT)
BT= B.T
print(BT)
BTAT == BT.dot(AT)
print(BTAT)
AB_T = AB.T
AB_T == BTAT
####
Ainv = np.linalg.inv(A)
print(Ainv)
Binv = np.linalg.inv(B)
print(Binv)
AB_inv = np.linalg.inv(AB)
print(AB_inv)
BinvAinv = Binv.dot(Ainv)
print(BinvAinv)
ABinv == Binv.dot(Ainv)

#####
BplusC = B+C
print(BplusC)
A_BplusC = A.dot(BplusC)
print(A_BplusC)
AB = A.dot(B)
print(AB)
AC = A.dot(C)
print(AC)
A_BplusCAC = AB +AC
print(A_BplusCAC)
A_BplusC==A_BplusCAC

#### AT =AT
AT =A.T
print(AT)
AT_inv =  np.linalg.inv(AT)
print(AT_inv)
Ainv = np.linalg.inv(A)
print(Ainv)
AinvT= Ainv.T
AT_inv ==AinvT
##
AT=  A.T
print(AT)
AA = A.dot(A)
print(AA)
ATAT = AT.dot(A).T
print(ATAT)
ATAT == AT.dot(A)
###

Cin = np.linalg.inv(C)
print(Cin)
DetCin = np.linalg.det(Cin)
print(DetCin)
DetC = np.linalg.det(C)
print(DetC)
onebyDetC = 1/DetC
print(onebyDetC)
DetCin == onebyDetC

(2) Find the critical points of the function  f(x)=x5−5x4+5x3−1
import sympy as sp

# Define the variable
x = sp.symbols('x')

# Define the function
f = x**5 - 5*x**4 + 5*x**3 - 1

# Calculate the derivative of the function
f_prime = sp.diff(f, x)

# Find the critical points by solving f'(x) = 0
critical_points = sp.solve(f_prime, x)

# Output the critical points
print("Critical points:", critical_points)

(3) The revenue generation function of an IT company is 3000x - 20x2 + 200   rupees where x is the number of employees. Find out the marginal revenue generation when 10 employees are hired.
# Define the revenue generation function
revenue_function = 3000*x - 20*x**2 + 200

# Calculate the derivative of the revenue generation function
marginal_revenue_function = sp.diff(revenue_function, x)

# Evaluate the marginal revenue function at x = 10
marginal_revenue_at_10_employees = marginal_revenue_function.subs(x, 10)

# Output the result
print("Marginal revenue generation when 10 employees are hired:", marginal_revenue_at_10_employees, "rupees")
(4) Calculate the angle between two given vectors. The two vectors are, a = i + 2j and b = 9 i + 3j
import numpy as np

# Define the vectors
a = np.array([1, 2])  # Vector a = i + 2j
b = np.array([9, 3])  # Vector b = 9i + 3j

# Calculate the dot product of the vectors
dot_product = np.dot(a, b)

# Calculate the magnitudes of the vectors
magnitude_a = np.linalg.norm(a)
magnitude_b = np.linalg.norm(b)

# Calculate the cosine of the angle between the vectors
cosine_theta = dot_product / (magnitude_a * magnitude_b)

# Calculate the angle in radians
angle_radians = np.arccos(cosine_theta)

# Convert radians to degrees
angle_degrees = np.degrees(angle_radians)

# Output the result
print("Angle between vectors a and b (in degrees):", angle_degrees)

np.linalg.norm(u) # Euclidean Distance L_2 norm

np.linalg.norm(u, ord=2)
np.linalg.norm(u, ord=1) # Manhattan Distance
Cosine Similarity
cos(θ)=u.v||u||||v||
a = np.array([12000,34])
b = np.array([174,1500000])
cos_theta = np.dot(a,b)/[np.linalg.norm(a)*np.linalg.norm(b)] # Euclidean Distance]
cos_theta
Scalar and Vector Projections
If u and v are two vectors then we define projv(u)=u.v||v||
We also define vector projection as
projv(u)=u.v||v||2v
sc_proj = np.dot(u, v) / np.linalg.norm(v)   # u.v/||v||
sc_proj
# Vector projection of u on v
vec_proj = (v * sc_proj)/ np.linalg.norm(v)   # (v * (np.dot(u,v)/np.linalg.norm(v)) / np.linalg.norm(v)  
vec_proj
# Vector Projection (vector * scalar projection) / v norm

u = np.array([1, 2, 3])   # vector u 
v = np.array([5, 6, 2])   # vector v: 
  
vec_proj = v * (np.dot(u,v)/np.linalg.norm(v)) / np.linalg.norm(v)

print("Projection of Vector u on Vector v is: ", vec_proj) 

def direction(x):
    return x/np.linalg.norm(x)

---
import numpy as np

A = np.array([[5, 6, 2], [4, 7, 19],[0, 3, 12]])
B = np.array([14, 4, 5])
C= np.array([1,2,3])
D=np.array([[1,2],[3,4]])
I= np.identity(3)
I2= np.identity(2)
det= np.linalg.det(A+I)     
print("|A+I|= :" , det)

BC= np.dot(B,C)
print("BC is :" , BC)
M= np.dot(A,B)
print("The Multiplication of  A and B is :" , M)
P= np.dot(A,A)
print("The A square is :" , P)
print('D^2-5D-2I=',np.dot(D,D)-5*D-2*I2)
Convert the following matrix to an orthogonal matrix using Gram Schmidt Process?
import numpy as np
u1= np.array([1,7,9])
u2= np.array([3,4,5])
u3= np.array([1,2,1])
v1=np.copy(u1)
#v1=np.array(v1.append(u1))
print(v1/(np.linalg.norm(v1)))
v2= u2-(np.dot(u2,v1)/(np.linalg.norm(v1))**2)*v1
print(v2/(np.linalg.norm(v2)))
v3=u3-(np.dot(u3,v1)/(np.linalg.norm(v1))**2)*v1 -(np.dot(u3,v2)/(np.linalg.norm(v2))**2)*v2
print(v3/(np.linalg.norm(v3)))
Find Scalar and Vector Projection of u on v
u = [6,7,8] v= [2,3,-1]

Verify Q is orthogonal
# Scalar Projection of u on v
u = np.array([6,7,8])
v = np.array([2,3,-1])
sc_proj = np.dot(u, v) / np.linalg.norm(v)   # u.v/||v||
sc_proj
# Vector projection of u on v
vec_proj = (v * sc_proj)/ np.linalg.norm(v)   # (v * (np.dot(u,v)/np.linalg.norm(v)) / np.linalg.norm(v)  
vec_proj
from numpy.linalg import inv
# define orthogonal matrix
Q =1/3* array([ 
[2,-2, 1],
[1, 2, 2],
[2,-1, -2]])
print(Q)
# inverse equivalence
V = inv(Q)
#print(Q.T)
print(V)
# identity equivalence
I = Q.dot(Q.T)
print(I)
---
The Following table lists the weight and heights of 5 boys Find the covariance matrix for the data.

x = [120,125,125,135,145]
y = [61,60,64,68,72]
X = np.stack((x, y), axis=0)
np.cov(X)
---
Find eigen values and eigen vectors for

from numpy import linalg as LA
import numpy as np

a = np.array([[5, 6, 2],[4,7,19],[0,3,12]])
w, v = np.linalg.eig(a)

# singular-value decomposition
from numpy import array
from scipy.linalg import svd
# define a matrix
A = array([
[1, 2],
[3, 4],
[5, 6]])
print(A)
# factorize
U, s, V = svd(A)
print(U)
print(s)
print(V)

----
principal_df
From sklearn.dataset import iris data
Preprocess the data Preprocess the data by subtracting the mean and dividing by the standard deviation of each attribute value. The resulting data should be zero-mean with variance 1.
Compute the covriance matrix
Factorize the covariance matrix using singular value decomposition (SVD) and obtain the eigenvalues and eigenvectors.
Find principle components
def PCA(X , num_components):
     
    #Step-1
    X_meaned = X - np.mean(X , axis = 0)
     
    #Step-2
    cov_mat = np.cov(X_meaned , rowvar = False)
     
    #Step-3
    eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)
     
    #Step-4
    sorted_index = np.argsort(eigen_values)[::-1]
    sorted_eigenvalue = eigen_values[sorted_index]
    sorted_eigenvectors = eigen_vectors[:,sorted_index]
     
    #Step-5
    eigenvector_subset = sorted_eigenvectors[:,0:num_components]
     
    #Step-6
    X_reduced = np.dot(eigenvector_subset.transpose() , X_meaned.transpose() ).transpose()
     
    return X_reduced

import pandas as pd
 
#Get the IRIS dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
data = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])
 
#prepare the data
x = data.iloc[:,0:4]
 
#prepare the target
target = data.iloc[:,4]
 
#Applying it to PCA function
mat_reduced = PCA(x , 2)
 
#Creating a Pandas DataFrame of reduced Dataset
principal_df = pd.DataFrame(mat_reduced , columns = ['PC1','PC2'])
 
#Concat it with target variable to create a complete Dataset
principal_df = pd.concat([principal_df , pd.DataFrame(target)] , axis = 1)

--
Define the concept of a unit vector and explain its significance in vector operations.
Calculate the angle between two given vectors. The two vectors are, 
                                    a = i ⃗ + 2j ⃗  and 
                                    b = 9 i ⃗  + 3j ⃗ 
Ans: A unit vector is a vector with a magnitude of 1. It is often denoted by a hat symbol and is used to represent direction 
in space. In vector operations, unit vectors are significant because they allow us to separate information about 
direction from information about magnitude, making calculations more intuitive and efficient.
cos Ɵ =  ( (a ) ⃗.*  (b ) ⃗)/(‖ (a ) ⃗‖ ‖ (b ) ⃗‖  )    = ((1*9)+(2*3))/(√(1+4)  √(81+9))  
= 15/(√5  √90)   =   15/(5√18)   
= 3/(3√2  )     =  1/(√2  )  

Hence Ɵ = 45°


rite down all the arithmetic operations possible on vectors.
Find the vector projection of the vector a=[3,4] on b=[5,-12]

Ans: Arithmetic operations that can be performed on vectors include:
Vector addition: Adding corresponding elements of two vectors to create a new vector.
Vector subtraction: Subtracting corresponding elements of two vectors to create a new vector.
Scalar multiplication: Multiplying a vector by a scalar value to scale its magnitude.
Dot product: Multiplying corresponding elements of two vectors and summing the results.
Cross product: Finding a new vector that is perpendicular to the plane defined by two vectors.
Vector normalization: Dividing a vector by its magnitude to create a unit vector in the same direction.
Vector projection: Finding the component of one vector in the direction of another vector.
Vector reflection: Finding the mirror image of a vector across a specified plane or line.
           

What are local minima and global minima ?
Find out the minima of the following function for the interval ( -5, -2)  f(x)=x3+2x   

Ans: Local minima and global minima are concepts in optimization and function analysis.
	Local Minima: A local minimum is a point on the graph of a function where the value of the function is smaller than 
	the values of the function at all nearby points. Formally, a point x is considered a local minimum of a function f(x)
	 if there exists a neighborhood around x such that f(x) is less than or equal to f(y) for all y in the neighborhood. 
	In other words, it is the lowest point in the vicinity of a particular location. However, it may not necessarily be 
	the lowest point of the entire function.
	
	Global Minima: A global minimum is the lowest value of a function over its entire domain. 

	It is the absolute lowest point of the function and cannot be surpassed by any other point in the function's domain.

	 Formally, a point x is considered a global minimum of a function f(x) if f(x) is less than or equal to f(y) for all y 

	in the domain of f(x).
           In essence, a local minimum is a point that is lower than its neighboring points, 
while a global minimum is the lowest point of the entire function.
 It's important to note that a global minimum is also a local minimum, but not vice versa.
derivative of f(x)

f'(x)= 3x^2 +2
critical points by setting f(x) equal to zero:
3x^2 =-2
x^2 =-2/3
Since x^2 cannot be negative, there are no real solutions for x. Hence, there are no critical points. Since there are
 no critical points within the interval (−5,−2), we'll evaluate the function f(x) at the endpoints of the interval to
 determine the minimum:

--
Write the transformation matrix for rotation and reflection of a 2d image.
Ans: 
Rotation Matrix (2D):
For rotation of an angle θ counterclockwise about the origin in a 2D plane, the transformation matrix is:
 
This matrix represents a linear transformation that rotates points in the 2D plane counterclockwise around 
the origin by an angle θ.

Reflection Matrix (2D):
For reflection about the x-axis, the transformation matrix is:
[-1 2
0 1]

This matrix reflects points in the 2D plane about the x-axis. For reflection about the y-axis, 
the transformation matrix is:
 
This matrix reflects points in the 2D plane about the y-axis. These transformation matrices can
 be used to perform rotations and reflections of 2D images by multiplying them with the coordinates 
of the points in the image.

: Critical points in a function are the values of the independent variable (usually denoted as x) 
where the derivative of the function is either zero or undefined. In other words, critical points are
 the points where the function may have a local minimum, local maximum, or a saddle point.
 f(x)=x5−5x4+5x3−1


print(w)
print(v)

plt.figure(figsize=(10, 4))
plt.plot(np.linspace(0, 30, len(record_voice)), record_voice)
plt.title('Recorded Audio Signal')
plt.xlabel('Time (s)')
plt.ylabel('Amplitude')
plt.grid(True)
plt.show()
# Normalize the signal values
pow_audio_signal = record_voice / np.power(2, 15)
# We shall now extract the first 100 values from the signal 
pow_audio_signal = pow_audio_signal [:100]
time_axis = 1000 * np.arange(0, len(pow_audio_signal), 1) / float(fs)

# Visualize the signal
plt.plot(time_axis, pow_audio_signal, color='blue')
plt.xlabel('Time (milliseconds)')
plt.ylabel('Amplitude')
plt.title('Input audio signal')
plt.show()
record_15_sec = record_voice[:int(fs * 15)]

plt.plot(np.linspace(0, 30, len(record_15_sec)), record_15_sec)
plt.
title('Trimmed to 15sec Audio Signal')
plt.xlabel('Time (s)')
plt.ylabel('Amplitude')
plt.grid(True)
plt.show()

gray_image = img.convert('L')

# Apply the Sobel filter for edge detection
edges = gray_image.filter(ImageFilter.FIND_EDGES)

# Display the original image and the detected edges
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.imshow(img)
plt.title('Original Image')
plt.subplot(1, 2, 2)
plt.imshow(edges)
plt.title('Edges Detected')
plt.show()

# use image transformation and use vector dot product 
flipped_image = img.transpose(Image.FLIP_LEFT_RIGHT)
plt.figure(figsize=(10, 5))

plt.imshow(flipped_image)
plt.title('Flipped Image')
plt.show()
# use image transformation and use vector dot product 
# Define the new dimensions for the zoomed out image
zoom_width = img.width // 2 
zoom_height = img.height // 2 
zoomed_out_image = img.resize((zoom_width, zoom_height), Image.BILINEAR)

plt.imshow(zoomed_out_image)
plt.title('Zoomed Out Image')
plt.show()
