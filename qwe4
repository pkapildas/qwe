
# Step 1: Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, cohen_kappa_score, confusion_matrix, roc_curve, auc
from scipy.stats import zscore
from imblearn.over_sampling import SMOTE
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Step 2: Load Dataset
df = pd.read_csv("loan.csv")

# -------------------- (A) Data Exploration --------------------
print("\nDataset Shape:", df.shape)
print("\nData Info:\n", df.info())
print("\nMissing Values:\n", df.isnull().sum())

# Identify Numerical & Categorical Columns
numerical_cols = df.select_dtypes(include=['number']).columns.tolist()
categorical_cols = df.select_dtypes(exclude=['number']).columns.tolist()

print("\nNumerical Columns:", numerical_cols)
print("\nCategorical Columns:", categorical_cols)

data_clust.isnull().sum().sum()
#Keep the input and output column seperate
inp_data_dime=data_clust.drop('M3',axis=1)
out=data_clust['M3']
For dimensionality reduction scaling is the must do pre-processing step. Students must perform scaling prior to dimensionality reduction. 
(Students may also performed outlier treatment, which is good to consider for awarding more marks). If they have not performed scaling, then mark must be reduced drastically.


from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
data_sc = scaler.fit_transform(inp_data_dime)
data_sc=pd.DataFrame(data_sc,columns=inp_data_dime.columns)
#1.b Perform univariate and bivariate analysis and remove any variable which is absolutely 
#    insignificant. (2 Marks)
inp_data_dime.describe()
out.value_counts().plot(kind='bar')
--
import seaborn as sb
from matplotlib import pyplot as plt
plt.figure(figsize=(18,8))
sb.heatmap(inp_data_dime.corr(),annot=True)

for i in inp_data_dime.columns:
    sb.boxplot(data_clust['M3'],data_clust[i])
    plt.show()

# 2. Apply K means clustering and identify the ideal value of K using elbow and silhoutee method
from sklearn.cluster import KMeans
wcss=[]
cl=[1,2,3,4,5,6,7,8]
for i in cl:
    mod=KMeans(n_clusters=i,random_state=10)
    mod.fit(data_sc)
    print(mod.inertia_)
    wcss.append(mod.inertia_)
plt.plot(cl,wcss) 
sil=[]
from sklearn.metrics import silhouette_score
cl=[2,3,4,5,6,7,8]
for i in cl:
    mod=KMeans(n_clusters=i)
    mod.fit(data_sc)
    sil.append(silhouette_score(data_sc,mod.labels_))
res=pd.DataFrame({'k':cl,'silhoutee':sil})
res
Apply PCA on the data. How many PCs are required to reproduce the 95% charecteristics of original data. What is the top 5 features contributing in PC1 ?
from sklearn.decomposition import PCA 
pca = PCA(n_components = data_sc.shape[1])
pca_data = pca.fit_transform(data_sc)
exp_var_ratio= pca.explained_variance_ratio_
exp_var_ratio.round(3)

cum_var=exp_var_ratio[0]
itr=2 # defined as two as first pc1 variance defined outside the loop
for j in exp_var_ratio[1:]:
    cum_var=cum_var+j
    if cum_var >= 0.95:
        break
    itr=itr+1

print('The number of principle components capturing 95 percent varaition is data is : ',itr,' Varaince explained is ', cum_var)
# Variance Ratio bar plot for each PCA components.
ax = plt.bar(range(1,len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)
plt.xlabel("PCA Components",fontweight = 'bold')
plt.ylabel("Variance Ratio",fontweight = 'bold')
# PC1 is derived from first eigen vector
e1=pd.DataFrame(pca.components_[0,:]) # first eigen vector
e1.index=data_sc.columns
e11=np.abs(e1)
e11.sort_values(0,ascending=False).head(5) # Top 5 features contributing in PC1
### 5. Build the following ML model and compare its performace: (5 Marks)
    #a. ML model with original inp_data_dime and out
    #b. ML model with inp_data_dime_pca and out
    #(Note: For pca and svd use the number of components which captures the 95 percent of variance) 
    
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

xtrain,xtest,ytrain,ytest=train_test_split(data_sc,out,test_size=0.2,random_state=48)
rf=RandomForestClassifier(random_state=48)
rf.fit(xtrain,ytrain)
ypred=rf.predict(xtest)
print('The number of input feature ',xtrain.shape[1])
print(classification_report(ytest,ypred))

xtrain,xtest,ytrain,ytest=train_test_split(pca_data[:,:5],out,test_size=0.2,random_state=48)
rf=RandomForestClassifier(random_state=48)
rf.fit(xtrain,ytrain)
ypred=rf.predict(xtest)
print('The number of PCA components ',xtrain.shape[1])
print(classification_report(ytest,ypred))
# Recomendation 
data_recom.head(2)
from surprise import KNNWithMeans,SVDpp
from surprise import Dataset
from surprise import accuracy
from surprise import Reader
from surprise.model_selection import train_test_split,cross_validate
#data_recom=pd.read_csv('recommendation_mini.csv')
ratings = data_recom
reader = Reader(rating_scale=(1, 5))

trainsetfull = rating_data.build_full_trainset()
print('Number of users: ', trainsetfull.n_users, '\n')
print('Number of items: ', trainsetfull.n_items, '\n')

alg=SVDpp()
alg.fit(trainsetfull) 
alg.predict(uid = 'A2CX7LUOHB2NDG', iid ='059400232X')

results = cross_validate(
    algo = alg, data = rating_data, measures=['RMSE'], 
    cv=3)
results['test_rmse'].mean()
---
3 (a) Build the popularity based recommendation system and suggest top 5 items. (5 Marks)
data_recom=data.iloc[:,24:28]
data_recom.shape
# Top 5 Items based on Average Rating
pd.DataFrame(data.groupby('ItemID')['Rating'].mean().sort_values(ascending=False))
popularity_table = data.groupby('ItemID').agg({'Rating' : 'mean'})
popularity_table.head(5)
popularity_table.sort_values('Rating', ascending=False).head(5)
#Build collaborative recommendation engine to recommend a top 5 items to the specific user. Measure the model quality in terms of RMSE
reader = Reader(rating_scale =(1,5))
rating_data = Dataset.load_from_df(data_recom[['UserID', 'ItemID', 'Rating']], reader)
rating_data
[train_set, test_set] = train_test_split(rating_data, test_size=.15, shuffle=True)
trainsetfull = rating_data.build_full_trainset()
print('Number of Users  : ',trainsetfull.n_users, '\n')
print('Number of Items  : ',trainsetfull.n_items, '\n')
algo = KNNWithMeans(k= 15, min_k = 5 , sim_options = {'name':'pearson', 'user_based' :False}, verbose =True)
results = cross_validate( algo = algo, data =rating_data , measures =['RMSE'], 
                        cv =5, return_train_measures=True)
print(results['test_rmse'].mean())

alg = SVDpp()
alg.fit(trainsetfull)
alg.predict(uid =76, iid =2)
item_id = data_recom['ItemID'].unique()
item_id76 = data_recom.loc[data_recom['UserID']==76, 'ItemID']
item_id_pred = np.setdiff1d(item_id, item_id76)
testset= [[76, iid, 4] for iid in item_id_pred]
pred = alg.test(testset)
pred
--
pred_ratings = np.array([pred1.est for pred1 in pred])
i_max= pred_ratings.argmax()
iid = item_id_pred[i_max]
print('Top item for user has iid {0} with predicted rating{1}'.
      format(iid, pred_ratings[i_max]))
r_df = data_recom.pivot(index ='UserID', columns ='ItemID', values='Rating').fillna(0)



model = SVD(random_state =42)
model.fit(trainsetfull)
pred = model.test(testset)
print("Collabrative Filter Model RMSE ", accuracy.rmse(pred))
# for a user
num_recomendations =5
userId =76
user_row_number = userId -1 
all_items= data_recom['ItemID'].unique()
pred_ratings = [(item, model.predict(userId, item).est) for item in all_items]
top_5_reco = sorted(pred_ratings, key = lambda x :x[1], reverse =True)[:5]
print(f' Top 5 Recomended items for User id {userId}')
for item , rating in top_5_reco :
    print(f'item : {item}, Predicted Rating {rating}')



#Imabalanced data
# -------------------- (B) Data Cleaning --------------------

#First run a check for the presence of missing values and their percentage for each column. Then choose the right approach to treat them.
Total = df_admissions.isnull().sum().sort_values(ascending=False)          
Percent = (df_admissions.isnull().sum()*100/df_admissions.isnull().count()).sort_values(ascending=False)   
missing_data = pd.concat([Total, Percent], axis = 1, keys = ['Total', 'Percentage of Missing Values'])    
missing_data
df_num = df_feature.select_dtypes(include = [np.number])
df_cat = df_feature.select_dtypes(include = [np.object])

 

"""
imputer_num = SimpleImputer(strategy="median")
df[numerical_cols] = imputer_num.fit_transform(df[numerical_cols])

imputer_cat = SimpleImputer(strategy="most_frequent")
df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])
"""

for col in numerical_cols:
    if df[col].isnull().sum() > 0:  
        median_value = df[col].median()  # Compute median dynamically
        df[col].fillna(median_value, inplace=True)  # Replace missing values with median

for col in categorical_cols:
    if df[col].isnull().sum() > 0:
        mode_value = df[col].mode()[0]  # Compute mode dynamically
        df[col].fillna(mode_value, inplace=True)  # Replace missing values with mode
#Examine Outliers
for i in df_f.columns:
    sb.boxplot(df_f[i])
    plt.show()

for i in df_f.columns:
    sb.kdeplot(stats.zscore(df_f[i]))
    plt.axvline(3, ymin=0, ymax=0.01, c='r')
    plt.axvline(-3, ymin=0, ymax=0.01, c='r')
    plt.show()

# Handling Outliers using Z-score
z_scores = np.abs(zscore(df[numerical_cols]))
df = df[(z_scores < 3).all(axis=1)]  

 # removal of outliers
Q1=data[num_cols].quantile(0.25)
Q3=data[num_cols].quantile(0.75)
IQR=Q3-Q1
lower_bound=Q1-1.5*IQR
upper_bound=Q3+1.5*IQR
data_fix=data[~((data[num_cols]<lower_bound) | (data[num_cols]>upper_bound)).any(axis=1)]
print(data.shape)
print(data_fix.shape)

# for the independent numeric variables, we plot the histogram to check the distribution of the variables
# Note: the hist() function considers the numeric variables only, by default
# we drop the target variable using drop()
# 'axis=1' drops the specified column
df_admissions.drop('Chance of Admit', axis = 1).hist()
plt.tight_layout()
# skew() returns the coefficient of skewness for each variable
df_admissions.drop('Chance of Admit', axis = 1).skew()
#Correlation 
    sb.heatmap(df_n.corr(), annot=True, cmap='rainbow')

# Check skewness for numerical features
for col in df_num:
    skewness = skew(df[col])
    print(f"Skewness of {col}: {skewness}")

# -------------------- (C) Feature Engineering --------------------
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

