Machine Learning is the science to make computers learn from data without programming them explicitly and improve their learning over time in an autonomous fashion
This learning comes by feeding the data in the form of observations and real-world interactions
Supervised Learning - Training happens based on labelled data●Unsupervised Learning - Meant to recognise patterns in unlabelled data●Reinforcement Learning - Machine gets rewarded for right outcome
Sl -Class of machine learning that work on externally supplied instances in form of predictor attributes and associated target values.-The model learns from the training data using these ‘target variables’ as reference variables.
Regression Linear Regression-kNN regressor-SVR-Decision tree regressor-Random forest regressor-Neural Network
Classification- Logistic regression-k Nearest Neighbours-Decision tree-Support vector machines-Random forest-Naive bayes
Unsupervised learning-K-means clustering-Hierarchical clustering-Principal component analysis-Hidden Markov Model -FP-Growth-Apriori Analysis
What is regression analysis?●Regression analysis allows us to examine which independent variables have an impact on the dependent variable●Regression analysis investigates and models the relationship between variables●Determine which independent variables can be ignored, which ones are most important and how they influence each other●We shall first see simple linear regression and then multiple linear regression
U Measures of Variation he sum of squares total (SST) is the sum of squared differences between the observation and its mean●It can be seen as the total variation of the response variable about its mean value ●SST is the measure of variability in the response variable without considering the effect of dependent variable ●Also known as Total Sum of Square (TSS)
/ The sum of squares regression (SSR) is the sum of squared differences between the predicted value and the mean of the response variable●SSR is the measure of variability in the response variable considering the effect of dependent variable ●It is the explained variation●Also known as Regression Sum of Square (RSS)
//The sum of squares of error (SSE) is the sum of squared differences between observed response variable and its predicted value ●SSE is the measure of variability in the response variable remaining after considering the effect of dependent variable ●It is the unexplained variation●Also known as Error Sum of Square (ESS)
Total variation   =     Explained variation + Unexplained variation SST =SSr+SSe
R-squared●Since 0 ≤ SSE ≤ SST, mathematically we have 0 ≤ R2 ≤ 1●R2 assumes that all the independent variables explain the variation in dependent variable●For simple linear regression, the squared correlation between the response variable Y and independent variable X is the R2 value●For our model, R2  = 0.226. It implies that 22.6% variation in premium amounts is explained by the mileage of a ca
Demerits of R-squared●The value of R2 increases as new numeric predictors are added to themodel, it may appear that it is a better model, which can be misleading●Also, if the model has too many variables, the model is feared to beoverfitted. Overfitted data generally has a high R2 value.
R2 also called the coefficient of determinationgives total percentage of variation in Y that is explained by predictor variable.
R^2 = explainedVariatio/TotalVariation SSr/SSt 0<=R2<=1  R2 = 1- sse/sst
Adjusted R2 gives the percentage of variation explained by independent variables that actually affect the dependent variable
R2adj≤ R2 (always)●As the number of independent variables in the model increase, the adjusted R2 will decrease unless the model significantly increases the  R2●So to know whether addition of a variable explains the variation of the response variable, compare the R2adj values along with R2
What is multicollinearity?●Multicollinearity arises when the independent variables have high correlation among each other●Multicollinearity may be introduced if there exists empirical relationship among variables such as income = expenditure + saving●In presence of it, the best fit line obtained from OLS method is no more “best”●Also, the confidence interval obtained for β’s is wider since the SE(β) becomes large
Determinant of correlation matrix●Condition Number (CN)●Correlation matrix●Variance Inflation Number (VIF)Is there multicollinearity present?Which variables are involved in multicollinearity?
Correlation matrix: If the off-diagonal values tend to 土1 then it indicates high correlation between the variable pair. However this inspection is not enough
hich variables are involved in multicollinearity? Variance Inflation Factor (VIF) = 1/1-R^2, Which variables are involved in multicollinearity? Variance Inflation Factor (VIF): Where R2 is obtained by regressing a predictor variable over all the other predictors in the model
VIF > 5 High correlation  5 > VIF >1Moderate correlationVIF = 1No correlation
ests after model building●Linear relationship between dependent and independent variables●Independence of observations should exist (i.e. Absence of Autocorrelation)●The error terms should be homoscedastic●The error terms must follow normal distribution
 Assumption of autocorrelation is violated when residuals are correlated within themselves, i.e. they are serially correlated●Autocorrelation does not impact the regression coefficients but the associated standard errors are reduced●This reduction in standard error leads to a reduction in associated p-value●It incorrectly concludes that a predictor is statistically significant
Causes of autocorrelation●Some important variables are not considered in the data●If the relationship between the target and predictor variables is non-linear and is incorrectly considered linear●Presence of carry over effectExample: The additional expenses from the budget for last month are carried over in creating the budget for next month
Durbin - Watson Test●Failing to reject H0, will imply that the error terms are autocorrelated●To test whether the error terms are autocorrelated,we Durbin-Watson test●We test whether autocorrelation is present or not●The hypothesis is given by:                                              against H0:The error terms are not autocorrelatedH1:The error terms are autocorrelated
Homoscedasticity assumptionVariance of the residual is assumed to be independent of the explanatoryvariables●Heteroscedasticity: non-constant variance of residuals●It happens due to the presence of extreme valuesHomoscedasticitySameVariance
Homoscedasticity There is no visible funnel or bow type pattern in the plot●We can see presence of “Homoscedasticity”Homoscedasticity

Heteroscedasticity Funnel type shape is seen in the graph●Hence we can say that there is a presence of “Heteroscedasticity”
The plot of residuals against the fitted values tells whether the error terms have equal variance
It should look random, i.e., it should notexhibit much distinctive pattern, no non-linear trends or changes in variabili
The statistical test to test for the homoskedasticity of the errors are●Goldfeld Quandt test●Breusch Pagan test
Quantile-Quantile Plot (QQ plot)●Used to determine whether two datasets follow the same distribution●The quantiles of two datasets are plotted against each other●A reference line is plotted at 450●If the points lie on the reference line we conclude they follow the same distribution
The model evaluation metrics are ●R2●Adjusted R2●The F test for overall significance
The R2 value gives the percentage of variation in the response variable explained by the predictor variables●If the values of R2 = 0.87, it implies that 87% of variation in the response variable is explained by the predictor variables
Adjusted R2 gives the percentage of variation explained by independent variables that actually affect the dependent variable●If the values of R2 = 0.87, it implies that 87% of variation in the response variable is explained by the predictor variable
Linear regression of categorical variable●The regression method fails in presence of categorical variable●Thus we need to convert the categorical variable to numeric variable●In order to so, we use N - 1 dummy encoding
N-1 dummy encoding●Dummy variables are binary variables used to represent categorical data ●For a categorical variable that can take k values k-1 dummy variables need to be created ●Dummy variable is assigned 1 if it takes a particular value else it is assigned 0
Feature Transformation: Replacing the existing features by function of these feature●Feature Engineering: Creating new features based on empirical relationships●Feature Selection: Fitting a model of  significant features
Why do we need feature transformation?●Incase of skewed (predictor and/or dependent) variable, we transform it to reduce the skewness●If the assumptions of linear regression are not met, transformation of skewed target variable can be used for making the error terms more compatible to the assumptions●If the relationship between a predictor and the response variable is non-linear, it can be linearized using transformation
Comparison of model performance should be done using the original units for the target variable and not the units after transformatio
Transformation methods●Logarithmic transformation●Square root transformation●Reciprocal transformation●Exponential transformation●Box-cox transformation
To linearize, values of a variableare replaced with its natural log●It cannot be used on a categorical variable after dummy encoding since ln(0) is undefineAlso if a variable takes zero or negative values, logarithmic transform cannot be used on it
Sqauar Values of a variableare replaced with its square root●To reduce right skewness, we may use square root transformation●It can be applied even when the variable takes a zero value
Reciprocal transformation●Values of a variableare replaced with its reciprocal ●It can not be applied only when the variable takes zero values●However, can be applied to negative values●Example: population per area (population density) transforms to area per person
Values of a variableare replaced with its exponential●It is generally used to transform logarithmic transformed data to get the original data back 
The Box-Cox transformation can only be used on positive variables●Generalized form of logarithmic transformation
Feature scaling●It is a technique used to transform the data into a common scale●Since the features have various ranges, it becomes a necessary step in data preprocessing while using machine learning algorithms●Since most machine learning algorithms use distance calculations, features taking higher values will weigh in more in the distance compared to features taking values of low magnitude
Normalization●Standardization
Normalization is the process of rescaling features in the range 0 to 1, Standardization rescales the feature such that it has mean 0 and unit variance●The procedure involves subtracting the mean from observation and then dividingby the standard deviation
Normalization is a good technique to use when you do not know the distribution of yourdata or when you know the distribution is not Gaussian (a bell curve).Standardization assumes that your data has a Gaussian (bell curve) distribution. Thisdoes not strictly have to be true, but the technique is more effective if your attributedistribution is Gaussian.
Normalization or Min-Max normalization tries to get the values closer to mean, but when there are outliersin the data which are important and we don't want to lose their impact, we go with Standardization or Zscore normalizationMin-  Max  tries  to  get  the  values  closer  to  mean.  But  when  there  are  outliers  in  the  data  which  areimportant and we don’t want to lose their impact, we go with Z score. In this case, we rescale an originalvariable to have a mean of zero and a standard deviation of one. It does not have any units: hence isuseful for comparing variables expressed in different units.   Standardization makes no difference to theshape of a distribution
Feature Selection This can be achieved by:○Forward selection method○Backward elimination method○Stepwise method
Forward Selection = Start with a null model (with no predictors)2.Obtain the correlation between Y and each variable. The variable with highest correlation gets added to the model (say Xm). Build a model Y ~ Xm3.Obtain the correlation between Y and remaining (k-1) variables. The next variable (say Xp) is included, which has the highest correlation with Y after removing Xm 4.Build a model Y ~ Xm + Xp. If Xp is significant include it in the model else discard5.Repeat steps (3) and (4) until reaching the stopping rule or running out of variables
Backward elimination method
Start with a full model (model with all k predictors)2.Remove the variable which is least significant (variable with largest p-value)3.Fit a new model with remaining (k-1) regressors4.The next variable (say Xp) is removed if it is least significant 5.Repeat steps (3) and (4) until reaching the stopping rule or all variables are significant
It is a combination of forward selection and backward elimination method ●Procedure:○Start with a null model (with no predictors)○At each step add or remove variable based on its corresponding p-value○Stop when no variable can be added or removed justifiably
Recursive feature elimination (RFE)●It is an instance of backward feature elimination●Procedure:○Train a full model○Create subsets for features○Set the subset size○Compute the ranking criteria for each feature subset○Remove the feature subset that has the least ranking
Bias is the difference between a model’s predicted values and the observed values●Variance of a model is the difference between predictions if the model is fit to different datasets
Bias-Variance for a simple model●If the model is too simple it will have a high bias and low variance●Such a model will give not perfectly accurate predictions, but the predictions will be consistent●The model will not be flexible enough to learn from majority of given data, this is termed as underfitting
Bias-Variance for a complex model●If the model is too complex it will have a low bias and high variance●Such a model will give accurate predictions but inconsistently●The high variance indicates it will have a much better fit on the train data compared to the test data, this is termed as overfitting
The model validation methods use test data to validate the model built using train data●The model validation:○k - fold cross validation○Leave one out cross validation (LOOCV)
Cross validation Train setTest setTrain setTest setRun = 1Run = 2●Procedure:○Consider a data having ‘2n’ observations○Partition the dataset into two subsets: train and test sets of the equal size (n) ○Measure the model performance○Swap the train and test sets○Total error is obtained by summing up the errors for both runs●This method is known as two fold cross validation●Here, each observation is used exactly once for training and once for testing
K Fold Procedure:○Partition the dataset into ‘k’ subsets○Consider one subset as the test set and remaining subsets as train set○Measure the model performance○Repeat this until all k subsets are considered as test set○Total error is obtained by summing up the errors for all the k runs●This method is known as the k - fold cross validation●Here, each observation is used exactly k times for training and exactly once for testin
LOOCV (leave one (record) out cross validation)●It is a special case of k - fold cross validation method. Instead of subsetting the data, at every run one observation is considered as the test set●For n observations, there are n runs●The total error is the sum of errors for n runs●In LOOCV, the estimates from each fold are highly correlated and their average can have a high level of variance
cost function tells how good the model performs at making predictions for a given set of parameters●Cost function = Loss function = Error function●For  linear regression, the cost function is given by the sum of squares of residuals
 Types of gradient descent●Batch gradient descent●Stochastic gradient descent●Mini batch gradient descen  -Proce -Start with some initial set of parameters●Compute the cost function●The derivative of the cost function (delta; δ)  is calculated●Update the parameters based on learning rate α and derivative δ●Repeat the procedure until the derivative of cost function is zero
if a model performs very well on the training data but does not perform well on the testing data, it is said to have high generalization error●High generalization error implies overfitting●Generalization error can be reduced by avoiding overfitting in the model
Prevention of overfitting for Linear regressionTo make our model more robust and fix the problem of overfitting, we need to:●Shrink the coefficients or weights of features in model●Eliminate high degree polynomial feature from a polynomial modelThis can be achieved by using regularization.
To make our model more robust and fix the problem of overfitting, we need to shrink the coefficients or weights of features in model.This can be achieved by using regularization.
Regularization refers to the modifications we make to a learning algorithm,that help in reducing its generalization error but not its training error●Regularization adds a penalty term to the cost function such that the modelwith higher variance receives a larger penalty●It chooses a model with smaller parameter values (i.e. shrinked coefficients)that has less error
Ridge regression uses squared L-2 norm regularization i.e it adds a squared L-2 penalty ●Also known as L-2 regularization●Squared L-2 penalty is equal to squares of magnitudes of β coefficients
Lasso regression uses L-1 norm regularization i.e it adds a L-1 penalty ●Also known as L-1 regularization●L-1 penalty is equal to absolute value of β coefficients●It extinguishes the insignificant predictors
Comparison of RMSE values - We check the train accuracy and test accuracy on train and test data respectively  A good model that generalizes well needs to have very similar errors on train and test sets●Here, the difference between errors for train and test sets is significant, hence we can conclude that the model is overfitting the train data
he lasso regression shrinks the β coefficients of variables Mileage and Age to 0, thus eliminating them from the final model ●This is an instance of how lasso regression performs feature selection

When to use which regularization?●If there are many interactions present or it is important to consider all the predictors in the model, ridge regression is used●If the dataset contains some useless independent variables that can be eliminated from the model, lasso regression is used●If the dataset contains too many variables where it is practically impossible to determine whether to use ridge or lasso regression, elastic-net regression is used
he grid search is the process of tuning the hyperparameters to obtain the optimum values of the hyperparameters●We use the the ‘GridSearchCV’ method to tune the hyperparameters●Procedure:○Just as the name suggests, a grid of performance measure is obtained○Wherein each measure corresponds to a given hyperparameter values○Obtain the corresponding hyperparameters whose performance measure is highest
Bayesian Optimization (BO)●It builds a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function.●Keep track of past evaluation results which they use to form a probabilistic model mapping hyperparameters to a probability of a score on the objective function.●It spend less time to reach the highest accuracy model than the previously discussed methods.
BO Essentials●Search Space : A domain of hyperparameters over which to search●Objective function : to input hyperparameters and outputs a score that has to minimize (or maximize)● Probability Model : probability representation of the objective function built using previous evaluation●Criteria : selection function is the criteria by which the next set of hyperparameters are chosen from the Probability /surrogate function

a. Read the dataset (tab, csv, xls, txt, inbuilt dataset). What are the number of rows and no. of cols & types of variables (continuous, categorical etc.)? (1 MARK)

b. Calculate five-point summary for numerical variables (1 MARK)

c. Summarize observations for categorical variables – no. of categories, % observations in each category. (1 mark)

d. Check for defects in the data such as missing values, null, outliers, etc. (2 marks)
cars = pd.read_csv('Car_Data.csv')
cars.head()
cars.shape
cat_cols = cars.select_dtypes(include = 'object')
num_cols = cars.select_dtypes(include = np.number)
print('Continuous variables are : ',num_cols.columns)
print('Categorical Columns are : ',cat_cols.columns)
cars.describe() # b 
c
cars.info()
print('% Values in each categorical columns')
for i in cat_cols.columns:
    print('\n% Values in column ',i)
    print((cars[i].value_counts()/len(cars[i])*100))
d
cars.isnull().sum()
outliers
plt.figure(figsize=(10,20))
for i,col in enumerate(num_cols,1):
    plt.subplot(16,1,i)
    sns.boxplot(cars[col])
    plt.ylabel(col)
plt.show()
Data Cleaning
cars['CarName'].unique()
#Splitting company name from CarName column
CompanyName = cars['CarName'].apply(lambda x : x.split(' ')[0])
cars.insert(3,"CompanyName",CompanyName)
cars.drop(['CarName'],axis=1,inplace=True)
cars.head()
cars.CompanyName.unique()
Fixing invalid name 
--
There seems to be some spelling error in the CompanyName column.

maxda = mazda
Nissan = nissan
porsche = porcshce
toyota = toyouta
vokswagen = volkswagen = vw
---
cars.CompanyName = cars.CompanyName.str.lower()

def replace_name(a,b):
    cars.CompanyName.replace(a,b,inplace=True)

replace_name('maxda','mazda')
replace_name('porcshce','porsche')
replace_name('toyouta','toyota')
replace_name('vokswagen','volkswagen')
replace_name('vw','volkswagen')

cars.CompanyName.unique()

plt.figure(figsize=(20,8))

plt.subplot(1,2,1)
plt.title('Car Price Distribution Plot')
sns.distplot(cars.price)

plt.subplot(1,2,2)
plt.title('Car Price Spread')
sns.boxplot(y=cars.price)

plt.show()
print(cars.price.describe(percentiles = [0.25,0.50,0.75,0.85,0.90,1]))
Inference :
The plot seemed to be right-skewed, meaning that the most prices in the dataset are low(Below 15,000).
There is a significant difference between the mean and the median of the price distribution.
The data points are far spread out from the mean, which indicates a high variance in the car prices.(85% of the prices are below 18,500, whereas the remaining 15% are between 18,500 and 45,400.)
2. Data Preparation (15 marks)
a. Fix the defects found above and do appropriate treatment if any. (5 marks)

b. Visualize the data using relevant plots. Find out the variables which are highly correlated with target variable? (5 marks)

c. Do you want to exclude some variables from the model based on this analysis? What other actions will you take? (2 marks)

d. Split dataset into train and test (70:30). Are both train and test representative of the overall data? How would you ascertain this statistically? (3 marks)
plt.figure(figsize=(25, 6))

plt.subplot(1,3,1)
plt1 = cars.CompanyName.value_counts().plot('bar')
plt.title('Companies Histogram')
plt1.set(xlabel = 'Car company', ylabel='Frequency of company')

plt.subplot(1,3,2)
plt1 = cars.fueltype.value_counts().plot('bar')
plt.title('Fuel Type Histogram')
plt1.set(xlabel = 'Fuel Type', ylabel='Frequency of fuel type')

plt.subplot(1,3,3)
plt1 = cars.carbody.value_counts().plot('bar')
plt.title('Car Type Histogram')
plt1.set(xlabel = 'Car Type', ylabel='Frequency of Car type')

plt.show()
Inference :
Toyota seemed to be favored car company.
Number of gas fueled cars are more than diesel.
sedan is the top car type prefered.

plt.figure(figsize=(20,8))

plt.subplot(1,2,1)
plt.title('Symboling Histogram')
sns.countplot(cars.symboling, palette=("cubehelix"))

plt.subplot(1,2,2)
plt.title('Symboling vs Price')
sns.boxplot(x=cars.symboling, y=cars.price, palette=("cubehelix"))

plt.show()
Inference :
It seems that the symboling with 0 and 1 values have high number of rows (i.e. They are most sold.)
The cars with -1 symboling seems to be high priced (as it makes sense too, insurance risk rating -1 is quite good). But it seems that symboling with 3 value has the price range similar to -2 value. There is a dip in price at symboling 1.
plt.figure(figsize=(20,8))

plt.subplot(1,2,1)
plt.title('Engine Type Histogram')
sns.countplot(cars.enginetype, palette=("Blues_d"))

plt.subplot(1,2,2)
plt.title('Engine Type vs Price')
sns.boxplot(x=cars.enginetype, y=cars.price, palette=("PuBuGn"))

plt.show()

df = pd.DataFrame(cars.groupby(['enginetype'])['price'].mean().sort_values(ascending = False))
df.plot.bar(figsize=(8,6))
plt.title('Engine Type vs Average Price')
plt.show()
---
Inference :
ohc Engine type seems to be most favored type.
ohcv has the highest price range (While dohcv has only one row), ohc and ohcf have the low price range.
---
plt.figure(figsize=(25, 6))

df = pd.DataFrame(cars.groupby(['CompanyName'])['price'].mean().sort_values(ascending = False))
df.plot.bar()
plt.title('Company Name vs Average Price')
plt.show()

df = pd.DataFrame(cars.groupby(['fueltype'])['price'].mean().sort_values(ascending = False))
df.plot.bar()
plt.title('Fuel Type vs Average Price')
plt.show()

df = pd.DataFrame(cars.groupby(['carbody'])['price'].mean().sort_values(ascending = False))
df.plot.bar()
plt.title('Car Type vs Average Price')
plt.show()
---
Inference :
Jaguar and Buick seem to have highest average price.
diesel has higher average price than gas.
hardtop and convertible have higher average price.
---
plt.figure(figsize=(15,5))

plt.subplot(1,2,1)
plt.title('Door Number Histogram')
sns.countplot(cars.doornumber, palette=("plasma"))

plt.subplot(1,2,2)
plt.title('Door Number vs Price')
sns.boxplot(x=cars.doornumber, y=cars.price, palette=("plasma"))

plt.show()

plt.figure(figsize=(15,5))

plt.subplot(1,2,1)
plt.title('Aspiration Histogram')
sns.countplot(cars.aspiration, palette=("plasma"))

plt.subplot(1,2,2)
plt.title('Aspiration vs Price')
sns.boxplot(x=cars.aspiration, y=cars.price, palette=("plasma"))

plt.show()
--
Inference :
doornumber variable is not affacting the price much. There is no sugnificant difference between the categories in it.
It seems aspiration with turbo have higher price range than the std(though it has some high values outside the whiskers.
def plot_count(x,fig):
    plt.subplot(4,2,fig)
    plt.title(x+' Histogram')
    sns.countplot(cars[x],palette=("magma"))
    plt.subplot(4,2,(fig+1))
    plt.title(x+' vs Price')
    sns.boxplot(x=cars[x], y=cars.price, palette=("magma"))
    
plt.figure(figsize=(15,20))

plot_count('enginelocation', 1)
plot_count('cylindernumber', 3)
plot_count('fuelsystem', 5)
plot_count('drivewheel', 7)

plt.tight_layout()
Inference :
Very few datapoints for enginelocation categories to make an inference.
Most common number of cylinders are four, six and five. Though eight cylinders have the highest price range.
mpfi and 2bbl are most common type of fuel systems. mpfi and idi having the highest price range. But there are few data for other categories to derive any meaningful inference
A very significant difference in drivewheel category. Most high ranged cars seeme to prefer rwd drivewheel.
---
def scatter(x,fig):
    plt.subplot(5,2,fig)
    plt.scatter(cars[x],cars['price'])
    plt.title(x+' vs Price')
    plt.ylabel('Price')
    plt.xlabel(x)

plt.figure(figsize=(10,20))

scatter('carlength', 1)
scatter('carwidth', 2)
scatter('carheight', 3)
scatter('curbweight', 4)

plt.tight_layout()
Inference :
carwidth, carlength and curbweight seems to have a poitive correlation with price.
carheight doesn't show any significant trend with price.
c. Do you want to exclude some variables from the model based on this analysis? What other actions will you take? (2 marks)
def pp(x,y,z):
    sns.pairplot(cars, x_vars=[x,y,z], y_vars='price',size=4, aspect=1, kind='scatter')
    plt.show()

pp('enginesize', 'boreratio', 'stroke')
pp('compressionratio', 'horsepower', 'peakrpm')
pp('wheelbase', 'citympg', 'highwaympg')
Inference :
enginesize, boreratio, horsepower, wheelbase - seem to have a significant positive correlation with price.
citympg, highwaympg - seem to have a significant negative correlation with price.

np.corrcoef(cars['carlength'], cars['carwidth'])[0, 1]
features engineering
#Fuel economy
cars['fueleconomy'] = (0.55 * cars['citympg']) + (0.45 * cars['highwaympg'])
#Binning the Car Companies based on avg prices of each Company.
cars['price'] = cars['price'].astype('int')
temp = cars.copy()
table = temp.groupby(['CompanyName'])['price'].mean()
temp = temp.merge(table.reset_index(), how='left',on='CompanyName')
bins = [0,10000,20000,40000]
cars_bin=['Budget','Medium','Highend']
cars['carsrange'] = pd.cut(temp['price_y'],bins,right=False,labels=cars_bin)
cars.head()
Bivariate Analysis
plt.figure(figsize=(8,6))

plt.title('Fuel economy vs Price')
sns.scatterplot(x=cars['fueleconomy'],y=cars['price'],hue=cars['drivewheel'])
plt.xlabel('Fuel Economy')
plt.ylabel('Price')

plt.show()
plt.tight_layout()
Inference :
fueleconomy has an obvios negative correlation with price and is significant.
plt.figure(figsize=(25, 6))

df = pd.DataFrame(cars.groupby(['fuelsystem','drivewheel','carsrange'])['price'].mean().unstack(fill_value=0))
df.plot.bar()
plt.title('Car Range vs Average Price')
plt.show()
Inference :
High ranged cars prefer rwd drivewheel with idi or mpfi fuelsystem.
cars_lr = cars[['price', 'fueltype', 'aspiration','carbody', 'drivewheel','wheelbase',
                  'curbweight', 'enginetype', 'cylindernumber', 'enginesize', 'boreratio','horsepower', 
                    'fueleconomy', 'carlength','carwidth', 'carsrange']]
cars_lr.head()
sns.pairplot(cars_lr)
plt.show()

# Defining the map function
def dummies(x,df):
    temp = pd.get_dummies(df[x], drop_first = True)
    df = pd.concat([df, temp], axis = 1)
    df.drop([x], axis = 1, inplace = True)
    return df
# Applying the function to the cars_lr

cars_lr = dummies('fueltype',cars_lr)
cars_lr = dummies('aspiration',cars_lr)
cars_lr = dummies('carbody',cars_lr)
cars_lr = dummies('drivewheel',cars_lr)
cars_lr = dummies('enginetype',cars_lr)
cars_lr = dummies('cylindernumber',cars_lr)
cars_lr = dummies('carsrange',cars_lr)

cars_lr.head()

d. Split dataset into train and test (70:30). Are both train and test representative of the overall data? How would you ascertain this statistically? (3 marks)
from sklearn.model_selection import train_test_split

np.random.seed(0)
df_train, df_test = train_test_split(cars_lr, train_size = 0.7, test_size = 0.3, random_state = 100)
--
stats.ttest_ind(df_train.iloc[:,1:], df_test.iloc[:,1:])
---
stats.ttest_ind(df_train.iloc[:,0], df_test.iloc[:,0])
#All the pvalues> 0.05
--
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
num_vars = ['wheelbase', 'curbweight', 'enginesize', 'boreratio', 'horsepower','fueleconomy','carlength','carwidth','price']
df_train[num_vars] = scaler.fit_transform(df_train[num_vars])
--
df_train.head()
--
df_test.head()
---
df_train.describe()
--
#Correlation using heatmap
plt.figure(figsize = (30, 25))
sns.heatmap(df_train.corr(), annot = True, cmap="YlGnBu")
plt.show()
---
Highly correlated variables to price are - curbweight, enginesize, horsepower,carwidth and highend.

#Dividing data into X and y variables
y_train = df_train.pop('price')
X_train = df_train
==
from scipy import stats
for i in X_train:
    if i != 'price':
        print(ttest_1samp(X_train[i],X[i].mean()))
--
for i in X_test:
    if i != 'price':
        print(ttest_1samp(X_test[i],X[i].mean()))
3. Model Building (20 marks)
a. Fit a base model and observe the overall R- Squared, RMSE and MAPE values of the model. Please comment on whether it is good or not. (5 marks)

b. Check for multi-collinearity and treat the same. (3 marks)

c. How would you improve the model? Write clearly the changes that you will make before re-fitting the model. Fit the final model. (6 marks)

d. Write down a business interpretation/explanation of the model – which variables are affecting the target the most and explain the relationship. Feel free to use charts or graphs to explain. (4 marks)

e. What changes from the base model had the most effect on model performance? (2 marks)
a
#RFE
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm 
from statsmodels.stats.outliers_influence import variance_inflation_factor

lm = LinearRegression()
lm.fit(X_train,y_train)
rfe = RFE(lm, 10)
rfe = rfe.fit(X_train, y_train)

X_train.columns[rfe.support_]

Building model using statsmodel, for the detailed statistics
X_train_rfe = X_train[X_train.columns[rfe.support_]]
X_train_rfe.head()
def build_model(X,y):
    X = sm.add_constant(X) #Adding the constant
    lm = sm.OLS(y,X).fit() # fitting the model
    print(lm.summary()) # model summary
    return X
    
def checkVIF(X):
    vif = pd.DataFrame()
    vif['Features'] = X.columns
    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    vif['VIF'] = round(vif['VIF'], 2)
    vif = vif.sort_values(by = "VIF", ascending = False)
    return(vif)

MODEL 1

X_train_new = build_model(X_train_rfe,y_train)

--Coment 
p-vale of twelve seems to be higher than the significance value of 0.05, hence dropping it as it is insignificant in presence of other variables.

Here overall R- Squared
R-squared: 0.929
--
X_train_new = X_train_rfe.drop(["twelve"], axis = 1)
X_train_new = build_model(X_train_new,y_train)
---
X_train_new = X_train_new.drop(["fueleconomy"], axis = 1)  #pvalue>0.05

X_train_new = build_model(X_train_new,y_train)

variables are significant?
List of significant variables :
b. Check for multi-collinearity and treat the same. (3 marks)
#Calculating the Variance Inflation Factor
checkVIF(X_train_new)
dropping curbweight because of high VIF value. (shows that curbweight has high multicollinearity.)
X_train_new = X_train_new.drop(["curbweight"], axis = 1)
c. How would you improve the model? Write clearly the changes that you will make before re-fitting the model. Fit the final model. (6 marks)
X_train_new = build_model(X_train_new,y_train)
checkVIF(X_train_new)

dropping sedan because of high VIF value.


X_train_new = X_train_new.drop(["sedan"], axis = 1)
MODEL
X_train_new = build_model(X_train_new,y_train)
checkVIF(X_train_new)
dropping wagon because of high p-value.

X_train_new = X_train_new.drop(["wagon"], axis = 1)

X_train_new = build_model(X_train_new,y_train)
checkVIF(X_train_new)
#Dropping dohcv to see the changes in model statistics
X_train_new = X_train_new.drop(["dohcv"], axis = 1)
X_train_new = build_model(X_train_new,y_train)
checkVIF(X_train_new)
Residual Analysis of Model
lm = sm.OLS(y_train,X_train_new).fit()
y_train_price = lm.predict(X_train_new)
# Plot the histogram of the error terms
fig = plt.figure()
sns.distplot((y_train - y_train_price), bins = 20)
fig.suptitle('Error Terms', fontsize = 20)                  # Plot heading 
plt.xlabel('Errors', fontsize = 18)  
Error terms seem to be approximately normally distributed, so the assumption on the linear modeling seems to be fulfilled.

#Scaling the test set
num_vars = ['wheelbase', 'curbweight', 'enginesize', 'boreratio', 'horsepower','fueleconomy','carlength','carwidth','price']
df_test[num_vars] = scaler.fit_transform(df_test[num_vars])

#Dividing into X and y
y_test = df_test.pop('price')
X_test = df_test
--
# Now let's use our model to make predictions.
X_train_new = X_train_new.drop('const',axis=1)
# Creating X_test_new dataframe by dropping variables from X_test
X_test_new = X_test[X_train_new.columns]

# Adding a constant variable 
X_test_new = sm.add_constant(X_test_new)
===
# Making predictions
y_pred = lm.predict(X_test_new)
Evaluation of test via comparison of y_pred and y_test
from sklearn.metrics import r2_score 
r2_score(y_test, y_pred)

#EVALUATION OF THE MODEL
# Plotting y_test and y_pred to understand the spread.
fig = plt.figure()
plt.scatter(y_test,y_pred)
fig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading 
plt.xlabel('y_test', fontsize=18)                          # X-label
plt.ylabel('y_pred', fontsize=16) 

Evaluation of the model using Statistics

print(lm.summary())
d. Write down a business interpretation/explanation of the model – which variables are affecting the target the most and explain the relationship. Feel free to use charts or graphs to explain. (4 marks)
R-sqaured and Adjusted R-squared (extent of fit) - 0.899 and 0.896 - 90% variance explained.
F-stats and Prob(F-stats) (overall model fit) - 308.0 and 1.04e-67(approx. 0.0) - Model fir is significant and explained 90% variance is just not by chance.
p-values - p-values for all the coefficients seem to be less than the significance level of 0.05. - meaning that all the predictors are statistically significant.



Adj. R-squared: 0.923


----------------------
What is Multicollinearity?
Multicollinearity occurs when two or more independent variables(also known as predictor) are highly correlated with one another in a regression model.

This means that an independent variable can be predicted from another independent variable in a regression model. For Example, height, and weight, student consumption and father income, age and experience, mileage and price of a car, etc.
The problem with having multicollinearity
Since in a regression model our research objective is to find out how each predictor is impacting the target variable individually which is also an assumption of a method namely Ordinary Least Squares through which we can find the parameters of a regression model.
What causes multicollinearity?
Multicollinearity might occur due to the following reasons:

1. Multicollinearity could exist because of the problems in the dataset at the time of creation. These problems could be because of poorly designed experiments, highly observational data, or the inability to manipulate the data. (This is known as Data related multicollinearity)

2. Multicollinearity could also occur when new variables are created which are dependent on other variables(Basically when we do the data preprocessing or feature engineering to make the new feature from the existing features . This is known as Structure related multicollinearity)
3. When there are identical variables in the dataset.
4. When we want to encode the categorical features to numerical features for applying the machine learning algorithms since ML algorithms only understand numbers not text. So for this task, we use the concept of the Dummy variable. Inaccurate use of Dummy variables can also cause multicollinearity. (This is known as Dummy Variable Trap)
5. Insufficient data in some cases can also cause multicollinearity problems.
Detecting Multicollinearity using VIF (Variance Inflation Factor)
” VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable. “
or
VIF score of an independent variable represents how well the variable is explained by other independent variables.
R² value is determined to find out how well an independent variable is described by the other independent variables. A high value of R² means that the variable is highly correlated with the other variables. This is captured by the VIF which is denoted below:

VIF=1/(1-R²)

So, the closer the R² value to 1, the higher the value of VIF and the higher the multicollinearity with the particular independent variable.

VIF starts at 1(when R²=0, VIF=1 — minimum value for VIF) and has no upper limit.
VIF = 1, no correlation between the independent variable and the other variables.
VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others.
Some researchers assume VIF>5 as a serious issue for our model while some researchers assume VIF>10 as serious, it varies from person to person.

Addressing Multicollinearity:
Feature Selection: One approach to mitigate multicollinearity is by selecting a subset of relevant features. We can remove variables that have high VIF values and retain those that are crucial for the model.
2. Principal Component Analysis (PCA): It transforms the original features into a set of uncorrelated principal components,
3. Regularization Techniques:

Regularization methods like Ridge Regression (L2 regularization) and Lasso Regression (L1 regularization) can also help mitigate multicollinearity. These techniques add penalty terms to the regression model, reducing the impact of highly correlated features. Let’s demonstrate Ridge Regression in Python
from sklearn.linear_model import Ridge

# Create a Ridge Regression model
ridge_model = Ridge(alpha=1.0)

# Fit the model to the data
ridge_model.fit(X, y)

# Get the coefficients
coefficients = ridge_model.coef_
print("Ridge Regression coefficients:", coefficients)
----
Homoscedasticity essentially means ‘same variance' and is an important concept in linear regression.

Homoscedasticity describes how the error term (the noise or disturbance between independent and dependent variables) is the same across the values of the independent variables. So, in homoscedasticity, the residual term is constant across observations, i.e., the variance is constant. In simple terms, as the value of the dependent variable changes, the error term does not vary much.
n contrast, heteroscedasticity occurs when the size of the error term differs across the independent variable’s value. Heteroscedasticity may lead to inaccurate inferences and occurs when the standard deviations of a predicted variable, monitored across different values of an independent variable, are non-constant. 

Heteroscedasticity is an issue for linear regression because ordinary least squares (OLS) regression assumes that residuals have constant variance (homoscedasticity).

Heteroscedasticity doesn’t create bias, but it means the results of a regression analysis become hard to trust. More specifically, while heteroscedasticity increases the variance of the regression coefficient estimates, the regression model itself fails to pick up on this. Homoscedasticity and heteroscedasticity form a scale; as one increases, the other decreases. 
There are many tests for heteroscedasticity, but you can also just plot the errors against predicted values and see visually detect the hallmark pattern of heteroscedasticity. The Breusch-Pagan and Goldfeld-Quandt are two tests that detect and analyze homoscedasticity and heteroscedasticity. 
Homoscedasticity occurs when the variance in a dataset is constant, making it easier to estimate the standard deviation and variance of a data set. 

Heteroscedasticity occurs for many reasons, but many issues lie in the dataset itself. 

Models that utilize a wider range of observed values are more prone to heteroscedasticity. This is generally because the difference between the smallest and large values is more significant in these datasets, thus increasing the chance of heteroscedasticity. 


Reasons for Multicollinearity
Multicollinearity can exist when two independent variables are highly correlated. It can also happen if an independent variable is computed from other variables in the data set or if two independent variables provide similar and repetitive results.


What is Overfitting?
When a model performs very well for training data but has poor performance with test data (new data), it is known as overfitting. In this case, the machine learning model learns the details and noise in the training data such that it negatively affects the performance of the model on test data. Overfitting can happen due to low bias and high variance.
The Complete Guide on Overfitting and Underfitting in Machine Learning
Lesson 26 of 38By Avijeet Biswal

Last updated on Mar 12, 2024112286
The Complete Guide on Overfitting and Underfitting in Machine Learning
PreviousNext
Table of Contents
What is Overfitting?Reasons for OverfittingWays to Tackle OverfittingWhat is Underfitting?Reasons for UnderfittingView More
Overfitting and Underfitting are two crucial concepts in machine learning and are the prevalent causes for the poor performance of a machine learning model. This tutorial will explore Overfitting and Underfitting in machine learning, and help you understand how to avoid them with a hands-on demonstration.

Your AI/ML Career is Just Around The Corner!
AI Engineer Master's ProgramExplore ProgramYour AI/ML Career is Just Around The Corner!
What is Overfitting?
When a model performs very well for training data but has poor performance with test data (new data), it is known as overfitting. In this case, the machine learning model learns the details and noise in the training data such that it negatively affects the performance of the model on test data. Overfitting can happen due to low bias and high variance.

Overfitting_in-ML

Become an AI and ML Expert in 2024
Discover the Power of AI and ML With UsEXPLORE NOWBecome an AI and ML Expert in 2024
Reasons for Overfitting
Data used for training is not cleaned and contains noise (garbage values) in it
The model has a high variance
The size of the training dataset used is not enough
The model is too complex
Ways to Tackle Overfitting
Using K-fold cross-validation
Using Regularization techniques such as Lasso and Ridge
Training model with sufficient data
Adopting ensembling techniques
What is Underfitting?
When a model has not learned the patterns in the training data well and is unable to generalize well on the new data, it is known as underfitting. An underfit model has poor performance on the training data and will result in unreliable predictions. Underfitting occurs due to high bias and low variance.

Reasons for Underfitting
Data used for training is not cleaned and contains noise (garbage values) in it
The model has a high bias
The size of the training dataset used is not enough
The model is too simple
Ways to Tackle Underfitting
Increase the number of features in the dataset
Increase model complexity
Reduce noise in the data

Increase the duration of training the data
Now that you have understood what overfitting and underfitting are, let’s see what is a good fit model in this tutorial on overfitting and underfitting in machine learning. 
What Is a Good Fit In Machine Learning?
To find the good fit model, you need to look at the performance of a machine learning model over time with the training data. As the algorithm learns over time, the error for the model on the training data reduces, as well as the error on the test dataset. If you train the model for too long, the model may learn the unnecessary details and the noise in the training set and hence lead to overfitting. In order to achieve a good fit, you need to stop training at a point where the error starts to increase.

-------------
import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder
import matplotlib.pyplot as plt
import seaborn as sns
df_base = pd.read_csv("Bengaluru_House_Data.csv")
df_base
#2. Print/show the dimensions of Dataframe i.e., no of rows and columns. (1 mark)
df_base.shape
#3. Print/show the data types of all the features/columns. (1 mark)
df_base.info()
#4. Print/show statistical summary of all the numeric featurs. (1 mark)
df_base.select_dtypes(include=np.number).describe()
#5. Print/show statistical summary for all the categorical variable. (2 marks)
df_base.select_dtypes(include='object').describe()  df.describe(incude='object).T
#1. Show/Visualize the relationship between fetures 'bath' and 'price' using scattered plot. (1 marks)
plt.scatter(df_base['bath'].values, df_base['price'].values)
plt.title("Bath vs Price plot")
plt.show()
#2. Show/Visualize the relationship between fetures 'balcony'and 'price' using scattered plot. (1 mark)
plt.scatter(df_base['balcony'].values, df_base['price'].values)
plt.title("Balcony vs Price plot")
plt.show()
#show/Visualize the relationship between features 'bath','balcony' and 'price' using 3D Scatterplot. (2 marks)
ax = plt.axes(projection='3d')
ax.scatter(df_base['balcony'].values, df_base['bath'].values, df_base['price'].values)
#Show outliers distribution of variable 'bath' by drawing Boxplot. (3marks)
sns.boxplot(x=df_base['bath'].values)
#1. Replace missing values of the feature 'balcony' with numerical value 0 and convert its feature type to int.(2 marks)
df_base.balcony.fillna(0, inplace=True)
df_base['balcony'] = df_base['balcony'].astype(int)
df_base.info()
#2. Replace missing values of the feature 'bath' missing values with numerical 1 and convert feature type to int.(2 marks)
df_base.bath.fillna(1, inplace=True)
df_base['bath'] = df_base['bath'].astype(int)
df_base.info()
#3. Replace missing values of the feature 'location' with a constant "missing".(2 marks)
df_base.location.fillna('missing', inplace=True)
df_base.isnull().sum()
#5. Convert the feature 'size' to int by removing alphabetic content and keep only numeric content.
#In case of missing/null content replace by constant numeric value- 2. (3 marks)
df_base['size'].fillna('2', inplace=True)
df_base['size'] = df_base['size'].apply(lambda x: int(x) if(str(x).isalnum()) else int(2))
df_base.isnull().sum()
#6. Convert the feature 'total_sqft' to numerical using 'to_numeric' method. 
#Also, replace all its missing entries by mean.(3 marks)

df_base['total_sqft'] = pd.to_numeric(df_base['total_sqft'], errors="coerce")
df_base.total_sqft.fillna(df_base.total_sqft.mean(), inplace=True)
lb_enc = LabelEncoder()
df_base['avilability_encoded'] = lb_enc.fit_transform(df_base['availability'])
df_base['location_encoded'] = lb_enc.fit_transform(df_base['location'])
df_base['society_encoded'] = lb_enc.fit_transform(df_base['society'])
df_base = pd.get_dummies(df_base, columns=['area_type'], dtype=int)
df_base.drop(columns=['availability', 'location', 'society'], axis=1, inplace=True) 
df_base.dtypes
#1. Split the processed Dataframe into 2 parts train and test with ratio as 70:30. 
#Ensure feature 'price' as target(y). (3 marks)

from sklearn.model_selection import train_test_split
target = df_base['price']
X = df_base.drop(columns='price', axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.3, random_state=100)
X_train.shape
#2. Use OLS statsmodels package to build the Linear Regression model on the train set. 
#Also,generate the summary report. (6 marks)

lr = sm.OLS(y_train, X_train)
lr = lr.fit()
lr.summary()
Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 8.18e-26. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
#linear reg model
from sklearn.linear_model import LinearRegression

sk_lr = LinearRegression()
sk_lr = sk_lr.fit(X_train, y_train)
print(sk_lr.score(X_train, y_train))
sk_lr.coef_
#Train below models and obtain values using 5 fold cross validation on train data and 'RMSE' metric. 
#Find the metric (RMSE) score in test set and suggest the best model
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
score = cross_val_score(estimator=LinearRegression(), X=X_train, y=y_train, cv=5, scoring='r2')
print(score)
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error
ridge = Ridge(alpha=1, max_iter=500)
ridge = ridge.fit(X_train, y_train)
r_score = cross_val_score(estimator=ridge, X=X_train, y=y_train, cv=5, scoring='r2')
r_rmse = np.sqrt(mean_squared_error(y_test, ridge.predict(X_test)))
print(r_score, np.mean(r_score), r_rmse)
#Lasso (alpha = 0.01, max_iter = 500) (5 marks)

lasso = Lasso(alpha=0.01, max_iter=500)
lasso.fit(X_train, y_train)
l_score = cross_val_score(estimator=lasso, X=X_train, y=y_train, cv=5, scoring='r2')
l_rmse = np.sqrt(mean_squared_error(y_test, lasso.predict(X_test)))
print(l_score, np.mean(l_score), l_rmse)
#- ElasticNet(alpha = 0.1, l1_ratio = 0.01, max_iter = 500) (5 marks)
el_net = ElasticNet(alpha=0.1, l1_ratio=0.01, max_iter=500)
el_net.fit(X_train, y_train)
el_score = cross_val_score(estimator=el_net, X=X_train, y=y_train, cv=5, scoring='r2')
el_rmse = np.sqrt(mean_squared_error(y_test, el_net.predict(X_test)))
print(el_score, np.mean(el_score), el_rmse)
#Using Random search on Lasso model find the best value of alpha and corresponding RMSE value on test set
from sklearn.model_selection import RandomizedSearchCV

tuned_paramaters = [{'alpha':[1e-15, 1e-10, 1e-8, 0.0001, 0.001, 0.01, 0.1, 1, 5, 10, 20],
                     'max_iter':[10,50,100,150,200,250,300,400,500]}]
rsearch = RandomizedSearchCV(estimator=Lasso(), param_distributions=tuned_paramaters, scoring='r2')
rsearch = rsearch.fit(X_train, y_train)
rsearch.score(X_train, y_train)
rsearch.best_params_
